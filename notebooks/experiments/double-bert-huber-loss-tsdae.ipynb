{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWNlJZBBlUEn"
      },
      "source": [
        "# Fine-tuning transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ANkGj4glUEs"
      },
      "source": [
        "### Getting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4ogkf1vlUEt"
      },
      "source": [
        "#### Train/test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:19:48.438044Z",
          "iopub.status.busy": "2024-12-18T16:19:48.437033Z",
          "iopub.status.idle": "2024-12-18T16:19:55.417511Z",
          "shell.execute_reply": "2024-12-18T16:19:55.416372Z",
          "shell.execute_reply.started": "2024-12-18T16:19:48.437974Z"
        },
        "id": "oaW0RUIYlUEt",
        "outputId": "bd282e19-8ece-4103-f22c-7a5c69f3b4f2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-18 16:19:51--  https://drive.usercontent.google.com/download?id=1iqg1FIPfbrZWlung6gZqve1MeQWc0Je4&export=download&authuser=1&confirm=t\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.197.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 218678075 (209M) [application/octet-stream]\n",
            "Saving to: './data/dataset.csv'\n",
            "\n",
            "./data/dataset.csv  100%[===================>] 208.55M   111MB/s    in 1.9s    \n",
            "\n",
            "2024-12-18 16:19:55 (111 MB/s) - './data/dataset.csv' saved [218678075/218678075]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!mkdir data/history\n",
        "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1iqg1FIPfbrZWlung6gZqve1MeQWc0Je4&export=download&authuser=1&confirm=t' -O './data/dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:19:55.420135Z",
          "iopub.status.busy": "2024-12-18T16:19:55.41976Z",
          "iopub.status.idle": "2024-12-18T16:19:58.998256Z",
          "shell.execute_reply": "2024-12-18T16:19:58.997442Z",
          "shell.execute_reply.started": "2024-12-18T16:19:55.420091Z"
        },
        "id": "KO2AC9sHlUEv",
        "outputId": "b8f02976-4a0f-4029-b729-fc9d3560e954",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22224 entries, 0 to 22223\n",
            "Data columns (total 13 columns):\n",
            " #   Column                              Non-Null Count  Dtype  \n",
            "---  ------                              --------------  -----  \n",
            " 0   title                               22224 non-null  object \n",
            " 1   location                            22224 non-null  object \n",
            " 2   company                             22224 non-null  object \n",
            " 3   skills                              14384 non-null  object \n",
            " 4   description                         22224 non-null  object \n",
            " 5   salary_from                         22224 non-null  float64\n",
            " 6   source                              22224 non-null  object \n",
            " 7   experience_from                     22224 non-null  float64\n",
            " 8   description_no_numbers              22224 non-null  object \n",
            " 9   log_salary_from                     22224 non-null  float64\n",
            " 10  description_no_numbers_with_skills  22224 non-null  object \n",
            " 11  experience_to_adjusted_10           22224 non-null  float64\n",
            " 12  description_size                    22224 non-null  int64  \n",
            "dtypes: float64(4), int64(1), object(8)\n",
            "memory usage: 2.2+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv('./data/dataset.csv')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyH0cCfnlUEw"
      },
      "source": [
        "### Extra dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:19:58.999807Z",
          "iopub.status.busy": "2024-12-18T16:19:58.999525Z",
          "iopub.status.idle": "2024-12-18T16:20:10.39408Z",
          "shell.execute_reply": "2024-12-18T16:20:10.393073Z",
          "shell.execute_reply.started": "2024-12-18T16:19:58.99978Z"
        },
        "id": "EfxAcpVNlUEx",
        "outputId": "29fc70db-63da-4e5d-c111-713fa3f3de9d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U sentence-transformers datasets -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:10.396558Z",
          "iopub.status.busy": "2024-12-18T16:20:10.396229Z",
          "iopub.status.idle": "2024-12-18T16:20:11.788262Z",
          "shell.execute_reply": "2024-12-18T16:20:11.787349Z",
          "shell.execute_reply.started": "2024-12-18T16:20:10.396528Z"
        },
        "id": "mBVcMO97lUEx",
        "outputId": "bbaebe5c-c7c2-41d9-f47f-385dc5be25a5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6hvnstWlUEz"
      },
      "source": [
        "### Service functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:11.78993Z",
          "iopub.status.busy": "2024-12-18T16:20:11.789515Z",
          "iopub.status.idle": "2024-12-18T16:20:31.997722Z",
          "shell.execute_reply": "2024-12-18T16:20:31.997034Z",
          "shell.execute_reply.started": "2024-12-18T16:20:11.789903Z"
        },
        "id": "A_vtIi4GlUEz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import gc\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
        "from sentence_transformers import models, util, datasets, evaluation, losses\n",
        "from numba import cuda\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "def memory_cleanup():\n",
        "    \"Clean up memory\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def get_sentence_lengths(text):\n",
        "    \"Get number of words in each sentence in the text\"\n",
        "    pattern = r'(?<=[.!?])'\n",
        "    sentences = re.split(pattern, text)\n",
        "\n",
        "    # remove empty strings\n",
        "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
        "\n",
        "    # get number of words in each sentence\n",
        "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
        "\n",
        "    return sentences, sentence_lengths\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    \"Set seed for reproducibility\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def train_tsdae_bert(model_name, train_sentences):\n",
        "    \"\"\"Train a denoising auto-encoder model with BERT model.\n",
        "    more examples at https://sbert.net/examples/unsupervised_learning/TSDAE/README.html\"\"\"\n",
        "    word_embedding_model = models.Transformer(model_name)\n",
        "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "    # Create the special denoising dataset that adds noise on-the-fly\n",
        "    train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
        "\n",
        "    # DataLoader to batch your data\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Use the denoising auto-encoder loss\n",
        "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
        "        model, decoder_name_or_path=model_name, tie_encoder_decoder=True,\n",
        "    )\n",
        "\n",
        "    # Call the fit method\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        epochs=1,\n",
        "        weight_decay=0,\n",
        "        scheduler=\"constantlr\",\n",
        "        optimizer_params={\"lr\": 3e-5},\n",
        "        show_progress_bar=True,\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq8mq2UnlUE0"
      },
      "source": [
        "Display model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:31.99945Z",
          "iopub.status.busy": "2024-12-18T16:20:31.998904Z",
          "iopub.status.idle": "2024-12-18T16:20:32.010976Z",
          "shell.execute_reply": "2024-12-18T16:20:32.010145Z",
          "shell.execute_reply.started": "2024-12-18T16:20:31.999421Z"
        },
        "id": "1ahVGOQllUE1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import t\n",
        "\n",
        "\n",
        "def display_metrics_with_ci(history: dict):\n",
        "    # plot mean and ci for train and test r2 for all seeds and all iterations, averaged over seeds\n",
        "    seeds = list(history.keys())\n",
        "    def mean_confidence_interval(data, confidence=0.95):\n",
        "        n = len(data)\n",
        "        m, se = np.mean(data), np.std(data) / np.sqrt(n)\n",
        "        h = se * t.ppf((1 + confidence) / 2, n-1)\n",
        "        return m, m-h, m+h\n",
        "\n",
        "    r2_train_values = [history[seed]['train_r2'] for seed in seeds]\n",
        "    r2_test_values = [history[seed]['test_r2'] for seed in seeds]\n",
        "\n",
        "    r2_train_values = np.array(r2_train_values)\n",
        "    r2_test_values = np.array(r2_test_values)\n",
        "\n",
        "    r2_train_mean = np.mean(r2_train_values, axis=0)\n",
        "    r2_test_mean = np.mean(r2_test_values, axis=0)\n",
        "\n",
        "    r2_train_ci = np.array([mean_confidence_interval(r2_train_values[:, i]) for i in range(r2_train_values.shape[1])])\n",
        "    r2_test_ci = np.array([mean_confidence_interval(r2_test_values[:, i]) for i in range(r2_test_values.shape[1])])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(r2_train_mean[1:], label='train')\n",
        "    plt.fill_between(range(len(r2_train_mean[1:])), r2_train_ci[1:, 1], r2_train_ci[1:, 2], alpha=0.3)\n",
        "\n",
        "    plt.plot(r2_test_mean[1:], label='test')\n",
        "    plt.fill_between(range(len(r2_test_mean[1:])), r2_test_ci[1:, 1], r2_test_ci[1:, 2], alpha=0.3)\n",
        "    plt.title('Mean R2 by epoch, with 95% CI')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('R2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    mae_test_values = [history[seed]['test_mae'] for seed in seeds]\n",
        "    rmse_test_values = [history[seed]['test_rmse'] for seed in seeds]\n",
        "\n",
        "    mae_test_values = np.array(mae_test_values)\n",
        "    rmse_test_values = np.array(rmse_test_values)\n",
        "\n",
        "    mae_test_mean = np.mean(mae_test_values, axis=0)\n",
        "    rmse_test_mean = np.mean(rmse_test_values, axis=0)\n",
        "\n",
        "    mae_test_ci = np.array([mean_confidence_interval(mae_test_values[:, i]) for i in range(mae_test_values.shape[1])])\n",
        "    rmse_test_ci = np.array([mean_confidence_interval(rmse_test_values[:, i]) for i in range(rmse_test_values.shape[1])])\n",
        "\n",
        "    # get an index of the epoch, where the test R2 is the highest\n",
        "    # get mean and CI for this epoch\n",
        "    best_epoch = np.argmax(r2_test_mean)\n",
        "    best_epoch_r2 = r2_test_mean[best_epoch]\n",
        "    best_epoch_mae = mae_test_mean[best_epoch]\n",
        "    best_epoch_rmse = rmse_test_mean[best_epoch]\n",
        "    best_epoch_r2_ci = r2_test_ci[best_epoch]\n",
        "    best_epoch_mae_ci = mae_test_ci[best_epoch]\n",
        "    best_epoch_rmse_ci = rmse_test_ci[best_epoch]\n",
        "\n",
        "    print(f'TEST METRICS FOR THE BEST EPOCH (#{best_epoch+1})')\n",
        "    print(f'R2: mean = {best_epoch_r2:.4f}, 95% CI = [{best_epoch_r2_ci[1]:.4f}, {best_epoch_r2_ci[2]:.4f}]')\n",
        "    print(f'MAE: mean = {best_epoch_mae:.4f}, 95% CI = [{best_epoch_mae_ci[1]:.4f}, {best_epoch_mae_ci[2]:.4f}]')\n",
        "    print(f'RMSE: mean = {best_epoch_rmse:.4f}, 95% CI = [{best_epoch_rmse_ci[1]:.4f}, {best_epoch_rmse_ci[2]:.4f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyiRxdHblUE1"
      },
      "source": [
        "### Traning-related classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dnRhksxlUE3"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:32.01244Z",
          "iopub.status.busy": "2024-12-18T16:20:32.012103Z",
          "iopub.status.idle": "2024-12-18T16:20:32.043939Z",
          "shell.execute_reply": "2024-12-18T16:20:32.043099Z",
          "shell.execute_reply.started": "2024-12-18T16:20:32.012404Z"
        },
        "id": "kfmizKTflUE3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Dataset for dual textual features\n",
        "class DualTextDataset(Dataset):\n",
        "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
        "        print('Creating the dataset...')\n",
        "        # Pre-tokenize and store inputs\n",
        "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        self.targets = targets.tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return only the slice for idx\n",
        "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
        "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
        "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
        "        return inputs1, inputs2, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW2CU1G1lUE4"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1diPd8clUE6"
      },
      "source": [
        "##### Double-head BERT with TSDAE pre-tuning and MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:32.10322Z",
          "iopub.status.busy": "2024-12-18T16:20:32.102983Z",
          "iopub.status.idle": "2024-12-18T16:20:32.118606Z",
          "shell.execute_reply": "2024-12-18T16:20:32.117695Z",
          "shell.execute_reply.started": "2024-12-18T16:20:32.103197Z"
        },
        "id": "ojKta8pklUE6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TSDAEDualBERTWithMLP(nn.Module):\n",
        "    \"\"\"two berts for two text features with MLP head for regression.\n",
        "    The model is pre-tuned with TSDAE.\"\"\"\n",
        "    def __init__(self, config, bert1, bert2):\n",
        "        super(TSDAEDualBERTWithMLP, self).__init__()\n",
        "        # Load TSDAE-ed BERT models\n",
        "        self.bert1 = bert1\n",
        "        self.bert2 = bert2\n",
        "\n",
        "        # Define MLP head\n",
        "        hidden_size = config['hidden_size']\n",
        "        mlp_hidden_size = config['mlp_hidden_size']\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
        "        )\n",
        "\n",
        "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
        "        # idea from: https://github.com/UKPLab/sentence-transformers/issues/2494\n",
        "        # Forward pass through BERT1\n",
        "        input_dict1 = {\n",
        "            'input_ids': input1,\n",
        "            'attention_mask': attention_mask1\n",
        "        }\n",
        "        cls1 = self.bert1(input_dict1)['sentence_embedding']\n",
        "\n",
        "        # Forward pass through BERT2\n",
        "        input_dict2 = {\n",
        "            'input_ids': input2,\n",
        "            'attention_mask': attention_mask2\n",
        "        }\n",
        "        cls2 = self.bert2(input_dict2)['sentence_embedding']\n",
        "\n",
        "        # Concatenate CLS embeddings\n",
        "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
        "\n",
        "        # Pass through MLP head\n",
        "        output = self.mlp(combined_cls)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTjTinuVlUE8"
      },
      "source": [
        "#### Training methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:32.134985Z",
          "iopub.status.busy": "2024-12-18T16:20:32.134715Z",
          "iopub.status.idle": "2024-12-18T16:20:32.15094Z",
          "shell.execute_reply": "2024-12-18T16:20:32.150289Z",
          "shell.execute_reply.started": "2024-12-18T16:20:32.134961Z"
        },
        "id": "zDer2FqQlUE8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "def fit_eval(\n",
        "    seed,\n",
        "    model,\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train_reg,\n",
        "    y_test_reg,\n",
        "    criterion,\n",
        "    tokenizer,\n",
        "    config,\n",
        "    text_col_1,\n",
        "    text_col_2,\n",
        "):\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Memory cleanup\n",
        "    memory_cleanup()\n",
        "\n",
        "    # Unpack config\n",
        "    learning_rate = config[\"learning_rate\"]\n",
        "    num_epochs = config[\"num_epochs\"]\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    seq_length = config[\"seq_length\"]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Make datasets\n",
        "    train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train_reg, tokenizer, seq_length)\n",
        "    test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test_reg, tokenizer, seq_length)\n",
        "    # Make dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training and Evaluation Loop\n",
        "    history = {\n",
        "                \"train_loss\": [],\n",
        "               \"test_loss\": [],\n",
        "                \"train_rmse\": [],\n",
        "               \"test_rmse\": [],\n",
        "               \"train_r2\": [],\n",
        "               \"train_r2\": [],\n",
        "               \"test_r2\": [],\n",
        "               \"train_mae\": [],\n",
        "               \"test_mae\": [],\n",
        "               \"y_pred\": [],\n",
        "               \"y_test\": [],\n",
        "               }\n",
        "\n",
        "    print('Starting training/eval loop...')\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting training...')\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in train_dataloader:\n",
        "            inputs1, inputs2, targets = batch\n",
        "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
        "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
        "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
        "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
        "            outputs = outputs.flatten()\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            all_preds.extend(outputs.cpu().detach().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "\n",
        "        train_r2 = r2_score(all_labels, all_preds)\n",
        "\n",
        "        train_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
        "\n",
        "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_r2\"].append(train_r2)\n",
        "        history[\"train_rmse\"].append(train_rmse)\n",
        "        history[\"train_mae\"].append(train_mae)\n",
        "\n",
        "        # Evaluation Phase\n",
        "        print('Epoch done, evaluating...')\n",
        "        model.eval()\n",
        "        test_losses = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                inputs1, inputs2, targets = batch\n",
        "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
        "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
        "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
        "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
        "                outputs = outputs.flatten()\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                test_losses.append(loss.item())\n",
        "\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "        history[\"y_pred\"].append(all_preds)\n",
        "        history[\"y_test\"].append(all_labels)\n",
        "\n",
        "        test_loss = np.mean(test_losses)\n",
        "\n",
        "        test_r2 = r2_score(all_labels, all_preds)\n",
        "\n",
        "        test_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
        "\n",
        "        test_mae = mean_absolute_error(all_labels, all_preds)\n",
        "\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "\n",
        "        history[\"test_r2\"].append(test_r2)\n",
        "\n",
        "        history[\"test_rmse\"].append(test_rmse)\n",
        "\n",
        "        history[\"test_mae\"].append(test_mae)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
        "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa7z-DX-lUE-"
      },
      "source": [
        "### Training-eval loop with experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdP-wUI6lUE-"
      },
      "source": [
        "#### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVOVWU2flUE-"
      },
      "source": [
        "##### Define text feature/target columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:32.179413Z",
          "iopub.status.busy": "2024-12-18T16:20:32.17915Z",
          "iopub.status.idle": "2024-12-18T16:20:32.191833Z",
          "shell.execute_reply": "2024-12-18T16:20:32.191128Z",
          "shell.execute_reply.started": "2024-12-18T16:20:32.179389Z"
        },
        "id": "d0a2mx6ClUE-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "text_col_1 = 'description_no_numbers'\n",
        "text_col_1_with_prompt = text_col_1 + '_with_prompt' # Add prompt to text column\n",
        "\n",
        "text_col_2 = 'title_company_location_skills_source' # Merged text column, second feature\n",
        "\n",
        "target_col = 'log_salary_from' # regression target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pralQ35wlUE_"
      },
      "source": [
        "##### Create merged title/skills/location/source feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:32.192953Z",
          "iopub.status.busy": "2024-12-18T16:20:32.192734Z",
          "iopub.status.idle": "2024-12-18T16:20:32.605849Z",
          "shell.execute_reply": "2024-12-18T16:20:32.604909Z",
          "shell.execute_reply.started": "2024-12-18T16:20:32.192931Z"
        },
        "id": "ujJvSvjplUE_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['skills'] = df['skills'].fillna('Не указаны')\n",
        "\n",
        "title_company_location_skills_feature_template = \"\"\"\n",
        "Позиция: {position}\n",
        "Компания: {company}\n",
        "Место: {location}\n",
        "Навыки: {skills}\n",
        "Источник: {source}\n",
        "\"\"\"\n",
        "\n",
        "df[text_col_2] = df.apply(lambda x: title_company_location_skills_feature_template.format(\n",
        "    position=x['title'],\n",
        "    company=x['company'],\n",
        "    location=x['location'],\n",
        "    skills=x['skills'],\n",
        "    source=x['source']\n",
        "), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Add a prompt to the feature 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt to be added to feature 1 for [MASK] token embedding regression\n",
        "# (may be not used here)\n",
        "prompt = \"\"\"\\\n",
        "[CLS] Далее указано описание вакансии. \\\n",
        "Судя по описанию, зарплата на этой позиции составляет [MASK].[SEP]\\\n",
        "\"\"\"\n",
        "\n",
        "df[text_col_1_with_prompt] = prompt + df[text_col_1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tizCr-IqlUFA"
      },
      "source": [
        "#### Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:32.687673Z",
          "iopub.status.busy": "2024-12-18T16:20:32.687419Z",
          "iopub.status.idle": "2024-12-18T16:20:33.012212Z",
          "shell.execute_reply": "2024-12-18T16:20:33.011359Z",
          "shell.execute_reply.started": "2024-12-18T16:20:32.687649Z"
        },
        "id": "zICXj-HZlUFA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "memory_cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuhmnm94lUFD"
      },
      "source": [
        "##### Experiment 5: Double BERT, Huber loss: pre-tune with TSDAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "0a02f0011cbe465eb0049d8d49199464",
            "334d54ddd5f24a97aa297ae802f4f6a4",
            "90660de43de244fa846edf7cb949432d",
            "11b3f53dcb8047988670885522645547",
            "505be7310695405a9bb273938c883083",
            "200bedba08654239a363d717977c45c5"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-12-18T16:20:49.562062Z",
          "iopub.status.busy": "2024-12-18T16:20:49.561718Z",
          "iopub.status.idle": "2024-12-18T16:23:07.786102Z",
          "shell.execute_reply": "2024-12-18T16:23:07.78522Z",
          "shell.execute_reply.started": "2024-12-18T16:20:49.562032Z"
        },
        "id": "SliTs_CQlUFD",
        "outputId": "91823941-1445-41c8-c9e8-69debd4bd9df",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import pickle\n",
        "import warnings\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "experiment_name = 'double_bert_huber_tsdae'\n",
        "print(experiment_name.upper())\n",
        "print('='*100)\n",
        "print()\n",
        "\n",
        "# Suppress all FutureWarnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "seeds = [42, 78687, 123123]\n",
        "combined_history = {}\n",
        "\n",
        "# Hyperparameters and configuration\n",
        "config = {\n",
        "    \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
        "    \"batch_size\": 32,\n",
        "    \"seq_length\": 1024,\n",
        "    \"hidden_size\": 312,\n",
        "    \"mlp_hidden_size\": 128,\n",
        "    \"num_epochs\": 10,\n",
        "    \"learning_rate\": 5e-6,\n",
        "    \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
        "}\n",
        "\n",
        "memory_cleanup()\n",
        "\n",
        "model_name = config['model_name']\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset and DataLoader\n",
        "# Prepare data\n",
        "X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
        "y = df[[target_col,]]\n",
        "\n",
        "for seed in seeds:\n",
        "    memory_cleanup()\n",
        "    print()\n",
        "    print(f'Starting for seed {str(seed)}...')\n",
        "    print('-' * 100)\n",
        "    print()\n",
        "\n",
        "    combined_history[seed] = {}\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Split train-test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    # further split train data into regression train and tsdae train data\n",
        "    X_train_tsdae, X_tsdae, y_train_tsdae, _ = train_test_split(X_train, y_train, test_size=0.01, random_state=seed)\n",
        "\n",
        "    # get test and train targets\n",
        "    y_test_reg = y_test[target_col]\n",
        "    y_train_tsdae_reg = y_train_tsdae[target_col]\n",
        "\n",
        "    # NOTE:\n",
        "    # X_tsdae used for TSDAE\n",
        "    # X_train_tsdae, y_train_tsdae used for the downstream regression training\n",
        "    # X_test, y_test used for the final model evaluation\n",
        "    # y_test_tsdae is not used (hence set to '_')\n",
        "\n",
        "    # TSDAE part:\n",
        "\n",
        "    # 1. convert job descriptions into a set of sentences\n",
        "    # and select 10-60 word sentences as a feature column.\n",
        "    # 2. Train with TSDAE task on these selected sentences.\n",
        "\n",
        "    # 1. Create a DataFrame of unique sentences and their lengths for X_tsdae\n",
        "    unique_sentences = []\n",
        "    unique_sentence_lengths = []\n",
        "    for text in X_tsdae[text_col_1]:\n",
        "        sentences, sentence_lengths = get_sentence_lengths(text)\n",
        "        unique_sentences.extend(sentences)\n",
        "        unique_sentence_lengths.extend(sentence_lengths)\n",
        "\n",
        "    unique_sentences_df = pd.DataFrame({\n",
        "        'sentence': unique_sentences,\n",
        "        'length': unique_sentence_lengths\n",
        "    })\n",
        "\n",
        "    unique_sentences = unique_sentences_df[(unique_sentences_df.length >= 10) & (unique_sentences_df.length <= 60)]['sentence']\n",
        "\n",
        "    # get array with features for each bert\n",
        "    train_sentences_array = [\n",
        "        unique_sentences.tolist(),\n",
        "        X_tsdae[text_col_2].tolist(),\n",
        "    ]\n",
        "\n",
        "    # 2. Train the models on TSDAE task\n",
        "    berts_after_tsdae = []\n",
        "    for index, train_sentences in enumerate(train_sentences_array):\n",
        "        memory_cleanup()\n",
        "        berts_after_tsdae.append(train_tsdae_bert(model_name, train_sentences))\n",
        "    memory_cleanup()\n",
        "\n",
        "    tsdae_bert1, tsdae_bert2 = berts_after_tsdae\n",
        "\n",
        "    # Initialize the model for regression\n",
        "    model = TSDAEDualBERTWithMLP(config, tsdae_bert1, tsdae_bert2)\n",
        "    model = torch.nn.DataParallel(model).to(device)\n",
        "\n",
        "    # Loss Function\n",
        "    criterion = nn.HuberLoss()\n",
        "\n",
        "    # fit-eval regression\n",
        "    model, history = fit_eval(\n",
        "        seed,\n",
        "        model,\n",
        "        X_train_tsdae,\n",
        "        X_test,\n",
        "        y_train_tsdae_reg,\n",
        "        y_test_reg,\n",
        "        criterion,\n",
        "        tokenizer,\n",
        "        config,\n",
        "        text_col_1,\n",
        "        text_col_2,\n",
        "    )\n",
        "\n",
        "    memory_cleanup()\n",
        "\n",
        "    combined_history[seed] = history\n",
        "\n",
        "# Display metrics\n",
        "display_metrics_with_ci(combined_history)\n",
        "\n",
        "# save the history as pickle\n",
        "with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
        "    pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
