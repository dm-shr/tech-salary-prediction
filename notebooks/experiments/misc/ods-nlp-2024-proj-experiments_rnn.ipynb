{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:03.810629Z",
     "iopub.status.busy": "2024-12-16T09:01:03.810343Z",
     "iopub.status.idle": "2024-12-16T09:01:16.186941Z",
     "shell.execute_reply": "2024-12-16T09:01:16.185958Z",
     "shell.execute_reply.started": "2024-12-16T09:01:03.810595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22224 entries, 0 to 22223\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   title                                 22224 non-null  object \n",
      " 1   company                               22224 non-null  object \n",
      " 2   location                              22224 non-null  object \n",
      " 3   skills                                14384 non-null  object \n",
      " 4   source                                22224 non-null  object \n",
      " 5   description_no_numbers_with_skills    22224 non-null  object \n",
      " 6   experience_from                       22224 non-null  float64\n",
      " 7   experience_to_adjusted_10             22224 non-null  float64\n",
      " 8   description_size                      22224 non-null  int64  \n",
      " 9   description                           22224 non-null  object \n",
      " 10  description_no_numbers                22224 non-null  object \n",
      " 11  description_no_numbers_v2             22224 non-null  object \n",
      " 12  title_company_location_skills_source  22224 non-null  object \n",
      " 13  salary_from                           22224 non-null  float64\n",
      " 14  log_salary_from                       22224 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(10)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymystem3 swifter -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "# from string import punctuation\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "# russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ну что сказать <comma> я видеть кто <dash>то наступать на грабли <comma> ты разочаровывать я <comma> ты быть натравлять вот такой дело <period>\n",
      "\n",
      "по асфальт мимо цемент <comma> looking at the strangers <comma> избегать зевака под аплодисменты <period> обитатель спальный аррондисман\n",
      "\n",
      "ну что сказать <comma> я видеть кто <dash>то наступать на грабли <comma> ты разочаровывать я <comma> ты быть натравлять <period>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def token_lookup(word):\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Token corresponding to the punctuation.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = ' <PERIOD>'\n",
    "    token[','] = ' <COMMA>'\n",
    "    token[','] = ' <COMMA>'\n",
    "    token['\"'] = ' <QUOTATION_MARK>'\n",
    "    token[';'] = ' <SEMICOLON>'\n",
    "    token['!'] = ' <EXCLAMATION_MARK>'\n",
    "    token['?'] = ' <QUESTION_MARK>'\n",
    "    token['('] = ' <LEFT_PAREN>'\n",
    "    token[')'] = ' <RIGHT_PAREN>'\n",
    "    token['-'] = ' <DASH>'\n",
    "    token['\\n'] = ' <NEW_LINE>'\n",
    "    return token.get(word, word)\n",
    "\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    text = ''.join(map(token_lookup, text))\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "\n",
    "    # tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "    #           and token != \" \" \\\n",
    "    #           and token.strip() not in punctuation]\n",
    "\n",
    "    \n",
    "    text = \"\".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Examples    \n",
    "print(preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен вот такие дела.\"))\n",
    "\n",
    "print(preprocess_text(\"По асфальту мимо цемента, looking at the strangers, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\"))\n",
    "\n",
    "print(preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np vectorize to apply the function to the whole column\n",
    "# import numpy as np\n",
    "import swifter\n",
    "\n",
    "# description_no_numbers_v2_lemmatized = map(preprocess_text, df['description_no_numbers_v2'])\n",
    "# df['description_no_numbers_v2_lemmatized'] = np.array(list(description_no_numbers_v2_lemmatized))\n",
    "df['description_no_numbers_v2_lemmatized'] = df['description_no_numbers_v2'].swifter.apply(preprocess_text)\n",
    "# df['description_no_numbers_v2_lemmatized'] = df['description_no_numbers_v2'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:30.725630Z",
     "iopub.status.busy": "2024-12-16T09:01:30.725301Z",
     "iopub.status.idle": "2024-12-16T09:02:03.249282Z",
     "shell.execute_reply": "2024-12-16T09:02:03.248529Z",
     "shell.execute_reply.started": "2024-12-16T09:01:30.725601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"Clean up memory\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    \"Get number of words in each sentence in the text\"\n",
    "    # pattern = r'(?<=[.!?])\\s+'\n",
    "    pattern = r'(?<=[.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    # remove empty strings\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    # get number of words in each sentence\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    return sentences, sentence_lengths\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"Set seed for reproducibility\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_tsdae_bert(model_name, train_sentences):\n",
    "    \"\"\"Train a denoising auto-encoder model with BERT model.\n",
    "    more examples at https://sbert.net/examples/unsupervised_learning/TSDAE/README.html\"\"\"\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # Create the special denoising dataset that adds noise on-the-fly\n",
    "    train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "    \n",
    "    # DataLoader to batch your data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Use the denoising auto-encoder loss\n",
    "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "        model, decoder_name_or_path=model_name, tie_encoder_decoder=True,\n",
    "    )\n",
    "    \n",
    "    # Call the fit method\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        weight_decay=0,\n",
    "        scheduler=\"constantlr\",\n",
    "        optimizer_params={\"lr\": 3e-5},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def display_metrics_with_ci(history: dict):\n",
    "    # plot mean and ci for train and test r2 for all seeds and all iterations, averaged over seeds\n",
    "    seeds = list(history.keys())\n",
    "    def mean_confidence_interval(data, confidence=0.95):\n",
    "        n = len(data)\n",
    "        m, se = np.mean(data), np.std(data) / np.sqrt(n)\n",
    "        h = se * t.ppf((1 + confidence) / 2, n-1)\n",
    "        return m, m-h, m+h\n",
    "\n",
    "    r2_train_values = [history[seed]['train_r2'] for seed in seeds]\n",
    "    r2_test_values = [history[seed]['test_r2'] for seed in seeds]\n",
    "\n",
    "    r2_train_values = np.array(r2_train_values)\n",
    "    r2_test_values = np.array(r2_test_values)\n",
    "\n",
    "    r2_train_mean = np.mean(r2_train_values, axis=0)\n",
    "    r2_test_mean = np.mean(r2_test_values, axis=0)\n",
    "\n",
    "    r2_train_ci = np.array([mean_confidence_interval(r2_train_values[:, i]) for i in range(r2_train_values.shape[1])])\n",
    "    r2_test_ci = np.array([mean_confidence_interval(r2_test_values[:, i]) for i in range(r2_test_values.shape[1])])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(r2_train_mean, label='train')\n",
    "    plt.fill_between(range(len(r2_train_mean)), r2_train_ci[:, 1], r2_train_ci[:, 2], alpha=0.3)\n",
    "\n",
    "    plt.plot(r2_test_mean, label='test')\n",
    "    plt.fill_between(range(len(r2_test_mean)), r2_test_ci[:, 1], r2_test_ci[:, 2], alpha=0.3)\n",
    "    plt.title('Mean R2 by iteration, with 95% CI')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('R2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    mae_test_values = [history[seed]['test_mae'] for seed in seeds]\n",
    "    rmse_test_values = [history[seed]['test_rmse'] for seed in seeds]\n",
    "\n",
    "    mae_test_values = np.array(mae_test_values)\n",
    "    rmse_test_values = np.array(rmse_test_values)\n",
    "\n",
    "    mae_test_mean = np.mean(mae_test_values, axis=0)\n",
    "    rmse_test_mean = np.mean(rmse_test_values, axis=0)\n",
    "\n",
    "    mae_test_ci = np.array([mean_confidence_interval(mae_test_values[:, i]) for i in range(mae_test_values.shape[1])])\n",
    "    rmse_test_ci = np.array([mean_confidence_interval(rmse_test_values[:, i]) for i in range(rmse_test_values.shape[1])])\n",
    "\n",
    "    # get an index of the epoch, where the test R2 is the highest\n",
    "    # get mean and CI for this epoch\n",
    "    best_epoch = np.argmax(r2_test_mean)\n",
    "    best_epoch_r2 = r2_test_mean[best_epoch]\n",
    "    best_epoch_mae = mae_test_mean[best_epoch]\n",
    "    best_epoch_rmse = rmse_test_mean[best_epoch]\n",
    "    best_epoch_r2_ci = r2_test_ci[best_epoch]\n",
    "    best_epoch_mae_ci = mae_test_ci[best_epoch]\n",
    "    best_epoch_rmse_ci = rmse_test_ci[best_epoch]\n",
    "\n",
    "    print(f'TEST METRICS FOR THE BEST EPOCH: {best_epoch+1}')\n",
    "    print(f'R2: mean = {best_epoch_r2:.4f}, 95% CI = [{best_epoch_r2_ci[1]:.4f}, {best_epoch_r2_ci[2]:.4f}]')\n",
    "    print(f'MAE: mean = {best_epoch_mae:.4f}, 95% CI = [{best_epoch_mae_ci[1]:.4f}, {best_epoch_mae_ci[2]:.4f}]')\n",
    "    print(f'RMSE: mean = {best_epoch_rmse:.4f}, 95% CI = [{best_epoch_rmse_ci[1]:.4f}, {best_epoch_rmse_ci[2]:.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning-related classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "    \n",
    "\n",
    "# Dataset and DataLoader\n",
    "class SalaryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with [MASK] token-based regression and MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with multitask learning and MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-eval loop with experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define text feature/target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col_1 = 'description_no_numbers'\n",
    "\n",
    "text_col_2 = 'title_company_location_skills_source' # Merged text column, second feature\n",
    "\n",
    "target_col = 'log_salary_from' # regression target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create merged title/skills/location/source feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['skills'].fillna('Не указаны', inplace=True)\n",
    "\n",
    "title_company_location_skills_feature_template = \"\"\"\n",
    "Позиция: {position}\n",
    "Компания: {company}\n",
    "Место: {location}\n",
    "Навыки: {skills}\n",
    "Источник: {source}\n",
    "\"\"\"\n",
    "\n",
    "df['title_company_location_skills_source'] = df.apply(lambda x: title_company_location_skills_feature_template.format(\n",
    "    position=x['title'],\n",
    "    company=x['company'],\n",
    "    location=x['location'],\n",
    "    skills=x['skills'],\n",
    "    source=x['source']\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:44.844772Z",
     "iopub.status.busy": "2024-12-16T09:03:44.844155Z",
     "iopub.status.idle": "2024-12-16T09:03:45.218052Z",
     "shell.execute_reply": "2024-12-16T09:03:45.217364Z",
     "shell.execute_reply.started": "2024-12-16T09:03:44.844739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1: Single BERT, MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:45.256720Z",
     "iopub.status.busy": "2024-12-16T09:03:45.256157Z",
     "iopub.status.idle": "2024-12-16T09:03:48.121783Z",
     "shell.execute_reply": "2024-12-16T09:03:48.120485Z",
     "shell.execute_reply.started": "2024-12-16T09:03:45.256690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for seed 42...\n",
      "double_huber_multitask model...\n",
      "Creating the dataset...\n",
      "Creating the dataset...\n",
      "Starting training/eval loop...\n",
      "Starting training...\n",
      "3\n",
      "32\n",
      "tensor([83835, 83836, 83836, 83836, 83836, 83836, 83833, 83836, 83836, 83835,\n",
      "        83835, 83835, 83834, 83836, 83836, 83835], device='cuda:0')\n",
      "mask_embedding shape: torch.Size([11, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n",
      "tensor([83835, 83836, 83836, 83836, 83837, 83837, 83836, 83836, 83836, 83833,\n",
      "        83836, 83836, 83836, 83835, 83835, 83837], device='cuda:1')\n",
      "mask_embedding shape: torch.Size([5, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_eval_with_bins\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_targets_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatboost_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_col_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_col_1_with_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m memory_cleanup()\n\u001b[1;32m    106\u001b[0m combined_history[seed][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m history    \n",
      "Cell \u001b[0;32mIn[17], line 239\u001b[0m, in \u001b[0;36mfit_eval_with_bins\u001b[0;34m(seed, model, X_train, X_test, y_train, y_test, bin_targets_col, catboost_preds, criterion, tokenizer, config, text_col_1, text_col_2)\u001b[0m\n\u001b[1;32m    237\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Forward pass: include bin_ids\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m salary_output, similarity_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(salary_output, targets) \u001b[38;5;241m+\u001b[39m similarity_loss  \u001b[38;5;66;03m# Combine regression loss and similarity loss\u001b[39;00m\n\u001b[1;32m    241\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Data preparation\n",
    "X = df_new2['total'].values\n",
    "y = df_new2['mean_salary_all'].values.astype(int)\n",
    "\n",
    "# Tokenizer replacement using PyTorch\n",
    "def tokenize_texts(texts, vocab=None):\n",
    "    if vocab is None:\n",
    "        vocab = Counter(word for text in texts for word in text.split())\n",
    "    word2idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "    tokenized = [[word2idx[word] for word in text.split() if word in word2idx] for text in texts]\n",
    "    return tokenized, word2idx\n",
    "\n",
    "def pad_sequences(sequences, max_len):\n",
    "    return np.array([seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenized_texts, vocab = tokenize_texts(X)\n",
    "vocabulary_size = len(vocab) + 1  # Add 1 for padding token\n",
    "row_max_length = max(len(seq) for seq in tokenized_texts)\n",
    "X_padded = pad_sequences(tokenized_texts, max_len=row_max_length)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=10)\n",
    "\n",
    "\n",
    "train_dataset = SalaryDataset(X_train, y_train)\n",
    "test_dataset = SalaryDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "class BiGRUCNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_length):\n",
    "        super(BiGRUCNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.bigru = nn.GRU(embed_dim, 128, bidirectional=True, batch_first=True)\n",
    "        self.conv1d = nn.Conv1d(256, 64, kernel_size=3, padding='same')\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(64 * (max_length // 2), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        gru_out, _ = self.bigru(x)\n",
    "        conv_out = self.conv1d(gru_out.permute(0, 2, 1))\n",
    "        pooled_out = self.pool(conv_out)\n",
    "        flat_out = self.flatten(pooled_out)\n",
    "        drop_out = self.dropout(flat_out)\n",
    "        output = self.fc(drop_out)\n",
    "        return output\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "embed_dim = 256\n",
    "model = BiGRUCNNModel(vocab_size=vocabulary_size, embed_dim=embed_dim, max_length=row_max_length)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X).squeeze()\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_pred, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        predictions = model(batch_X).squeeze()\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "        y_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R2 Score: {round(r2 * 100, 2)}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
