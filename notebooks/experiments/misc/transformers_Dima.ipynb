{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Embeddings from pre-trained transformers + MLP, combine (or not) with catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code from task 2 to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def average_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def pool(hidden_state, mask, pooling_method=\"cls\"):\n",
    "    if pooling_method == \"mean\":\n",
    "        s = torch.sum(hidden_state * mask.unsqueeze(-1).float(), dim=1)\n",
    "        d = mask.sum(axis=1, keepdim=True).float()\n",
    "        return s / d\n",
    "    elif pooling_method == \"cls\":\n",
    "        return hidden_state[:, 0]\n",
    "\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)\n",
    "model_name = \"sismetanin/sbert-ru-sentiment-rureviews\"\n",
    "# Load AutoModel from huggingface model repository\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "# model = AutoModel.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ru-en-RoSBERTa\")\n",
    "# model = AutoModel.from_pretrained(\"ai-forever/ru-en-RoSBERTa\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"deepvk/USER-bge-m3\")\n",
    "# model = AutoModel.from_pretrained(\"deepvk/USER-bge-m3\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\n",
    "# model = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Check if multiple GPUs are available and use DataParallel\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "class FiveDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_seq_len):\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['text'].tolist()\n",
    "        # self.text = dataframe['text_with_instruct'].tolist()\n",
    "        self.targets = dataframe['rate'].tolist() if 'rate' in dataframe else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        # text = ' '.join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        if self.targets is not None:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text)\n",
    "\n",
    "# Assuming train_split, val_split, and test_data are already defined\n",
    "# train_dataset = FiveDataset(train_data_no_duplicates, tokenizer, MAX_LEN)\n",
    "train_dataset = FiveDataset(train_data, tokenizer, MAX_LEN)\n",
    "test_dataset = FiveDataset(test_data, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to compute and save pooled embeddings\n",
    "def compute_and_save_embeddings(dataloader, model, device):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_embeddings = mean_pooling(outputs, attention_mask)  # Ensure attention_mask is on the same device\n",
    "            cls_embeddings = outputs[0][:, 0]\n",
    "            pooled_embeddings = torch.cat((pooled_embeddings, cls_embeddings), 1)\n",
    "            \n",
    "            # pooled_embeddings = pool(outputs.last_hidden_state, attention_mask)  # Ensure attention_mask is on the same device\n",
    "            # pooled_embeddings = outputs[0][:, 0]\n",
    "            # pooled_embeddings = average_pool(outputs.last_hidden_state, attention_mask)  # Ensure attention_mask is on the same device\n",
    "            \n",
    "            embeddings.append(pooled_embeddings.cpu().numpy())\n",
    "            if 'targets' in batch:\n",
    "                labels.append(batch['targets'].cpu().numpy())\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.hstack(labels) if labels else None\n",
    "    return embeddings, labels\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# name = 'e5_instr_en_full'\n",
    "# name = 'USER-bge-m3_full'\n",
    "model_name = model_name.replace('/', '_')\n",
    "train_embeddings, train_labels = compute_and_save_embeddings(train_dataloader, model, device)\n",
    "np.save(f'train_embeddings_{model_name}.npy', train_embeddings)\n",
    "np.save(f'train_labels_{model_name}.npy', train_labels)\n",
    "test_embeddings, test_labels = compute_and_save_embeddings(test_dataloader, model, device)\n",
    "\n",
    "# Save embeddings and labels\n",
    "np.save(f'test_embeddings_{model_name}.npy', test_embeddings)\n",
    "np.save(f'test_labels_{model_name}.npy', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code by GPT for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ed1d07bf174695a9114260e4c14989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c287b1dc76b64ed5bf6d9870ea3ddb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812a1249dead4c9299b62e5304fc4f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46b7ae2d77e44b98f2988eed4cf413e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcbddf5207547a6af83106ba38f7041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97417c0e4cf1478ca0d696ab7b2f7377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[246], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[1;32m     69\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 70\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/transformers/modeling_utils.py:3825\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3809\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3811\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3824\u001b[0m     }\n\u001b[0;32m-> 3825\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3827\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3828\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3830\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/huggingface_hub/file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/huggingface_hub/file_download.py:1545\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1543\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1545\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1554\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1555\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/huggingface_hub/file_download.py:454\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    452\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 256  # Maximum sequence length\n",
    "BATCH_SIZE = 32  # Batch size\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# CLS Pooling - Use when only the sentence embedding is needed\n",
    "def cls_pooling(hidden_state, mask) -> torch.Tensor:\n",
    "    return hidden_state[:, 0]\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class JobDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_seq_len: int, text_column: str):\n",
    "        self.data = dataframe\n",
    "        self.texts = dataframe[text_column].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict:\n",
    "        text = str(self.texts[index])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Compute embeddings\n",
    "def compute_embeddings(dataloader: DataLoader, model, device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mean_embeddings = []\n",
    "    cls_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Compute mean and CLS embeddings\n",
    "            mean_emb = mean_pooling(outputs, attention_mask)\n",
    "            cls_emb = cls_pooling(outputs)\n",
    "            \n",
    "            mean_embeddings.append(mean_emb.cpu().numpy())\n",
    "            cls_embeddings.append(cls_emb.cpu().numpy())\n",
    "\n",
    "    return np.vstack(mean_embeddings), np.vstack(cls_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df.sample(100, random_state=42)\n",
    "# Prepare data\n",
    "X = df_short[['description', 'title_company_location_skills_source']]\n",
    "y = df_short['log_salary_from']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Description embeddings\n",
    "train_description_dataset = JobDataset(X_train, tokenizer, MAX_LEN, text_column='description')\n",
    "test_description_dataset = JobDataset(X_test, tokenizer, MAX_LEN, text_column='description')\n",
    "train_description_loader = DataLoader(train_description_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_description_loader = DataLoader(test_description_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Title+Company+Location+Skills+Source embeddings\n",
    "train_title_dataset = JobDataset(X_train, tokenizer, MAX_LEN, text_column='title_company_location_skills_source')\n",
    "test_title_dataset = JobDataset(X_test, tokenizer, MAX_LEN, text_column='title_company_location_skills_source')\n",
    "train_title_loader = DataLoader(train_title_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_title_loader = DataLoader(test_title_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Compute embeddings\n",
    "train_mean_desc, train_cls_desc = compute_embeddings(train_description_loader, model, device)\n",
    "test_mean_desc, test_cls_desc = compute_embeddings(test_description_loader, model, device)\n",
    "train_mean_title, train_cls_title = compute_embeddings(train_title_loader, model, device)\n",
    "test_mean_title, test_cls_title = compute_embeddings(test_title_loader, model, device)\n",
    "\n",
    "# Combine embeddings\n",
    "train_embeddings = np.hstack([train_mean_desc, train_cls_desc, train_mean_title, train_cls_title])\n",
    "test_embeddings = np.hstack([test_mean_desc, test_cls_desc, test_mean_title, test_cls_title])\n",
    "\n",
    "# Save embeddings\n",
    "np.save('train_embeddings.npy', train_embeddings)\n",
    "np.save('test_embeddings.npy', test_embeddings)\n",
    "np.save('y_train.npy', y_train.values)\n",
    "np.save('y_test.npy', y_test.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning transformer + MLP head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two berts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = 512\n",
    "hidden_size = 768  # BERT base hidden size\n",
    "mlp_hidden_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        cls1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "text_col_1 = 'description_no_numbers_v2'\n",
    "text_col_2 = 'title_company_location_skills_source'\n",
    "X = df[[text_col_1, text_col_2]]\n",
    "y = df['log_salary_from']\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train, tokenizer, seq_length)\n",
    "test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test, tokenizer, seq_length)\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = DualBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()  # For regression\n",
    "\n",
    "# Enhanced Training and Evaluation Loop\n",
    "history = {\"train_loss\": [],\n",
    "           \"test_loss\": [], \n",
    "           \"train_r2\": [],\n",
    "           \"test_r2\": [],\n",
    "           \"max_test_r2\": float('-inf'),\n",
    "           \"best_preds\": []\n",
    "           }\n",
    "\n",
    "test_labels = y_test\n",
    "test_preds = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in train_dataloader:\n",
    "        inputs1, inputs2, targets = batch\n",
    "        input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "        input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_r2 = r2_score(all_labels, all_preds)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_r2\"].append(train_r2)\n",
    "\n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            test_losses.append(loss.item())\n",
    "            all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"test_r2\"].append(test_r2)\n",
    "\n",
    "    if test_r2 > history[\"max_test_r2\"]:\n",
    "        history[\"max_test_r2\"] = test_r2\n",
    "        history[\"best_preds\"] = all_preds\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "    torch.save(model.state_dict(), f'./models/model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 Score\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_r2\"], label=\"Train R2\")\n",
    "plt.plot(history[\"test_r2\"], label=\"Test R2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.title(\"Train/Test R2 Score Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"./models/final_model.pth\")\n",
    "\n",
    "# save history as pickle\n",
    "import pickle\n",
    "\n",
    "with open('./models/history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "# last run:\n",
    "# Epoch 1/10, Train Loss: 0.2170, Test Loss: 0.1120, Test R2: 0.7158\n",
    "# Epoch 2/10, Train Loss: 0.1062, Test Loss: 0.1018, Test R2: 0.7416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = 512\n",
    "hidden_size = 768  # BERT base hidden size\n",
    "mlp_hidden_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        print(f\"len(self.tokenized_texts1['input_ids']): {len(self.tokenized_texts1['input_ids'])}\")\n",
    "        print(f\"len(self.tokenized_texts2['input_ids']): {len(self.tokenized_texts2['input_ids'])}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "\n",
    "\n",
    "class SingleBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(SingleBERTWithMLP, self).__init__()\n",
    "        # Initialize a single BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),  # Double hidden size for concatenation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Pass both inputs through the same BERT model\n",
    "        cls1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token for input1\n",
    "        cls2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token for input2\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "text_col_1 = 'description_no_numbers_v2'\n",
    "text_col_2 = 'title_company_location_skills_source'\n",
    "X = df[[text_col_1, text_col_2]]\n",
    "y = df['log_salary_from']\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train, tokenizer, seq_length)\n",
    "test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test, tokenizer, seq_length)\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = SingleBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()  # For regression\n",
    "\n",
    "# Enhanced Training and Evaluation Loop\n",
    "history = {\"train_loss\": [],\n",
    "           \"test_loss\": [], \n",
    "           \"train_r2\": [],\n",
    "           \"test_r2\": [],\n",
    "           \"max_test_r2\": float('-inf'),\n",
    "           \"best_preds\": []\n",
    "           }\n",
    "\n",
    "test_labels = y_test\n",
    "test_preds = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in train_dataloader:\n",
    "        inputs1, inputs2, targets = batch\n",
    "        input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "        input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_r2 = r2_score(all_labels, all_preds)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_r2\"].append(train_r2)\n",
    "\n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            test_losses.append(loss.item())\n",
    "            all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"test_r2\"].append(test_r2)\n",
    "\n",
    "    if test_r2 > history[\"max_test_r2\"]:\n",
    "        history[\"max_test_r2\"] = test_r2\n",
    "        history[\"best_preds\"] = all_preds\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "    torch.save(model.state_dict(), f'./models/model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 Score\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_r2\"], label=\"Train R2\")\n",
    "plt.plot(history[\"test_r2\"], label=\"Test R2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.title(\"Train/Test R2 Score Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"./models/final_model.pth\")\n",
    "\n",
    "# save history as pickle\n",
    "import pickle\n",
    "\n",
    "with open('./models/history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "# last run:\n",
    "# Epoch 1/10, Train Loss: 0.2170, Test Loss: 0.1120, Test R2: 0.7158\n",
    "# Epoch 2/10, Train Loss: 0.1062, Test Loss: 0.1018, Test R2: 0.7416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two berts + Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = 512\n",
    "hidden_size = 768  # BERT base hidden size\n",
    "mlp_hidden_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        cls1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "text_col_1 = 'description_no_numbers_v2'\n",
    "text_col_2 = 'title_company_location_skills_source'\n",
    "X = df[[text_col_1, text_col_2]]\n",
    "y = df['log_salary_from']\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train, tokenizer, seq_length)\n",
    "test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test, tokenizer, seq_length)\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = DualBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "# Enhanced Training and Evaluation Loop\n",
    "history = {\"train_loss\": [],\n",
    "           \"test_loss\": [], \n",
    "           \"train_r2\": [],\n",
    "           \"test_r2\": [],\n",
    "           \"max_test_r2\": float('-inf'),\n",
    "           \"best_preds\": []\n",
    "           }\n",
    "\n",
    "test_labels = y_test\n",
    "test_preds = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in train_dataloader:\n",
    "        inputs1, inputs2, targets = batch\n",
    "        input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "        input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_r2 = r2_score(all_labels, all_preds)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_r2\"].append(train_r2)\n",
    "\n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            test_losses.append(loss.item())\n",
    "            all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"test_r2\"].append(test_r2)\n",
    "\n",
    "    if test_r2 > history[\"max_test_r2\"]:\n",
    "        history[\"max_test_r2\"] = test_r2\n",
    "        history[\"best_preds\"] = all_preds\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "    torch.save(model.state_dict(), f'./models/model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 Score\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_r2\"], label=\"Train R2\")\n",
    "plt.plot(history[\"test_r2\"], label=\"Test R2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.title(\"Train/Test R2 Score Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"./models/final_model.pth\")\n",
    "\n",
    "# save history as pickle\n",
    "import pickle\n",
    "\n",
    "with open('./models/history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "# last run:\n",
    "# Epoch 1/10, Train Loss: 0.2170, Test Loss: 0.1120, Test R2: 0.7158\n",
    "# Epoch 2/10, Train Loss: 0.1062, Test Loss: 0.1018, Test R2: 0.7416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two berts + Mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "seq_length = 512\n",
    "hidden_size = 768  # BERT base hidden size\n",
    "mlp_hidden_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        output1 = self.bert1(input_ids=input1, attention_mask=attention_mask1)\n",
    "        mean_embedding_1 = mean_pooling(output1, attention_mask1)\n",
    "\n",
    "        # Forward pass through BERT1\n",
    "        output2 = self.bert2(input_ids=input2, attention_mask=attention_mask2)\n",
    "        mean_embedding_2 = mean_pooling(output2, attention_mask2)\n",
    "\n",
    "\n",
    "        # Concatenate mean embeddings\n",
    "        combined_mean_embeddings = torch.cat([mean_embedding_1, mean_embedding_2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_mean_embeddings)\n",
    "        return output\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "text_col_1 = 'description_no_numbers_v2'\n",
    "text_col_2 = 'title_company_location_skills_source'\n",
    "X = df[[text_col_1, text_col_2]]\n",
    "y = df['log_salary_from']\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Make datasets\n",
    "train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train, tokenizer, seq_length)\n",
    "test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test, tokenizer, seq_length)\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = DualBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()  # For regression\n",
    "\n",
    "# Enhanced Training and Evaluation Loop\n",
    "history = {\"train_loss\": [],\n",
    "           \"test_loss\": [], \n",
    "           \"train_r2\": [],\n",
    "           \"test_r2\": [],\n",
    "           \"max_test_r2\": float('-inf'),\n",
    "           \"best_preds\": []\n",
    "           }\n",
    "\n",
    "test_labels = y_test\n",
    "test_preds = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in train_dataloader:\n",
    "        inputs1, inputs2, targets = batch\n",
    "        input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "        input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_r2 = r2_score(all_labels, all_preds)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_r2\"].append(train_r2)\n",
    "\n",
    "    # Evaluation Phase\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            test_losses.append(loss.item())\n",
    "            all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_loss = np.mean(test_losses)\n",
    "    test_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"test_r2\"].append(test_r2)\n",
    "\n",
    "    if test_r2 > history[\"max_test_r2\"]:\n",
    "        history[\"max_test_r2\"] = test_r2\n",
    "        history[\"best_preds\"] = all_preds\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "    torch.save(model.state_dict(), f'./models/model_epoch_{epoch + 1}.pth')\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Test Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot R2 Score\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_r2\"], label=\"Train R2\")\n",
    "plt.plot(history[\"test_r2\"], label=\"Test R2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.title(\"Train/Test R2 Score Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"./models/final_model.pth\")\n",
    "\n",
    "# save history as pickle\n",
    "import pickle\n",
    "\n",
    "with open('./models/history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)\n",
    "\n",
    "# last run:\n",
    "# Epoch 1/10, Train Loss: 0.2170, Test Loss: 0.1120, Test R2: 0.7158\n",
    "# Epoch 2/10, Train Loss: 0.1062, Test Loss: 0.1018, Test R2: 0.7416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on the [MASK] token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSDAE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
