{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:03.810629Z",
     "iopub.status.busy": "2024-12-16T09:01:03.810343Z",
     "iopub.status.idle": "2024-12-16T09:01:16.186941Z",
     "shell.execute_reply": "2024-12-16T09:01:16.185958Z",
     "shell.execute_reply.started": "2024-12-16T09:01:03.810595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-16 09:01:06--  https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.202.132, 2607:f8b0:4001:c06::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.202.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 295792946 (282M) [application/octet-stream]\n",
      "Saving to: './data/dataset.csv'\n",
      "\n",
      "./data/dataset.csv  100%[===================>] 282.09M   153MB/s    in 1.8s    \n",
      "\n",
      "2024-12-16 09:01:10 (153 MB/s) - './data/dataset.csv' saved [295792946/295792946]\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22224 entries, 0 to 22223\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   title                                 22224 non-null  object \n",
      " 1   company                               22224 non-null  object \n",
      " 2   location                              22224 non-null  object \n",
      " 3   skills                                14384 non-null  object \n",
      " 4   source                                22224 non-null  object \n",
      " 5   description_no_numbers_with_skills    22224 non-null  object \n",
      " 6   experience_from                       22224 non-null  float64\n",
      " 7   experience_to_adjusted_10             22224 non-null  float64\n",
      " 8   description_size                      22224 non-null  int64  \n",
      " 9   description                           22224 non-null  object \n",
      " 10  description_no_numbers                22224 non-null  object \n",
      " 11  description_no_numbers_v2             22224 non-null  object \n",
      " 12  title_company_location_skills_source  22224 non-null  object \n",
      " 13  salary_from                           22224 non-null  float64\n",
      " 14  log_salary_from                       22224 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(10)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:16.189393Z",
     "iopub.status.busy": "2024-12-16T09:01:16.188980Z",
     "iopub.status.idle": "2024-12-16T09:01:19.663789Z",
     "shell.execute_reply": "2024-12-16T09:01:19.662565Z",
     "shell.execute_reply.started": "2024-12-16T09:01:16.189349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-16 09:01:17--  https://drive.usercontent.google.com/download?id=1gbA1owa-9aTxTaGABpTffGvE8y31qHze&export=download&authuser=1&confirm=t\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.202.132, 2607:f8b0:4001:c06::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.202.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 428138 (418K) [application/octet-stream]\n",
      "Saving to: './data/best_catboost_model_train_eval_history.pickle'\n",
      "\n",
      "./data/best_catboos 100%[===================>] 418.10K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2024-12-16 09:01:19 (70.0 MB/s) - './data/best_catboost_model_train_eval_history.pickle' saved [428138/428138]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1gbA1owa-9aTxTaGABpTffGvE8y31qHze&export=download&authuser=1&confirm=t' -O './data/best_catboost_model_train_eval_history.pickle'\n",
    "\n",
    "import pickle\n",
    "\n",
    "file = './data/best_catboost_model_train_eval_history.pickle'\n",
    "with open(file, 'rb') as f:\n",
    "    catboost_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:19.666074Z",
     "iopub.status.busy": "2024-12-16T09:01:19.665690Z",
     "iopub.status.idle": "2024-12-16T09:01:30.723032Z",
     "shell.execute_reply": "2024-12-16T09:01:30.722058Z",
     "shell.execute_reply.started": "2024-12-16T09:01:19.666034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:30.725630Z",
     "iopub.status.busy": "2024-12-16T09:01:30.725301Z",
     "iopub.status.idle": "2024-12-16T09:02:03.249282Z",
     "shell.execute_reply": "2024-12-16T09:02:03.248529Z",
     "shell.execute_reply.started": "2024-12-16T09:01:30.725601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"Clean up memory\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    \"Get number of words in each sentence in the text\"\n",
    "    # pattern = r'(?<=[.!?])\\s+'\n",
    "    pattern = r'(?<=[.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    # remove empty strings\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    # get number of words in each sentence\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    return sentences, sentence_lengths\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"Set seed for reproducibility\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_tsdae_bert(model_name, train_sentences):\n",
    "    \"\"\"Train a denoising auto-encoder model with BERT model.\n",
    "    more examples at https://sbert.net/examples/unsupervised_learning/TSDAE/README.html\"\"\"\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # Create the special denoising dataset that adds noise on-the-fly\n",
    "    train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "    \n",
    "    # DataLoader to batch your data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Use the denoising auto-encoder loss\n",
    "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "        model, decoder_name_or_path=model_name, tie_encoder_decoder=True,\n",
    "    )\n",
    "    \n",
    "    # Call the fit method\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        weight_decay=0,\n",
    "        scheduler=\"constantlr\",\n",
    "        optimizer_params={\"lr\": 3e-5},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m t\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m t\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/ods-final-project-salary/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def display_metrics_with_ci(history: dict):\n",
    "    # plot mean and ci for train and test r2 for all seeds and all iterations, averaged over seeds\n",
    "    seeds = list(history.keys())\n",
    "    def mean_confidence_interval(data, confidence=0.95):\n",
    "        n = len(data)\n",
    "        m, se = np.mean(data), np.std(data) / np.sqrt(n)\n",
    "        h = se * t.ppf((1 + confidence) / 2, n-1)\n",
    "        return m, m-h, m+h\n",
    "\n",
    "    r2_train_values = [history[seed]['train_r2'] for seed in seeds]\n",
    "    r2_test_values = [history[seed]['test_r2'] for seed in seeds]\n",
    "\n",
    "    r2_train_values = np.array(r2_train_values)\n",
    "    r2_test_values = np.array(r2_test_values)\n",
    "\n",
    "    r2_train_mean = np.mean(r2_train_values, axis=0)\n",
    "    r2_test_mean = np.mean(r2_test_values, axis=0)\n",
    "\n",
    "    r2_train_ci = np.array([mean_confidence_interval(r2_train_values[:, i]) for i in range(r2_train_values.shape[1])])\n",
    "    r2_test_ci = np.array([mean_confidence_interval(r2_test_values[:, i]) for i in range(r2_test_values.shape[1])])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(r2_train_mean, label='train')\n",
    "    plt.fill_between(range(len(r2_train_mean)), r2_train_ci[:, 1], r2_train_ci[:, 2], alpha=0.3)\n",
    "\n",
    "    plt.plot(r2_test_mean, label='test')\n",
    "    plt.fill_between(range(len(r2_test_mean)), r2_test_ci[:, 1], r2_test_ci[:, 2], alpha=0.3)\n",
    "    plt.title('Mean R2 by iteration, with 95% CI')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('R2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    mae_test_values = [history[seed]['test_mae'] for seed in seeds]\n",
    "    rmse_test_values = [history[seed]['test_rmse'] for seed in seeds]\n",
    "\n",
    "    mae_test_values = np.array(mae_test_values)\n",
    "    rmse_test_values = np.array(rmse_test_values)\n",
    "\n",
    "    mae_test_mean = np.mean(mae_test_values, axis=0)\n",
    "    rmse_test_mean = np.mean(rmse_test_values, axis=0)\n",
    "\n",
    "    mae_test_ci = np.array([mean_confidence_interval(mae_test_values[:, i]) for i in range(mae_test_values.shape[1])])\n",
    "    rmse_test_ci = np.array([mean_confidence_interval(rmse_test_values[:, i]) for i in range(rmse_test_values.shape[1])])\n",
    "\n",
    "    # get an index of the epoch, where the test R2 is the highest\n",
    "    # get mean and CI for this epoch\n",
    "    best_epoch = np.argmax(r2_test_mean)\n",
    "    best_epoch_r2 = r2_test_mean[best_epoch]\n",
    "    best_epoch_mae = mae_test_mean[best_epoch]\n",
    "    best_epoch_rmse = rmse_test_mean[best_epoch]\n",
    "    best_epoch_r2_ci = r2_test_ci[best_epoch]\n",
    "    best_epoch_mae_ci = mae_test_ci[best_epoch]\n",
    "    best_epoch_rmse_ci = rmse_test_ci[best_epoch]\n",
    "\n",
    "    print(f'TEST METRICS FOR THE BEST EPOCH: {best_epoch+1}')\n",
    "    print(f'R2: mean = {best_epoch_r2:.4f}, 95% CI = [{best_epoch_r2_ci[1]:.4f}, {best_epoch_r2_ci[2]:.4f}]')\n",
    "    print(f'MAE: mean = {best_epoch_mae:.4f}, 95% CI = [{best_epoch_mae_ci[1]:.4f}, {best_epoch_mae_ci[2]:.4f}]')\n",
    "    print(f'RMSE: mean = {best_epoch_rmse:.4f}, 95% CI = [{best_epoch_rmse_ci[1]:.4f}, {best_epoch_rmse_ci[2]:.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = '../data/history/transfomers_single_bert_mse.pickle'\n",
    "\n",
    "with open(file, 'rb') as f:\n",
    "    history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 78687, 123123]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(history.keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_loss',\n",
       " 'test_loss',\n",
       " 'train_rmse',\n",
       " 'test_rmse',\n",
       " 'train_r2',\n",
       " 'test_r2',\n",
       " 'train_mae',\n",
       " 'test_mae',\n",
       " 'y_pred',\n",
       " 'y_test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(history[42].keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n",
      "10\n",
      "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "(10, 40)\n",
      "Seed: 78687\n",
      "10\n",
      "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "(10, 40)\n",
      "Seed: 123123\n",
      "10\n",
      "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "(10, 40)\n"
     ]
    }
   ],
   "source": [
    "for seed, h in history.items():\n",
    "    print(f'Seed: {seed}')\n",
    "    print(len(h['y_pred']))\n",
    "    print([len(_) for _ in h['y_pred']])\n",
    "    arr = np.array(h['y_pred'])\n",
    "    print(arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history[42]['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(_) for _ in history[42]['y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-307.69464111328125, -301.8771057128906, -296.1627502441406, -290.5736999511719, -285.07257080078125, -279.64605712890625, -274.1941223144531, -268.7493591308594, -263.2622985839844, -257.6695556640625]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.array(history[42]['y_pred'])\n",
    "y_test = np.array(history[42]['y_test'])\n",
    "\n",
    "# get r2 for each epoch, use sklearn r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_scores = [r2_score(test, pred) for test, pred in zip(y_test, y_pred)]\n",
    "print(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92431636 0.71239403]\n",
      " [0.70296566 0.18266305]\n",
      " [0.60341758 0.38912301]\n",
      " [0.60290881 0.57907253]\n",
      " [0.95463751 0.12857726]\n",
      " [0.11054136 0.92209265]\n",
      " [0.89703852 0.90174168]\n",
      " [0.14938455 0.46916605]\n",
      " [0.25274267 0.31140565]\n",
      " [0.97926187 0.08863297]\n",
      " [0.0651928  0.04992249]\n",
      " [0.28021641 0.05312062]\n",
      " [0.56338374 0.11376246]\n",
      " [0.33407722 0.27244644]\n",
      " [0.80312417 0.42667398]\n",
      " [0.13258218 0.39507633]\n",
      " [0.11786011 0.95825117]\n",
      " [0.25698594 0.05218506]\n",
      " [0.21623289 0.06947998]\n",
      " [0.75081138 0.26554418]]\n"
     ]
    }
   ],
   "source": [
    "# generate two arrays, n_rows = 1000, n_cols = 8 and 2\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 10)\n",
    "y = np.random.rand(1000, 2)\n",
    "\n",
    "# do a train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_test.shape\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92431636 0.70296566 0.60341758 0.60290881 0.95463751 0.11054136\n",
      " 0.89703852 0.14938455 0.25274267 0.97926187 0.0651928  0.28021641\n",
      " 0.56338374 0.33407722 0.80312417 0.13258218 0.11786011 0.25698594\n",
      " 0.21623289 0.75081138]\n"
     ]
    }
   ],
   "source": [
    "# generate two arrays, n_rows = 1000, n_cols = 8 and 2\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 10)\n",
    "y = np.random.rand(1000, 2)\n",
    "\n",
    "# do a train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y[:, 0], test_size=0.2, random_state=42)\n",
    "\n",
    "y_test.shape\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71239403 0.18266305 0.38912301 0.57907253 0.12857726 0.92209265\n",
      " 0.90174168 0.46916605 0.31140565 0.08863297 0.04992249 0.05312062\n",
      " 0.11376246 0.27244644 0.42667398 0.39507633 0.95825117 0.05218506\n",
      " 0.06947998 0.26554418]\n"
     ]
    }
   ],
   "source": [
    "# generate two arrays, n_rows = 1000, n_cols = 8 and 2\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 10)\n",
    "y = np.random.rand(1000, 2)\n",
    "\n",
    "# do a train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y[:, 1], test_size=0.2, random_state=42)\n",
    "\n",
    "y_test.shape\n",
    "print(y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history[42]['y_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 40, 40, 40, 40, 40, 40, 40, 40, 40]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(_) for _ in history[42]['y_test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 40)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array(history[42]['y_test'])\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning-related classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset for multitask learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTextDatasetWithBins(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets_reg, targets_bin, tokenizer, max_len):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Targets\n",
    "        self.targets_reg = targets_reg.tolist()  # Log salary target for regression\n",
    "        self.targets_bin = targets_bin.tolist()  # Bin IDs for mask token prediction\n",
    "        \n",
    "        # Get the bin token ID offset from the tokenizer's vocabulary\n",
    "        self.bin_token_offset = tokenizer.convert_tokens_to_ids(\"[BIN_0]\")  # This will give the token ID of the first bin token\n",
    "        self.bin_to_id_mapping = {i: torch.tensor(i + self.bin_token_offset, dtype=torch.long) for i in range(15)}\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets_reg)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets_reg[idx], dtype=torch.float)\n",
    "        \n",
    "        # Map the bin_targets (0 to 14) to the corresponding token IDs\n",
    "        # bin_target = torch.tensor(self.bin_targets[idx] + self.bin_token_offset, dtype=torch.long)  # Add offset\n",
    "        bin_target = self.bin_to_id_mapping[self.targets_bin[idx]]\n",
    "        \n",
    "        return inputs1, inputs2, target, bin_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single-head BERT with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# single bert for two text features with MLP head for regression\n",
    "class SingleBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SingleBERTWithMLP, self).__init__()\n",
    "        model_name = config[\"model_name\"]\n",
    "        # Initialize a single BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        mlp_hidden_size = config[\"mlp_hidden_size\"]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),  # Double hidden size for concatenation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Pass both inputs through the same BERT model\n",
    "        cls1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token for input1\n",
    "        cls2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token for input2\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two berts for two text features with MLP head for regression\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        cls1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with [MASK] token-based regression and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two berts for two text features with MLP head for regression over MASK token embedding\n",
    "class MASKPoolDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MASKPoolDualBERTWithMLP, self).__init__()\n",
    "        model_name = config['model_name']\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mask_token_index = config['mask_token_index']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        mask1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, self.mask_token_index, :]  # mask token \n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        mask2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, self.mask_token_index, :]  # mask token\n",
    "\n",
    "        # concat mask embeddings\n",
    "        combined_mask = torch.cat([mask1, mask2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with TSDAE pre-tuning and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:02:03.250804Z",
     "iopub.status.busy": "2024-12-16T09:02:03.250242Z",
     "iopub.status.idle": "2024-12-16T09:02:03.270324Z",
     "shell.execute_reply": "2024-12-16T09:02:03.269478Z",
     "shell.execute_reply.started": "2024-12-16T09:02:03.250775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TSDAEDualBERTWithMLP(nn.Module):\n",
    "    \"\"\"two berts for two text features with MLP head for regression.\n",
    "    The model is pre-tuned with TSDAE.\"\"\"\n",
    "    def __init__(self, config, bert1, bert2):\n",
    "        super(TSDAEDualBERTWithMLP, self).__init__()\n",
    "        # Load TSDAE-ed BERT models\n",
    "        self.bert1 = bert1\n",
    "        self.bert2 = bert2\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # idea from: https://github.com/UKPLab/sentence-transformers/issues/2494\n",
    "        # Forward pass through BERT1\n",
    "        input_dict1 = {\n",
    "            'input_ids': input1,\n",
    "            'attention_mask': attention_mask1\n",
    "        }\n",
    "        cls1 = self.bert1(input_dict1)['sentence_embedding']\n",
    "        \n",
    "        # Forward pass through BERT2\n",
    "        input_dict2 = {\n",
    "            'input_ids': input2,\n",
    "            'attention_mask': attention_mask2\n",
    "        }\n",
    "        cls2 = self.bert2(input_dict2)['sentence_embedding']\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with multitask learning and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:02:03.272111Z",
     "iopub.status.busy": "2024-12-16T09:02:03.271744Z",
     "iopub.status.idle": "2024-12-16T09:02:03.307058Z",
     "shell.execute_reply": "2024-12-16T09:02:03.306457Z",
     "shell.execute_reply.started": "2024-12-16T09:02:03.272070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# two berts for two text features with MLP head for regression\n",
    "class MaskBinDualBERTWithMLP(nn.Module):\n",
    "    \"\"\"two berts for two text features with regression over CLS token\n",
    "    and cosine similarity loss between MASK token and bin embeddings.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(MaskBinDualBERTWithMLP, self).__init__()\n",
    "\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Add new bin tokens to the tokenizer and resize the model embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        new_tokens = [\"[BIN_0]\", \"[BIN_1]\", \"[BIN_2]\", \"[BIN_3]\", \"[BIN_4]\", \"[BIN_5]\", \"[BIN_6]\", \"[BIN_7]\",\n",
    "                      \"[BIN_8]\", \"[BIN_9]\", \"[BIN_10]\", \"[BIN_11]\", \"[BIN_12]\", \"[BIN_13]\", \"[BIN_14]\"]\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "        self.bert1.resize_token_embeddings(len(self.tokenizer))  # Resize the model to accommodate new tokens\n",
    "        self.mask_token_index = config['mask_token_index']\n",
    "\n",
    "        # Define MLP head for regression\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "        # CosineEmbeddingLoss setup for MASK token prediction\n",
    "        self.criterion_similarity = nn.CosineEmbeddingLoss(margin=0.0)\n",
    "\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2, bin_ids=None):\n",
    "        # Forward pass through BERT1 (for MASK token prediction and bin prediction)\n",
    "        outputs1 = self.bert1(input_ids=input1, attention_mask=attention_mask1)\n",
    "        cls1 = outputs1.last_hidden_state[:, 0, :]  # CLS token\n",
    "        mask_idx = (input2 == self.mask_token_index).nonzero(as_tuple=True)  # Assume [MASK] token is at index self.mask_token_index\n",
    "        mask_embedding = outputs1.last_hidden_state[mask_idx]  # Mask token embedding\n",
    "\n",
    "        # Forward pass through BERT2 (for CLS token regression)\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Concatenate CLS tokens from BERT1 and BERT2 and pass through MLP for regression output\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Concatenate CLS embeddings\n",
    "        \n",
    "        # Pass through MLP head for salary regression\n",
    "        salary_output = self.mlp(combined_cls)\n",
    "        \n",
    "        # If bin_ids are provided, compute cosine similarity between MASK token and bin embeddings\n",
    "        if bin_ids is not None:\n",
    "            # Retrieve bin embeddings from the BERT model's embedding layer\n",
    "            print(bin_ids)\n",
    "            bin_embedding = self.bert1.embeddings.word_embeddings(bin_ids)  # [batch_size, hidden_size]\n",
    "            print(f\"mask_embedding shape: {mask_embedding.shape}\")\n",
    "            print(f\"bin_embedding shape: {bin_embedding.shape}\")\n",
    "            similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n",
    "            return salary_output, similarity_loss\n",
    "        \n",
    "        return salary_output, None  # For inference, return salary output only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double bert, cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=8): \n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.attention(query, key, value, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "        # Add residual connection and layer norm\n",
    "        output = self.layer_norm(query + attn_output)  # residual connection with query\n",
    "        return output\n",
    "\n",
    "\n",
    "class DualBERTWithCrossAttention(nn.Module):\n",
    "    \"\"\"Dual BERT model with cross-attention between the two text features.\n",
    "    Here, query is the first text feature and key, value are the second text feature.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(DualBERTWithCrossAttention, self).__init__()\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.cross_attention = CrossAttentionLayer(hidden_size, num_heads=self.num_heads)\n",
    "      \n",
    "    def forward(self, input1: torch.Tensor, attention_mask1: torch.Tensor, input2: torch.Tensor, attention_mask2: torch.Tensor):\n",
    "        # Get BERT outputs\n",
    "        outputs1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state\n",
    "        outputs2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state\n",
    "        \n",
    "        # Get raw CLS token before attention\n",
    "        cls2 = outputs2[:, 0, :]\n",
    "\n",
    "        # prepare key_padding_mask\n",
    "        key_padding_mask = (attention_mask2 == 0).bool() # True indicates positions to exclude (padding tokens). Shape: (batch_size, source_len)\n",
    "\n",
    "        # prepare attention mask\n",
    "        # # Step 1: Expand to match query and key dimensions\n",
    "        # original shape: (batch_size, target_len)\n",
    "        attn_mask = attention_mask1.unsqueeze(2) # create a new dimension at the end to be able to expand. Shape: (batch_size, target_len, 1)\n",
    "        attn_mask = attn_mask.expand(-1, -1, attention_mask2.size(1)) # expand to match the source_len. Shape: (batch_size, target_len, source_len)\n",
    "\n",
    "        # # Step 2: Adjust for multi-head attention\n",
    "        attn_mask = attn_mask.unsqueeze(1) # Add head dimension at position 1. Shape: (batch_size, 1, target_len, source_len)\n",
    "        attn_mask = attn_mask.repeat(1, self.num_heads, 1, 1)  # Repeat for each head. Shape: (batch_size, num_heads, target_len, source_len)\n",
    "        attn_mask = attn_mask.view(-1, attention_mask1.size(1), attention_mask2.size(1))  # Merge batch and head dimensions. Shape: (batch_size * num_heads, target_len, source_len)\n",
    "\n",
    "        # # Step 3: Convert to boolean mask\n",
    "        attn_mask = (attn_mask == 0).bool()  # True indicates positions to exclude (padding tokens). Shape: (batch_size * num_heads, target_len, source_len)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        attended_features = self.cross_attention(\n",
    "            query=outputs1,\n",
    "            key=outputs2,\n",
    "            value=outputs2,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            attn_mask=attn_mask,\n",
    "        )\n",
    "\n",
    "        # Get CLS from attended features\n",
    "        cls1_attended = attended_features[:, 0, :]\n",
    "\n",
    "        # mean pooling:\n",
    "        # # Handle padding tokens during pooling\n",
    "        # valid_token_mask = attention_mask2.unsqueeze(-1).float()  # Shape: (batch_size, seq_len, 1)\n",
    "        # attended_features = attended_features * valid_token_mask  # Zero-out padding embeddings\n",
    "        # pooled_output = attended_features.sum(dim=1) / valid_token_mask.sum(dim=1)  # Mean pooling only over valid tokens\n",
    "\n",
    "\n",
    "        # Concatenate the two [CLS] tokens\n",
    "        combined = torch.cat([cls1_attended, cls2], dim=1) # Shape: (batch_size, 2 * hidden_size)\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single bert, cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleBERTWithCrossAttention(nn.Module):\n",
    "    \"\"\"Single BERT model with cross-attention between the two text features.\n",
    "    Here, query is the first text feature and key, value are the second text feature.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SingleBERTWithCrossAttention, self).__init__()\n",
    "        model_name = config['model_name']\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.cross_attention = CrossAttentionLayer(hidden_size, num_heads=self.num_heads)\n",
    "      \n",
    "    def forward(self, input1: torch.Tensor, attention_mask1: torch.Tensor, input2: torch.Tensor, attention_mask2: torch.Tensor):\n",
    "        # Get BERT outputs\n",
    "        outputs1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state\n",
    "        outputs2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state\n",
    "        \n",
    "        # Get raw CLS token before attention\n",
    "        cls2 = outputs2[:, 0, :]\n",
    "\n",
    "        # prepare key_padding_mask\n",
    "        key_padding_mask = (attention_mask2 == 0).bool() # True indicates positions to exclude (padding tokens). Shape: (batch_size, source_len)\n",
    "\n",
    "        # prepare attention mask\n",
    "        # # Step 1: Expand to match query and key dimensions\n",
    "        # original shape: (batch_size, target_len)\n",
    "        attn_mask = attention_mask1.unsqueeze(2) # create a new dimension at the end to be able to expand. Shape: (batch_size, target_len, 1)\n",
    "        attn_mask = attn_mask.expand(-1, -1, attention_mask2.size(1)) # expand to match the source_len. Shape: (batch_size, target_len, source_len)\n",
    "\n",
    "        # # Step 2: Adjust for multi-head attention\n",
    "        attn_mask = attn_mask.unsqueeze(1) # Add head dimension at position 1. Shape: (batch_size, 1, target_len, source_len)\n",
    "        attn_mask = attn_mask.repeat(1, self.num_heads, 1, 1)  # Repeat for each head. Shape: (batch_size, num_heads, target_len, source_len)\n",
    "        attn_mask = attn_mask.view(-1, attention_mask1.size(1), attention_mask2.size(1))  # Merge batch and head dimensions. Shape: (batch_size * num_heads, target_len, source_len)\n",
    "\n",
    "        # # Step 3: Convert to boolean mask\n",
    "        attn_mask = (attn_mask == 0).bool()  # True indicates positions to exclude (padding tokens). Shape: (batch_size * num_heads, target_len, source_len)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        attended_features = self.cross_attention(\n",
    "            query=outputs1,\n",
    "            key=outputs2,\n",
    "            value=outputs2,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            attn_mask=attn_mask,\n",
    "        )\n",
    "\n",
    "        # Get CLS from attended features\n",
    "        cls1_attended = attended_features[:, 0, :]\n",
    "\n",
    "        # Concatenate the two [CLS] tokens\n",
    "        combined = torch.cat([cls1_attended, cls2], dim=1) # Shape: (batch_size, 2 * hidden_size)\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# make 2 arrays from normal distribution:\n",
    "# shape of output1:  torch.Size([16, 1024, 312])\n",
    "# shape of output2:  torch.Size([16, 256, 312])\n",
    "\n",
    "output1 = torch.randn(16, 1024, 312)\n",
    "output2 = torch.randn(16, 256, 312)\n",
    "\n",
    "# make 2 arrays, for each in dim 1 first 20% values are 1, the rest are 0:\n",
    "# shape of att mask1:  torch.Size([16, 1024])\n",
    "# shape of att mask2:  torch.Size([16, 256])\n",
    "\n",
    "att_mask1 = torch.zeros(16, 1024)\n",
    "att_mask1[:, :int(0.2 * 1024)] = 1\n",
    "\n",
    "att_mask2 = torch.zeros(16, 256)\n",
    "att_mask2[:, :int(0.2 * 256)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_mask (Optional[Tensor]) – If specified, a 2D or 3D mask preventing attention to certain positions. \n",
    "# Must be of shape (L,S) or (N⋅num_heads,L,S),\n",
    "# where N is the batch size, L is the target sequence length, and S is the source sequence length. \n",
    "# A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. \n",
    "# Binary and float masks are supported. \n",
    "# For a binary mask, a True value indicates that the corresponding position is not allowed to attend.\n",
    "# For a float mask, the mask values will be added to the attention weight.\n",
    "# If both attn_mask and key_padding_mask are supplied, their types should match.\n",
    "\n",
    "# shape: 16 * 8, 1024, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024, 312])\n",
      "torch.Size([16, 256, 312])\n",
      "torch.Size([16, 1024])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "print(output1.shape, output2.shape, att_mask1.shape, att_mask2.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "key_padding_mask = (att_mask2 == 0).bool()  \n",
    "print(key_padding_mask[0, :20], key_padding_mask[0, -20:], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(key_padding_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape =  torch.Size([16, 1024])\n",
      "original n True =  13120\n",
      "unsqueezed shape =  torch.Size([16, 1024, 1])\n",
      "expanded shape =  torch.Size([16, 1024, 256])\n",
      "slice 0 shape =  torch.Size([16, 1024])\n",
      "unsqueezed for heads shape = torch.Size([16, 1, 1024, 256])\n",
      "repeated for heads shape = torch.Size([16, 8, 1024, 256])\n",
      "view shape = torch.Size([128, 1024, 256])\n",
      "num heads adjusted shape =  torch.Size([128, 1024, 256])\n",
      "bool shape =  torch.Size([128, 1024, 256])\n",
      "n True =  26869760\n"
     ]
    }
   ],
   "source": [
    "print('original shape = ', att_mask1.shape)  # Shape: (batch_size, query_len)\n",
    "original_mask = att_mask1\n",
    "original_mask_bool = (original_mask == 0).bool()\n",
    "# count True values\n",
    "original_n_true = original_mask_bool.sum().item()\n",
    "print('original n True = ', original_n_true)\n",
    "\n",
    "# Step 1: Expand to match query and key dimensions\n",
    "attn_mask = att_mask1.unsqueeze(2)\n",
    "print('unsqueezed shape = ', attn_mask.shape)  # Expected shape: (16, 1024, 1)\n",
    "\n",
    "\n",
    "attn_mask = attn_mask.expand(-1, -1, att_mask2.size(1))  # Shape: (batch_size, query_len, key_len)\n",
    "print('expanded shape = ', attn_mask.shape)  # Expected shape: (16, 1024, 256)\n",
    "\n",
    "slice_0 = attn_mask[:, :, 0]  # Shape: (batch_size, query_len)\n",
    "print('slice 0 shape = ', slice_0.shape)  # Expected shape: (16, 1024)\n",
    "slice_100 = attn_mask[:, :, 100]  # Shape: (batch_size, query_len)\n",
    "\n",
    "# Ensure the slices are equal to the unsqueezed mask (after squeezing last dimension)\n",
    "assert torch.equal(slice_0, slice_100), \"Slices [:, :, 0] and [:, :, 100] are not equal!\"\n",
    "assert torch.equal(slice_0, original_mask), \"Slice [:, :, 0] does not match the unsqueezed mask!\"\n",
    "assert torch.equal(slice_100, original_mask), \"Slice [:, :, 100] does not match the unsqueezed mask!\"\n",
    "\n",
    "\n",
    "# Step 2: Adjust for multi-head attention\n",
    "num_heads = 8  # Number of attention heads\n",
    "attn_mask = attn_mask.unsqueeze(1)\n",
    "print('unsqueezed for heads shape =', attn_mask.shape)  # Final shape: (16 * 8, 1024, 256)\n",
    "attn_mask = attn_mask.repeat(1, num_heads, 1, 1)  # Add head dimension and repeat for each head\n",
    "print('repeated for heads shape =', attn_mask.shape)  # Final shape: (16 * 8, 1024, 256)\n",
    "attn_mask = attn_mask.view(-1, att_mask1.size(1), att_mask2.size(1))  # Merge batch and head dimensions\n",
    "print('view shape =', attn_mask.shape)  # Final shape: (16 * 8, 1024, 256)\n",
    "\n",
    "# ensure [:16, :, 1] equals original mask\n",
    "assert torch.equal(attn_mask[:16, :, 1], original_mask), \"Slice [:16, :, 1] does not match the original mask!\"\n",
    "\n",
    "print('num heads adjusted shape = ', attn_mask.shape)  # Final shape: (16 * 8, 1024, 256)\n",
    "\n",
    "# Step 3: Convert to boolean mask\n",
    "attn_mask = (attn_mask == 0).bool()  # True indicates positions to exclude\n",
    "print('bool shape = ', attn_mask.shape)  # Final boolean mask shape: (16 * 8, 1024, 256)\n",
    "# count True values\n",
    "n_true = attn_mask.sum().item()\n",
    "print('n True = ', n_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:44.363945Z",
     "iopub.status.busy": "2024-12-16T09:03:44.363598Z",
     "iopub.status.idle": "2024-12-16T09:03:44.395122Z",
     "shell.execute_reply": "2024-12-16T09:03:44.394460Z",
     "shell.execute_reply.started": "2024-12-16T09:03:44.363911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "def fit_eval(\n",
    "    seed,\n",
    "    model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train_reg,\n",
    "    y_test_reg,\n",
    "    criterion,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    text_col_1,\n",
    "    text_col_2,\n",
    "):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    memory_cleanup()\n",
    "\n",
    "    # Unpack config\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seq_length = config[\"seq_length\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train_reg, tokenizer, seq_length)\n",
    "    test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test_reg, tokenizer, seq_length)\n",
    "    # Make dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training and Evaluation Loop\n",
    "    history = {\n",
    "                \"train_loss\": [],\n",
    "               \"test_loss\": [], \n",
    "                \"train_rmse\": [],\n",
    "               \"test_rmse\": [], \n",
    "               \"train_r2\": [],\n",
    "               \"train_r2\": [],\n",
    "               \"test_r2\": [],\n",
    "               \"train_mae\": [],\n",
    "               \"test_mae\": [],\n",
    "               \"y_pred\": [],\n",
    "               \"y_test\": [],\n",
    "               }\n",
    "\n",
    "    print('Starting training/eval loop...')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting training...')\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            outputs = outputs.flatten()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            all_preds.extend(outputs.cpu().detach().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "        train_loss = np.mean(train_losses)\n",
    "        \n",
    "        train_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "        train_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_r2\"].append(train_r2)\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"train_mae\"].append(train_mae)\n",
    "    \n",
    "        # Evaluation Phase\n",
    "        print('Epoch done, evaluating...')\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                inputs1, inputs2, targets = batch\n",
    "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "                targets = targets.to(device)\n",
    "    \n",
    "                outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "                outputs = outputs.flatten()\n",
    "                # loss = criterion(outputs.squeeze(), targets)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_losses.append(loss.item())\n",
    "                # all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        history[\"y_pred\"].append(all_preds)\n",
    "        history[\"y_test\"].append(all_labels)\n",
    "\n",
    "        test_loss = np.mean(test_losses)\n",
    "\n",
    "        test_r2 = r2_score(all_labels, all_preds)\n",
    "        \n",
    "        test_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        test_mae = mean_absolute_error(all_labels, all_preds)\n",
    "    \n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        \n",
    "        history[\"test_r2\"].append(test_r2)\n",
    "        \n",
    "        history[\"test_rmse\"].append(test_rmse)\n",
    "        \n",
    "        history[\"test_mae\"].append(test_mae)\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multitask training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_eval_with_bins(\n",
    "    seed,\n",
    "    model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train_reg,\n",
    "    y_test_reg,\n",
    "    y_train_bin,\n",
    "    y_test_bin,\n",
    "    criterion,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    text_col_1,\n",
    "    text_col_2,\n",
    "):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    memory_cleanup()\n",
    "\n",
    "    # Unpack config\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seq_length = config[\"seq_length\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = DualTextDatasetWithBins(X_train, text_col_1, text_col_2, y_train_reg, y_train_bin, tokenizer, seq_length)\n",
    "    test_dataset = DualTextDatasetWithBins(X_test, text_col_1, text_col_2, y_test_reg, y_test_bin, tokenizer, seq_length)\n",
    "    # Make dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training and Evaluation Loop\n",
    "    history = {\n",
    "                \"train_loss\": [],\n",
    "               \"test_loss\": [], \n",
    "                \"train_rmse\": [],\n",
    "               \"test_rmse\": [], \n",
    "               \"train_r2\": [],\n",
    "               \"train_r2\": [],\n",
    "               \"test_r2\": [],\n",
    "               \"train_mae\": [],\n",
    "               \"test_mae\": [],\n",
    "               \"y_pred\": [],\n",
    "               \"y_test\": [],\n",
    "               }\n",
    "\n",
    "    print('Starting training/eval loop...')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting training...')\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_dataloader:\n",
    "            inputs1, inputs2, targets, bin_ids = batch  # Now bin_ids are included in the dataset\n",
    "            print(len(inputs1))\n",
    "            print(len(bin_ids))\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "            bin_ids = bin_ids.to(device)  # Ensure bin_ids are moved to the same device\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: include bin_ids\n",
    "            salary_output, similarity_loss = model(input1, attention_mask1, input2, attention_mask2, bin_ids)\n",
    "            loss = criterion(salary_output, targets) + similarity_loss  # Combine regression loss and similarity loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            all_preds.extend(salary_output.cpu().detach().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            # outputs = outputs.flatten()\n",
    "            # # loss = criterion(outputs.squeeze(), targets)\n",
    "            # loss = criterion(outputs, targets)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # train_losses.append(loss.item())\n",
    "            # # all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "            # all_preds.extend(outputs.cpu().detach().numpy())\n",
    "            # all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "        train_loss = np.mean(train_losses)\n",
    "        \n",
    "        train_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "        train_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_r2\"].append(train_r2)\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"train_mae\"].append(train_mae)\n",
    "    \n",
    "        # Evaluation Phase\n",
    "        print('Epoch done, evaluating...')\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                inputs1, inputs2, targets, bin_ids = batch  # Get bin_ids during evaluation\n",
    "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "                targets = targets.to(device)\n",
    "                bin_ids = bin_ids.to(device)  # Ensure bin_ids are moved to the same device\n",
    "    \n",
    "                # Forward pass: include bin_ids\n",
    "                salary_output, similarity_loss = model(input1, attention_mask1, input2, attention_mask2, bin_ids)\n",
    "                loss = criterion(salary_output, targets) + similarity_loss  # Combine losses\n",
    "                test_losses.append(loss.item())\n",
    "                all_preds.extend(salary_output.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "            \n",
    "        history[\"y_pred\"].append(all_preds)\n",
    "        history[\"y_test\"].append(all_labels)\n",
    "\n",
    "        test_loss = np.mean(test_losses)\n",
    "\n",
    "        test_r2 = r2_score(all_labels, all_preds)\n",
    "        \n",
    "        test_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        test_mae = mean_absolute_error(all_labels, all_preds)\n",
    "    \n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        \n",
    "        history[\"test_r2\"].append(test_r2)\n",
    "        \n",
    "        history[\"test_rmse\"].append(test_rmse)\n",
    "        \n",
    "        history[\"test_mae\"].append(test_mae)\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-eval loop with experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define text feature/target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col_1 = 'description_no_numbers'\n",
    "text_col_1_with_prompt = text_col_1 + '_with_prompt' # Add prompt to text for multitask learning\n",
    "\n",
    "text_col_2 = 'title_company_location_skills_source' # Merged text column, second feature\n",
    "\n",
    "bin_targets_col = 'salary_bin' # Bin targets for MASK token prediction and multitask learning\n",
    "target_col = 'log_salary_from' # regression target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create merged title/skills/location/source feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['skills'].fillna('Не указаны', inplace=True)\n",
    "\n",
    "title_company_location_skills_feature_template = \"\"\"\n",
    "Позиция: {position}\n",
    "Компания: {company}\n",
    "Место: {location}\n",
    "Навыки: {skills}\n",
    "Источник: {source}\n",
    "\"\"\"\n",
    "\n",
    "df['title_company_location_skills_source'] = df.apply(lambda x: title_company_location_skills_feature_template.format(\n",
    "    position=x['title'],\n",
    "    company=x['company'],\n",
    "    location=x['location'],\n",
    "    skills=x['skills'],\n",
    "    source=x['source']\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create bins for target prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Compute bin edges for 15 equal bins\n",
    "num_bins = 15\n",
    "bin_edges = np.linspace(\n",
    "    df[target_col].min(), df[target_col].max(), num_bins + 1\n",
    ")\n",
    "\n",
    "# Assign bins to a new column\n",
    "df[bin_targets_col] = np.digitize(df[target_col], bins=bin_edges, right=True) - 1\n",
    "\n",
    "# Ensure bin labels are within range\n",
    "df[bin_targets_col] = df[bin_targets_col].clip(0, num_bins - 1)\n",
    "\n",
    "# prompt to be added to feature 1 for multitask learning\n",
    "prompt = \"\"\"\\\n",
    "[CLS] Далее указано описание вакансии. \\\n",
    "Судя по описанию, зарплата на этой позиции составляет [MASK].[SEP]\\\n",
    "\"\"\"\n",
    "\n",
    "df[text_col_1_with_prompt] = prompt + df[text_col_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:44.844772Z",
     "iopub.status.busy": "2024-12-16T09:03:44.844155Z",
     "iopub.status.idle": "2024-12-16T09:03:45.218052Z",
     "shell.execute_reply": "2024-12-16T09:03:45.217364Z",
     "shell.execute_reply.started": "2024-12-16T09:03:44.844739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1: Single BERT, MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:45.256720Z",
     "iopub.status.busy": "2024-12-16T09:03:45.256157Z",
     "iopub.status.idle": "2024-12-16T09:03:48.121783Z",
     "shell.execute_reply": "2024-12-16T09:03:48.120485Z",
     "shell.execute_reply.started": "2024-12-16T09:03:45.256690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for seed 42...\n",
      "double_huber_multitask model...\n",
      "Creating the dataset...\n",
      "Creating the dataset...\n",
      "Starting training/eval loop...\n",
      "Starting training...\n",
      "3\n",
      "32\n",
      "tensor([83835, 83836, 83836, 83836, 83836, 83836, 83833, 83836, 83836, 83835,\n",
      "        83835, 83835, 83834, 83836, 83836, 83835], device='cuda:0')\n",
      "mask_embedding shape: torch.Size([11, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n",
      "tensor([83835, 83836, 83836, 83836, 83837, 83837, 83836, 83836, 83836, 83833,\n",
      "        83836, 83836, 83836, 83835, 83835, 83837], device='cuda:1')\n",
      "mask_embedding shape: torch.Size([5, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_eval_with_bins\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_targets_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatboost_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_col_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_col_1_with_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m memory_cleanup()\n\u001b[1;32m    106\u001b[0m combined_history[seed][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m history    \n",
      "Cell \u001b[0;32mIn[17], line 239\u001b[0m, in \u001b[0;36mfit_eval_with_bins\u001b[0;34m(seed, model, X_train, X_test, y_train, y_test, bin_targets_col, catboost_preds, criterion, tokenizer, config, text_col_1, text_col_2)\u001b[0m\n\u001b[1;32m    237\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Forward pass: include bin_ids\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m salary_output, similarity_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(salary_output, targets) \u001b[38;5;241m+\u001b[39m similarity_loss  \u001b[38;5;66;03m# Combine regression loss and similarity loss\u001b[39;00m\n\u001b[1;32m    241\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import pickle\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "experiment_name = 'single_bert_mse'\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "seeds = [42, 78687, 123123]\n",
    "combined_history = {}\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "config = {\n",
    "    \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "    \"batch_size\": 32,\n",
    "    \"seq_length\": 1024,\n",
    "    \"hidden_size\": 312,\n",
    "    \"mlp_hidden_size\": 128,\n",
    "    # \"num_epochs\": 10,\n",
    "    \"num_epochs\": 2,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "}\n",
    "\n",
    "memory_cleanup()\n",
    "\n",
    "model_name = config['model_name']\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "X = df[[text_col_1, text_col_1_with_prompt, text_col_2]][:200]\n",
    "y = df[[target_col, bin_targets_col]][:200]\n",
    "\n",
    "for seed in seeds:\n",
    "    memory_cleanup()\n",
    "    print(f'Starting for seed {str(seed)}...')\n",
    "    print('-' * 100)\n",
    "\n",
    "    combined_history[seed] = {}\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    y_train_reg = y_train[target_col]\n",
    "    y_test_reg = y_test[target_col]\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SingleBERTWithMLP(config)\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # Loss Function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model, history = fit_eval(\n",
    "        seed,\n",
    "        model,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train_reg,\n",
    "        y_test_reg,\n",
    "        criterion,\n",
    "        tokenizer,\n",
    "        config,\n",
    "        text_col_1,\n",
    "        text_col_2,\n",
    "    )\n",
    "\n",
    "    memory_cleanup()\n",
    "\n",
    "    combined_history[seed] = history    \n",
    "\n",
    "# Display metrics\n",
    "display_metrics_with_ci(combined_history)\n",
    "\n",
    "# save the history as pickle\n",
    "with open(f'./history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for seed 42...\n",
      "double_huber_multitask model...\n",
      "Creating the dataset...\n",
      "Creating the dataset...\n",
      "Starting training/eval loop...\n",
      "Starting training...\n",
      "3\n",
      "32\n",
      "tensor([83835, 83836, 83836, 83836, 83836, 83836, 83833, 83836, 83836, 83835,\n",
      "        83835, 83835, 83834, 83836, 83836, 83835], device='cuda:0')\n",
      "mask_embedding shape: torch.Size([11, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n",
      "tensor([83835, 83836, 83836, 83836, 83837, 83837, 83836, 83836, 83836, 83833,\n",
      "        83836, 83836, 83836, 83835, 83835, 83837], device='cuda:1')\n",
      "mask_embedding shape: torch.Size([5, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\u001b[39;00m\n",
      "\u001b[0;32m--> 100\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_eval_with_bins\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_targets_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatboost_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_col_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_col_1_with_prompt\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    104\u001b[0m memory_cleanup()\n",
      "\u001b[1;32m    106\u001b[0m combined_history[seed][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m history    \n",
      "\n",
      "Cell \u001b[0;32mIn[17], line 239\u001b[0m, in \u001b[0;36mfit_eval_with_bins\u001b[0;34m(seed, model, X_train, X_test, y_train, y_test, bin_targets_col, catboost_preds, criterion, tokenizer, config, text_col_1, text_col_2)\u001b[0m\n",
      "\u001b[1;32m    237\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Forward pass: include bin_ids\u001b[39;00m\n",
      "\u001b[0;32m--> 239\u001b[0m salary_output, similarity_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    240\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(salary_output, targets) \u001b[38;5;241m+\u001b[39m similarity_loss  \u001b[38;5;66;03m# Combine regression loss and similarity loss\u001b[39;00m\n",
      "\u001b[1;32m    241\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n",
      "\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n",
      "\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n",
      "\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n",
      "\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n",
      "\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n",
      "\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n",
      "\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n",
      "\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n",
      "    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n",
      "    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n",
      "    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\n",
      "RuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import pickle\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "seeds = [42, 78687, 123123]\n",
    "combined_history = {}\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "config = {\n",
    "    \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "    \"batch_size\": 32,\n",
    "    \"seq_length\": 1024,\n",
    "    \"hidden_size\": 312,\n",
    "    \"mlp_hidden_size\": 128,\n",
    "    # \"num_epochs\": 10,\n",
    "    \"num_epochs\": 2,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"mask_token_index\": 17,\n",
    "    \n",
    "}\n",
    "\n",
    "memory_cleanup()\n",
    "model_name = config['model_name']\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "X = df[[text_col_1, text_col_1_with_prompt, text_col_2]][:200]\n",
    "y = df[[target_col, bin_targets_col]][:200]\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    memory_cleanup()\n",
    "    print(f'Starting for seed {str(seed)}...')\n",
    "    catboost_preds = catboost_history[seed]['y_pred'][:40]\n",
    "\n",
    "    combined_history[seed] = {}\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    y_train_reg = y_train[target_col]\n",
    "    y_test_reg = y_test[target_col]\n",
    "    y_train_bin = y_train[bin_targets_col]\n",
    "    y_test_bin = y_test[bin_targets_col]\n",
    "\n",
    "\n",
    "    # fit-eval non-TSDAE-ed model\n",
    "    # model = DualBERTWithMLP(config)\n",
    "    model = MaskBinDualBERTWithMLP(config)\n",
    "    tokenizer = model.get_tokenizer()\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    # Loss Function\n",
    "    # criterion = nn.MSELoss()  # For regression\n",
    "    criterion = nn.HuberLoss()\n",
    "\n",
    "    print(f'double_huber_multitask model...')\n",
    "    # model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\n",
    "    model, history = fit_eval_with_bins(\n",
    "        seed,\n",
    "        model,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train_reg,\n",
    "        y_test_reg,\n",
    "        y_train_bin,\n",
    "        y_test_bin,\n",
    "        catboost_preds, criterion, tokenizer, config,\n",
    "        text_col_1 = text_col_1_with_prompt,\n",
    "    )\n",
    "    memory_cleanup()\n",
    "\n",
    "    combined_history[seed]['double_huber_multitask'] = history    \n",
    "\n",
    "    # # further split train data into regression train and tsdae train data\n",
    "    # X_train_tsdae, X_tsdae, y_train_tsdae, y_tsdae = train_test_split(X_train, y_train, test_size=0.01, random_state=seed)\n",
    "\n",
    "    # # convert text_col_1 data into set of sentences and select 20-60 word sentences as a feature column:\n",
    "    # # Create a DataFrame of unique sentences and their lengths for X_tsdae\n",
    "    # unique_sentences = []\n",
    "    # unique_sentence_lengths = []\n",
    "    # for text in X_tsdae[text_col_1]:\n",
    "    #     sentences, sentence_lengths = get_sentence_lengths(text)\n",
    "    #     unique_sentences.extend(sentences)\n",
    "    #     unique_sentence_lengths.extend(sentence_lengths)\n",
    "\n",
    "    # unique_sentences_df = pd.DataFrame({\n",
    "    #     'sentence': unique_sentences,\n",
    "    #     'length': unique_sentence_lengths\n",
    "    # })\n",
    "\n",
    "    # unique_sentences = unique_sentences_df[(unique_sentences_df.length >= 10) & (unique_sentences_df.length <= 60)]['sentence']\n",
    "\n",
    "    # # get array with features for each bert\n",
    "    # train_sentences_array = [\n",
    "    #     unique_sentences.tolist(),\n",
    "    #     # X_tsdae[text_col_1].tolist(),\n",
    "    #     X_tsdae[text_col_2].tolist(),\n",
    "    # ]\n",
    "\n",
    "    # berts_after_tsdae = []\n",
    "    # for index, train_sentences in enumerate(train_sentences_array):\n",
    "    #     memory_cleanup()\n",
    "    #     berts_after_tsdae.append(train_tsdae_bert(model_name, train_sentences))\n",
    "    # memory_cleanup()\n",
    "\n",
    "    # tsdae_bert1, tsdae_bert2 = berts_after_tsdae\n",
    "\n",
    "    # # Initialize the non-TSDAE-ed BERT models\n",
    "    # model = TSDAEDualBERTWithMLP(config, tsdae_bert1, tsdae_bert2)\n",
    "    # model = torch.nn.DataParallel(model).to(device)\n",
    "    # # Loss Function\n",
    "    # # criterion = nn.MSELoss()  # For regression\n",
    "    # criterion = nn.HuberLoss()\n",
    "\n",
    "    # print(f'tsdae model...')\n",
    "    # model, history = fit_eval(seed, model, X_train_tsdae, X_test, y_train_tsdae, y_test, catboost_preds, criterion, tokenizer, config)\n",
    "\n",
    "    # combined_history[seed]['double_huber_1p_sentences_tsdae'] = history\n",
    "\n",
    "# save the history as pickle\n",
    "with open('./models/combined_history_exp_3.pickle', 'wb') as handle:\n",
    "    pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-16T08:39:54.591403Z",
     "iopub.status.idle": "2024-12-16T08:39:54.591704Z",
     "shell.execute_reply": "2024-12-16T08:39:54.591576Z",
     "shell.execute_reply.started": "2024-12-16T08:39:54.591561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the rows\n",
    "rows = []\n",
    "\n",
    "# Iterate through the dictionary\n",
    "for seed, experiments in combined_history.items():\n",
    "    for experiment_name, metrics in experiments.items():\n",
    "        num_epochs = len(metrics['train_loss'])\n",
    "        for epoch in range(num_epochs):\n",
    "            row = {\n",
    "                'random_seed': seed,\n",
    "                'experiment_name': experiment_name,\n",
    "                'epoch': epoch + 1,\n",
    "                \n",
    "                'train_loss': metrics['train_loss'][epoch],\n",
    "                'test_loss': metrics['test_loss'][epoch],\n",
    "                \n",
    "                'train_r2': metrics['train_r2'][epoch],\n",
    "                'test_r2': metrics['test_r2'][epoch],\n",
    "                'test_r2_with_catboost': metrics['test_r2_with_catboost'][epoch]\n",
    "                \n",
    "                'train_rmse': metrics['train_rmse'][epoch],\n",
    "                'test_rmse': metrics['test_rmse'][epoch],\n",
    "                'test_rmse_with_catboost': metrics['test_rmse_with_catboost'][epoch]\n",
    "            \n",
    "                'train_mae': metrics['train_mae'][epoch],\n",
    "                'test_mae': metrics['test_mae'][epoch],\n",
    "                'test_mae_with_catboost': metrics['test_mae_with_catboost'][epoch]\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "# Convert the list of rows into a DataFrame\n",
    "history = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:12:09.149945Z",
     "iopub.status.busy": "2024-12-14T00:12:09.149529Z",
     "iopub.status.idle": "2024-12-14T00:12:09.177254Z",
     "shell.execute_reply": "2024-12-14T00:12:09.176403Z",
     "shell.execute_reply.started": "2024-12-14T00:12:09.149883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Group by experiment name and epoch, then calculate mean and std\n",
    "grouped_history = history.groupby(['experiment_name', 'epoch']).agg(\n",
    "    train_loss_mean=('train_loss', 'mean'),\n",
    "    train_loss_std=('train_loss', 'std'),\n",
    "    test_loss_mean=('test_loss', 'mean'),\n",
    "    test_loss_std=('test_loss', 'std'),\n",
    "    train_r2_mean=('train_r2', 'mean'),\n",
    "    train_r2_std=('train_r2', 'std'),\n",
    "    test_r2_mean=('test_r2', 'mean'),\n",
    "    test_r2_std=('test_r2', 'std'),\n",
    "    test_r2_with_catboost_mean=('test_r2_with_catboost', 'mean'),\n",
    "    test_r2_with_catboost_std=('test_r2_with_catboost', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Display the grouped DataFrame\n",
    "print(\"\\nGrouped DataFrame with Mean and Std:\")\n",
    "print(grouped_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T23:58:43.180037Z",
     "iopub.status.busy": "2024-12-13T23:58:43.179738Z",
     "iopub.status.idle": "2024-12-13T23:58:43.184837Z",
     "shell.execute_reply": "2024-12-13T23:58:43.183820Z",
     "shell.execute_reply.started": "2024-12-13T23:58:43.180008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "# text_col_1 = 'description_no_numbers_v2_with_prompt' # for mask pooling\n",
    "# text_col_2 = 'title_company_location_skills_source_with_prompt' # for mask pooling\n",
    "\n",
    "# X = df[[text_col_1, text_col_2]]\n",
    "# y = df['log_salary_from']\n",
    "\n",
    "# # Split # already done in previous cell\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# model = SingleBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "\n",
    "\n",
    "# word_embedding_model1 = models.Transformer(model_name)\n",
    "# pooling_model1 = models.Pooling(word_embedding_model1.get_word_embedding_dimension(), \"cls\")\n",
    "# bert1 = SentenceTransformer(modules=[word_embedding_model1, pooling_model1])\n",
    "\n",
    "# word_embedding_model2 = models.Transformer(model_name)\n",
    "# pooling_model2 = models.Pooling(word_embedding_model2.get_word_embedding_dimension(), \"cls\")\n",
    "# bert2 = SentenceTransformer(modules=[word_embedding_model2, pooling_model2])\n",
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"./models/final_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
