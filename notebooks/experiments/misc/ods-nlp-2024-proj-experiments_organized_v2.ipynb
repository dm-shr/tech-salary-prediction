{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning transformer + MLP head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:03.810629Z",
     "iopub.status.busy": "2024-12-16T09:01:03.810343Z",
     "iopub.status.idle": "2024-12-16T09:01:16.186941Z",
     "shell.execute_reply": "2024-12-16T09:01:16.185958Z",
     "shell.execute_reply.started": "2024-12-16T09:01:03.810595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-16 09:01:06--  https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.202.132, 2607:f8b0:4001:c06::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.202.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 295792946 (282M) [application/octet-stream]\n",
      "Saving to: './data/dataset.csv'\n",
      "\n",
      "./data/dataset.csv  100%[===================>] 282.09M   153MB/s    in 1.8s    \n",
      "\n",
      "2024-12-16 09:01:10 (153 MB/s) - './data/dataset.csv' saved [295792946/295792946]\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22224 entries, 0 to 22223\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   title                                 22224 non-null  object \n",
      " 1   company                               22224 non-null  object \n",
      " 2   location                              22224 non-null  object \n",
      " 3   skills                                14384 non-null  object \n",
      " 4   source                                22224 non-null  object \n",
      " 5   description_no_numbers_with_skills    22224 non-null  object \n",
      " 6   experience_from                       22224 non-null  float64\n",
      " 7   experience_to_adjusted_10             22224 non-null  float64\n",
      " 8   description_size                      22224 non-null  int64  \n",
      " 9   description                           22224 non-null  object \n",
      " 10  description_no_numbers                22224 non-null  object \n",
      " 11  description_no_numbers_v2             22224 non-null  object \n",
      " 12  title_company_location_skills_source  22224 non-null  object \n",
      " 13  salary_from                           22224 non-null  float64\n",
      " 14  log_salary_from                       22224 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(10)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:16.189393Z",
     "iopub.status.busy": "2024-12-16T09:01:16.188980Z",
     "iopub.status.idle": "2024-12-16T09:01:19.663789Z",
     "shell.execute_reply": "2024-12-16T09:01:19.662565Z",
     "shell.execute_reply.started": "2024-12-16T09:01:16.189349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-16 09:01:17--  https://drive.usercontent.google.com/download?id=1gbA1owa-9aTxTaGABpTffGvE8y31qHze&export=download&authuser=1&confirm=t\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.202.132, 2607:f8b0:4001:c06::84\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.202.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 428138 (418K) [application/octet-stream]\n",
      "Saving to: './data/best_catboost_model_train_eval_history.pickle'\n",
      "\n",
      "./data/best_catboos 100%[===================>] 418.10K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2024-12-16 09:01:19 (70.0 MB/s) - './data/best_catboost_model_train_eval_history.pickle' saved [428138/428138]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1gbA1owa-9aTxTaGABpTffGvE8y31qHze&export=download&authuser=1&confirm=t' -O './data/best_catboost_model_train_eval_history.pickle'\n",
    "\n",
    "import pickle\n",
    "\n",
    "file = './data/best_catboost_model_train_eval_history.pickle'\n",
    "with open(file, 'rb') as f:\n",
    "    catboost_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:19.666074Z",
     "iopub.status.busy": "2024-12-16T09:01:19.665690Z",
     "iopub.status.idle": "2024-12-16T09:01:30.723032Z",
     "shell.execute_reply": "2024-12-16T09:01:30.722058Z",
     "shell.execute_reply.started": "2024-12-16T09:01:19.666034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:01:30.725630Z",
     "iopub.status.busy": "2024-12-16T09:01:30.725301Z",
     "iopub.status.idle": "2024-12-16T09:02:03.249282Z",
     "shell.execute_reply": "2024-12-16T09:02:03.248529Z",
     "shell.execute_reply.started": "2024-12-16T09:01:30.725601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"Clean up memory\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    \"Get number of words in each sentence in the text\"\n",
    "    # pattern = r'(?<=[.!?])\\s+'\n",
    "    pattern = r'(?<=[.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    # remove empty strings\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    # get number of words in each sentence\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    return sentences, sentence_lengths\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"Set seed for reproducibility\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def plot(history):\n",
    "    \"Plot training and validation metrics, skip the first epoch\"\n",
    "    # Plot Loss\n",
    "    plt.plot(range(2, len(history[\"train_loss\"]) + 1), history[\"train_loss\"][1:], label=\"Train Loss\")\n",
    "    plt.plot(range(2, len(history[\"test_loss\"]) + 1), history[\"test_loss\"][1:], label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Test Loss Over Epochs 2-...\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot R2 Score\n",
    "    plt.plot(range(2, len(history[\"train_r2\"]) + 1), history[\"train_r2\"][1:], label=\"Train R2\")\n",
    "    plt.plot(range(2, len(history[\"test_r2\"]) + 1), history[\"test_r2\"][1:], label=\"Test R2\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"R2 Score\")\n",
    "    plt.title(\"Train/Test R2 Score Over Epochs 2-...\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_tsdae_bert(model_name, train_sentences):\n",
    "    \"\"\"Train a denoising auto-encoder model with BERT model.\n",
    "    more examples at https://sbert.net/examples/unsupervised_learning/TSDAE/README.html\"\"\"\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # Create the special denoising dataset that adds noise on-the-fly\n",
    "    train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "    \n",
    "    # DataLoader to batch your data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Use the denoising auto-encoder loss\n",
    "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "        model, decoder_name_or_path=model_name, tie_encoder_decoder=True,\n",
    "    )\n",
    "    \n",
    "    # Call the fit method\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        weight_decay=0,\n",
    "        scheduler=\"constantlr\",\n",
    "        optimizer_params={\"lr\": 3e-5},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traning-related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:02:03.250804Z",
     "iopub.status.busy": "2024-12-16T09:02:03.250242Z",
     "iopub.status.idle": "2024-12-16T09:02:03.270324Z",
     "shell.execute_reply": "2024-12-16T09:02:03.269478Z",
     "shell.execute_reply.started": "2024-12-16T09:02:03.250775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "\n",
    "\n",
    "# single bert\n",
    "class SingleBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(SingleBERTWithMLP, self).__init__()\n",
    "        # Initialize a single BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),  # Double hidden size for concatenation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Pass both inputs through the same BERT model\n",
    "        cls1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token for input1\n",
    "        cls2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token for input2\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        cls1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token\n",
    "        # mask1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 23, :]  # mask token \n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "        # mask2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 23, :]  # mask token\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "        # concat mask embeddings\n",
    "        # combined_mask = torch.cat([mask1, mask2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        # output = self.mlp(combined_mask)\n",
    "        return output\n",
    "\n",
    "# Define the dual BERT model with an MLP head and MASK pooling\n",
    "class MASKPoolDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(MASKPoolDualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        mask1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 23, :]  # mask token \n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        mask2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 23, :]  # mask token\n",
    "\n",
    "        # concat mask embeddings\n",
    "        combined_mask = torch.cat([mask1, mask2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_mask)\n",
    "        return output\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class TSDAEDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config, bert1, bert2):\n",
    "        super(TSDAEDualBERTWithMLP, self).__init__()\n",
    "        # Load TSDAE-ed BERT models\n",
    "        self.bert1 = bert1\n",
    "        self.bert2 = bert2\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # idea from: https://github.com/UKPLab/sentence-transformers/issues/2494\n",
    "        # Forward pass through BERT1\n",
    "        input_dict1 = {\n",
    "            'input_ids': input1,\n",
    "            'attention_mask': attention_mask1\n",
    "        }\n",
    "        cls1 = self.bert1(input_dict1)['sentence_embedding']\n",
    "        \n",
    "        # Forward pass through BERT2\n",
    "        input_dict2 = {\n",
    "            'input_ids': input2,\n",
    "            'attention_mask': attention_mask2\n",
    "        }\n",
    "        cls2 = self.bert2(input_dict2)['sentence_embedding']\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:02:03.272111Z",
     "iopub.status.busy": "2024-12-16T09:02:03.271744Z",
     "iopub.status.idle": "2024-12-16T09:02:03.307058Z",
     "shell.execute_reply": "2024-12-16T09:02:03.306457Z",
     "shell.execute_reply.started": "2024-12-16T09:02:03.272070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DualTextDatasetWithBins(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, bin_targets_col, targets, tokenizer, max_len):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Targets\n",
    "        self.targets = targets.tolist()  # Log salary target for regression\n",
    "        self.bin_targets = df[bin_targets_col].tolist()  # Bin IDs for mask token prediction\n",
    "        \n",
    "        # Get the bin token ID offset from the tokenizer's vocabulary\n",
    "        self.bin_token_offset = tokenizer.convert_tokens_to_ids(\"[BIN_0]\")  # This will give the token ID of the first bin token\n",
    "        self.bin_to_id_mapping = {i: torch.tensor(i + self.bin_token_offset, dtype=torch.long) for i in range(15)}\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        \n",
    "        # Map the bin_targets (0 to 14) to the corresponding token IDs\n",
    "        # bin_target = torch.tensor(self.bin_targets[idx] + self.bin_token_offset, dtype=torch.long)  # Add offset\n",
    "        bin_target = self.bin_to_id_mapping[self.bin_targets[idx]]\n",
    "        \n",
    "        return inputs1, inputs2, target, bin_target\n",
    "\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class MaskBinDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MaskBinDualBERTWithMLP, self).__init__()\n",
    "\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Add new bin tokens to the tokenizer and resize the model embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        new_tokens = [\"[BIN_0]\", \"[BIN_1]\", \"[BIN_2]\", \"[BIN_3]\", \"[BIN_4]\", \"[BIN_5]\", \"[BIN_6]\", \"[BIN_7]\",\n",
    "                      \"[BIN_8]\", \"[BIN_9]\", \"[BIN_10]\", \"[BIN_11]\", \"[BIN_12]\", \"[BIN_13]\", \"[BIN_14]\"]\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "        self.bert1.resize_token_embeddings(len(self.tokenizer))  # Resize the model to accommodate new tokens\n",
    "        self.mask_token_index = config['mask_token_index']\n",
    "\n",
    "        # Define MLP head for regression\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "        # CosineEmbeddingLoss setup for MASK token prediction\n",
    "        self.criterion_similarity = nn.CosineEmbeddingLoss(margin=0.0)\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2, bin_ids=None):\n",
    "        # Forward pass through BERT1 (for MASK token prediction and bin prediction)\n",
    "        outputs1 = self.bert1(input_ids=input1, attention_mask=attention_mask1)\n",
    "        cls1 = outputs1.last_hidden_state[:, 0, :]  # CLS token\n",
    "        mask_idx = (input2 == self.mask_token_index).nonzero(as_tuple=True)  # Assume [MASK] token is at index self.mask_token_index\n",
    "        mask_embedding = outputs1.last_hidden_state[mask_idx]  # Mask token embedding\n",
    "\n",
    "        # Forward pass through BERT2 (for CLS token regression)\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Concatenate CLS tokens from BERT1 and BERT2 and pass through MLP for regression output\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Concatenate CLS embeddings\n",
    "        \n",
    "        # Pass through MLP head for salary regression\n",
    "        salary_output = self.mlp(combined_cls)\n",
    "        \n",
    "        # If bin_ids are provided, compute cosine similarity between MASK token and bin embeddings\n",
    "        if bin_ids is not None:\n",
    "            # Retrieve bin embeddings from the BERT model's embedding layer\n",
    "            print(bin_ids)\n",
    "            bin_embedding = self.bert1.embeddings.word_embeddings(bin_ids)  # [batch_size, hidden_size]\n",
    "            print(f\"mask_embedding shape: {mask_embedding.shape}\")\n",
    "            print(f\"bin_embedding shape: {bin_embedding.shape}\")\n",
    "            similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n",
    "            return salary_output, similarity_loss\n",
    "        \n",
    "        return salary_output, None  # For inference, return salary output only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:44.363945Z",
     "iopub.status.busy": "2024-12-16T09:03:44.363598Z",
     "iopub.status.idle": "2024-12-16T09:03:44.395122Z",
     "shell.execute_reply": "2024-12-16T09:03:44.394460Z",
     "shell.execute_reply.started": "2024-12-16T09:03:44.363911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "def fit_eval(\n",
    "    seed,\n",
    "    model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    catboost_preds,\n",
    "    criterion,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    text_col_1 = 'description_no_numbers_v2',\n",
    "    text_col_2 = 'title_company_location_skills_source',\n",
    "):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    memory_cleanup()\n",
    "\n",
    "    # Unpack config\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seq_length = config[\"seq_length\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train, tokenizer, seq_length)\n",
    "    test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test, tokenizer, seq_length)\n",
    "    # Make dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training and Evaluation Loop\n",
    "    history = {\n",
    "                \"train_loss\": [],\n",
    "               \"test_loss\": [], \n",
    "                \"train_rmse\": [],\n",
    "               \"test_rmse\": [], \n",
    "               \"test_rmse_with_catboost\": [],\n",
    "               \"train_r2\": [],\n",
    "               \"test_r2\": [],\n",
    "               \"test_r2_with_catboost\": [],\n",
    "               \"train_mae\": [],\n",
    "               \"test_mae\": [],\n",
    "               \"test_mae_with_catboost\": [],\n",
    "               \"max_test_loss\": float('inf'),\n",
    "               \"max_test_r2\": float('-inf'),\n",
    "               \"max_test_mae\": float('inf'),\n",
    "               \"best_preds\": []\n",
    "               }\n",
    "    \n",
    "    # test_labels = y_test\n",
    "    # test_preds = []\n",
    "    print('Starting training/eval loop...')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting training...')\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            outputs = outputs.flatten()\n",
    "            # loss = criterion(outputs.squeeze(), targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            # all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "            all_preds.extend(outputs.cpu().detach().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "        train_loss = np.mean(train_losses)\n",
    "        \n",
    "        train_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "        train_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_r2\"].append(train_r2)\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"train_mae\"].append(train_mae)\n",
    "    \n",
    "        # Evaluation Phase\n",
    "        print('Epoch done, evaluating...')\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                inputs1, inputs2, targets = batch\n",
    "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "                targets = targets.to(device)\n",
    "    \n",
    "                outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "                outputs = outputs.flatten()\n",
    "                # loss = criterion(outputs.squeeze(), targets)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_losses.append(loss.item())\n",
    "                # all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "        test_loss = np.mean(test_losses)\n",
    "\n",
    "        test_r2 = r2_score(all_labels, all_preds)\n",
    "        test_r2_with_catboost = r2_score(all_labels, (np.array(all_preds) + catboost_preds) / 2)\n",
    "        \n",
    "        test_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        test_rmse_with_catboost = mean_squared_error(all_labels, (np.array(all_preds) + catboost_preds) / 2, squared=False)\n",
    "        \n",
    "        test_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        test_mae_with_catboost = mean_absolute_error(all_labels, (np.array(all_preds) + catboost_preds) / 2)\n",
    "    \n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        \n",
    "        history[\"test_r2\"].append(test_r2)\n",
    "        history[\"test_r2_with_catboost\"].append(test_r2_with_catboost)\n",
    "        \n",
    "        history[\"test_rmse\"].append(test_rmse)\n",
    "        history[\"test_rmse_with_catboost\"].append(test_rmse_with_catboost)\n",
    "        \n",
    "        history[\"test_mae\"].append(test_mae)\n",
    "        history[\"test_mae_with_catboost\"].append(test_mae_with_catboost)\n",
    "        \n",
    "        if test_r2 > history[\"max_test_r2\"]:\n",
    "            history[\"max_test_r2\"] = test_r2\n",
    "            history[\"best_preds\"] = all_preds\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}, Test R2 with catboost: {test_r2_with_catboost:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}, \"\n",
    "              f\"Test MAE with catboost: {test_mae_with_catboost:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def fit_eval_with_bins(\n",
    "    seed,\n",
    "    model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    bin_targets_col,\n",
    "    catboost_preds,\n",
    "    criterion,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    text_col_1 = 'description_no_numbers_v2',\n",
    "    text_col_2 = 'title_company_location_skills_source',\n",
    "):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    memory_cleanup()\n",
    "\n",
    "    # Unpack config\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seq_length = config[\"seq_length\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = DualTextDatasetWithBins(X_train, text_col_1, text_col_2, bin_targets_col, y_train, tokenizer, seq_length)\n",
    "    test_dataset = DualTextDatasetWithBins(X_test, text_col_1, text_col_2, bin_targets_col, y_test, tokenizer, seq_length)\n",
    "    # Make dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training and Evaluation Loop\n",
    "    history = {\n",
    "                \"train_loss\": [],\n",
    "               \"test_loss\": [], \n",
    "                \"train_rmse\": [],\n",
    "               \"test_rmse\": [], \n",
    "               \"test_rmse_with_catboost\": [],\n",
    "               \"train_r2\": [],\n",
    "               \"test_r2\": [],\n",
    "               \"test_r2_with_catboost\": [],\n",
    "               \"train_mae\": [],\n",
    "               \"test_mae\": [],\n",
    "               \"test_mae_with_catboost\": [],\n",
    "               \"max_test_loss\": float('inf'),\n",
    "               \"max_test_r2\": float('-inf'),\n",
    "               \"max_test_mae\": float('inf'),\n",
    "               \"best_preds\": []\n",
    "               }\n",
    "    \n",
    "    # test_labels = y_test\n",
    "    # test_preds = []\n",
    "    print('Starting training/eval loop...')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting training...')\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_dataloader:\n",
    "            inputs1, inputs2, targets, bin_ids = batch  # Now bin_ids are included in the dataset\n",
    "            print(len(inputs1))\n",
    "            print(len(bin_ids))\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "            bin_ids = bin_ids.to(device)  # Ensure bin_ids are moved to the same device\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: include bin_ids\n",
    "            salary_output, similarity_loss = model(input1, attention_mask1, input2, attention_mask2, bin_ids)\n",
    "            loss = criterion(salary_output, targets) + similarity_loss  # Combine regression loss and similarity loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            all_preds.extend(salary_output.cpu().detach().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            # outputs = outputs.flatten()\n",
    "            # # loss = criterion(outputs.squeeze(), targets)\n",
    "            # loss = criterion(outputs, targets)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # train_losses.append(loss.item())\n",
    "            # # all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "            # all_preds.extend(outputs.cpu().detach().numpy())\n",
    "            # all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "        train_loss = np.mean(train_losses)\n",
    "        \n",
    "        train_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "        train_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_r2\"].append(train_r2)\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"train_mae\"].append(train_mae)\n",
    "    \n",
    "        # Evaluation Phase\n",
    "        print('Epoch done, evaluating...')\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                inputs1, inputs2, targets, bin_ids = batch  # Get bin_ids during evaluation\n",
    "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "                targets = targets.to(device)\n",
    "                bin_ids = bin_ids.to(device)  # Ensure bin_ids are moved to the same device\n",
    "    \n",
    "                # Forward pass: include bin_ids\n",
    "                salary_output, similarity_loss = model(input1, attention_mask1, input2, attention_mask2, bin_ids)\n",
    "                loss = criterion(salary_output, targets) + similarity_loss  # Combine losses\n",
    "                test_losses.append(loss.item())\n",
    "                all_preds.extend(salary_output.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "        test_loss = np.mean(test_losses)\n",
    "\n",
    "        test_r2 = r2_score(all_labels, all_preds)\n",
    "        test_r2_with_catboost = r2_score(all_labels, (np.array(all_preds) + catboost_preds) / 2)\n",
    "        \n",
    "        test_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        test_rmse_with_catboost = mean_squared_error(all_labels, (np.array(all_preds) + catboost_preds) / 2, squared=False)\n",
    "        \n",
    "        test_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        test_mae_with_catboost = mean_absolute_error(all_labels, (np.array(all_preds) + catboost_preds) / 2)\n",
    "    \n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        \n",
    "        history[\"test_r2\"].append(test_r2)\n",
    "        history[\"test_r2_with_catboost\"].append(test_r2_with_catboost)\n",
    "        \n",
    "        history[\"test_rmse\"].append(test_rmse)\n",
    "        history[\"test_rmse_with_catboost\"].append(test_rmse_with_catboost)\n",
    "        \n",
    "        history[\"test_mae\"].append(test_mae)\n",
    "        history[\"test_mae_with_catboost\"].append(test_mae_with_catboost)\n",
    "        \n",
    "        if test_r2 > history[\"max_test_r2\"]:\n",
    "            history[\"max_test_r2\"] = test_r2\n",
    "            history[\"best_preds\"] = all_preds\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}, Test R2 with catboost: {test_r2_with_catboost:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}, \"\n",
    "              f\"Test MAE with catboost: {test_mae_with_catboost:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training-eval loop with experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:44.844772Z",
     "iopub.status.busy": "2024-12-16T09:03:44.844155Z",
     "iopub.status.idle": "2024-12-16T09:03:45.218052Z",
     "shell.execute_reply": "2024-12-16T09:03:45.217364Z",
     "shell.execute_reply.started": "2024-12-16T09:03:44.844739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:03:45.256720Z",
     "iopub.status.busy": "2024-12-16T09:03:45.256157Z",
     "iopub.status.idle": "2024-12-16T09:03:48.121783Z",
     "shell.execute_reply": "2024-12-16T09:03:48.120485Z",
     "shell.execute_reply.started": "2024-12-16T09:03:45.256690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting for seed 42...\n",
      "double_huber_multitask model...\n",
      "Creating the dataset...\n",
      "Creating the dataset...\n",
      "Starting training/eval loop...\n",
      "Starting training...\n",
      "3\n",
      "32\n",
      "tensor([83835, 83836, 83836, 83836, 83836, 83836, 83833, 83836, 83836, 83835,\n",
      "        83835, 83835, 83834, 83836, 83836, 83835], device='cuda:0')\n",
      "mask_embedding shape: torch.Size([11, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n",
      "tensor([83835, 83836, 83836, 83836, 83837, 83837, 83836, 83836, 83836, 83833,\n",
      "        83836, 83836, 83836, 83835, 83835, 83837], device='cuda:1')\n",
      "mask_embedding shape: torch.Size([5, 312])\n",
      "bin_embedding shape: torch.Size([16, 312])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_eval_with_bins\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_targets_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatboost_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_col_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_col_1_with_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m memory_cleanup()\n\u001b[1;32m    106\u001b[0m combined_history[seed][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble_huber_multitask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m history    \n",
      "Cell \u001b[0;32mIn[17], line 239\u001b[0m, in \u001b[0;36mfit_eval_with_bins\u001b[0;34m(seed, model, X_train, X_test, y_train, y_test, bin_targets_col, catboost_preds, criterion, tokenizer, config, text_col_1, text_col_2)\u001b[0m\n\u001b[1;32m    237\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Forward pass: include bin_ids\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m salary_output, similarity_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(salary_output, targets) \u001b[38;5;241m+\u001b[39m similarity_loss  \u001b[38;5;66;03m# Combine regression loss and similarity loss\u001b[39;00m\n\u001b[1;32m    241\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_23/2258028468.py\", line 92, in forward\n    similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1299, in forward\n    return F.cosine_embedding_loss(input1, input2, target, margin=self.margin, reduction=self.reduction)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3581, in cosine_embedding_loss\n    return torch.cosine_embedding_loss(input1, input2, target, margin, reduction_enum)\nRuntimeError: The size of tensor a (11) must match the size of tensor b (16) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "# import logging\n",
    "# Set the logging level for the transformers library to ERROR\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "seeds = [42, 78687, 123123]\n",
    "combined_history = {}\n",
    "\n",
    "# Hyperparameters\n",
    "# seq_length = 512\n",
    "# hidden_size = 768  # BERT base hidden size\n",
    "# mlp_hidden_size = 256\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "    \"batch_size\": 32,\n",
    "    \"seq_length\": 1024,\n",
    "    \"hidden_size\": 312,\n",
    "    \"mlp_hidden_size\": 128,\n",
    "    \"num_epochs\": 10,\n",
    "    # \"num_epochs\": 2,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"mask_token_index\": 17,\n",
    "    \n",
    "}\n",
    "\n",
    "memory_cleanup()\n",
    "model_name = config['model_name']\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "\n",
    "# Compute bin edges for 15 equal bins\n",
    "num_bins = 15\n",
    "bin_edges = np.linspace(\n",
    "    df[\"log_salary_from\"].min(), df[\"log_salary_from\"].max(), num_bins + 1\n",
    ")\n",
    "\n",
    "# Assign bins to a new column\n",
    "df[\"salary_bin\"] = np.digitize(df[\"log_salary_from\"], bins=bin_edges, right=True) - 1\n",
    "\n",
    "# Ensure bin labels are within range\n",
    "df[\"salary_bin\"] = df[\"salary_bin\"].clip(0, num_bins - 1)\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "[CLS] Далее указано описание вакансии. \\\n",
    "Судя по описанию, зарплата на этой позиции составляет [MASK].[SEP]\\\n",
    "\"\"\"\n",
    "\n",
    "df['description_no_numbers_v2_with_prompt'] = prompt + df['description_no_numbers_v2']\n",
    "\n",
    "\n",
    "text_col_1_with_prompt = 'description_no_numbers_v2_with_prompt'\n",
    "text_col_1 = 'description_no_numbers_v2'\n",
    "text_col_2 = 'title_company_location_skills_source'\n",
    "bin_targets_col = 'salary_bin'\n",
    "\n",
    "\n",
    "X = df[[text_col_1, text_col_1_with_prompt, text_col_2, bin_targets_col]][:200]\n",
    "y = df['log_salary_from'][:200]\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    memory_cleanup()\n",
    "    print(f'Starting for seed {str(seed)}...')\n",
    "    catboost_preds = catboost_history[seed]['y_pred'][:40]\n",
    "\n",
    "    combined_history[seed] = {}\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # fit-eval non-TSDAE-ed model\n",
    "    # model = DualBERTWithMLP(config)\n",
    "    model = MaskBinDualBERTWithMLP(config)\n",
    "    tokenizer = model.get_tokenizer()\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    # Loss Function\n",
    "    # criterion = nn.MSELoss()  # For regression\n",
    "    criterion = nn.HuberLoss()\n",
    "\n",
    "    print(f'double_huber_multitask model...')\n",
    "    # model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\n",
    "    model, history = fit_eval_with_bins(\n",
    "        seed, model, X_train, X_test, y_train, y_test, bin_targets_col, catboost_preds, criterion, tokenizer, config,\n",
    "        text_col_1 = text_col_1_with_prompt,\n",
    "    )\n",
    "    memory_cleanup()\n",
    "\n",
    "    combined_history[seed]['double_huber_multitask'] = history    \n",
    "\n",
    "    # # further split train data into regression train and tsdae train data\n",
    "    # X_train_tsdae, X_tsdae, y_train_tsdae, y_tsdae = train_test_split(X_train, y_train, test_size=0.01, random_state=seed)\n",
    "\n",
    "    # # convert text_col_1 data into set of sentences and select 20-60 word sentences as a feature column:\n",
    "    # # Create a DataFrame of unique sentences and their lengths for X_tsdae\n",
    "    # unique_sentences = []\n",
    "    # unique_sentence_lengths = []\n",
    "    # for text in X_tsdae[text_col_1]:\n",
    "    #     sentences, sentence_lengths = get_sentence_lengths(text)\n",
    "    #     unique_sentences.extend(sentences)\n",
    "    #     unique_sentence_lengths.extend(sentence_lengths)\n",
    "\n",
    "    # unique_sentences_df = pd.DataFrame({\n",
    "    #     'sentence': unique_sentences,\n",
    "    #     'length': unique_sentence_lengths\n",
    "    # })\n",
    "\n",
    "    # unique_sentences = unique_sentences_df[(unique_sentences_df.length >= 10) & (unique_sentences_df.length <= 60)]['sentence']\n",
    "\n",
    "    # # get array with features for each bert\n",
    "    # train_sentences_array = [\n",
    "    #     unique_sentences.tolist(),\n",
    "    #     # X_tsdae[text_col_1].tolist(),\n",
    "    #     X_tsdae[text_col_2].tolist(),\n",
    "    # ]\n",
    "\n",
    "    # berts_after_tsdae = []\n",
    "    # for index, train_sentences in enumerate(train_sentences_array):\n",
    "    #     memory_cleanup()\n",
    "    #     berts_after_tsdae.append(train_tsdae_bert(model_name, train_sentences))\n",
    "    # memory_cleanup()\n",
    "\n",
    "    # tsdae_bert1, tsdae_bert2 = berts_after_tsdae\n",
    "\n",
    "    # # Initialize the non-TSDAE-ed BERT models\n",
    "    # model = TSDAEDualBERTWithMLP(config, tsdae_bert1, tsdae_bert2)\n",
    "    # model = torch.nn.DataParallel(model).to(device)\n",
    "    # # Loss Function\n",
    "    # # criterion = nn.MSELoss()  # For regression\n",
    "    # criterion = nn.HuberLoss()\n",
    "\n",
    "    # print(f'tsdae model...')\n",
    "    # model, history = fit_eval(seed, model, X_train_tsdae, X_test, y_train_tsdae, y_test, catboost_preds, criterion, tokenizer, config)\n",
    "\n",
    "    # combined_history[seed]['double_huber_1p_sentences_tsdae'] = history\n",
    "\n",
    "# save the history as pickle\n",
    "with open('./models/combined_history_exp_3.pickle', 'wb') as handle:\n",
    "    pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-16T08:39:54.591403Z",
     "iopub.status.idle": "2024-12-16T08:39:54.591704Z",
     "shell.execute_reply": "2024-12-16T08:39:54.591576Z",
     "shell.execute_reply.started": "2024-12-16T08:39:54.591561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the rows\n",
    "rows = []\n",
    "\n",
    "# Iterate through the dictionary\n",
    "for seed, experiments in combined_history.items():\n",
    "    for experiment_name, metrics in experiments.items():\n",
    "        num_epochs = len(metrics['train_loss'])\n",
    "        for epoch in range(num_epochs):\n",
    "            row = {\n",
    "                'random_seed': seed,\n",
    "                'experiment_name': experiment_name,\n",
    "                'epoch': epoch + 1,\n",
    "                \n",
    "                'train_loss': metrics['train_loss'][epoch],\n",
    "                'test_loss': metrics['test_loss'][epoch],\n",
    "                \n",
    "                'train_r2': metrics['train_r2'][epoch],\n",
    "                'test_r2': metrics['test_r2'][epoch],\n",
    "                'test_r2_with_catboost': metrics['test_r2_with_catboost'][epoch]\n",
    "                \n",
    "                'train_rmse': metrics['train_rmse'][epoch],\n",
    "                'test_rmse': metrics['test_rmse'][epoch],\n",
    "                'test_rmse_with_catboost': metrics['test_rmse_with_catboost'][epoch]\n",
    "            \n",
    "                'train_mae': metrics['train_mae'][epoch],\n",
    "                'test_mae': metrics['test_mae'][epoch],\n",
    "                'test_mae_with_catboost': metrics['test_mae_with_catboost'][epoch]\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "# Convert the list of rows into a DataFrame\n",
    "history = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T00:12:09.149945Z",
     "iopub.status.busy": "2024-12-14T00:12:09.149529Z",
     "iopub.status.idle": "2024-12-14T00:12:09.177254Z",
     "shell.execute_reply": "2024-12-14T00:12:09.176403Z",
     "shell.execute_reply.started": "2024-12-14T00:12:09.149883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Group by experiment name and epoch, then calculate mean and std\n",
    "grouped_history = history.groupby(['experiment_name', 'epoch']).agg(\n",
    "    train_loss_mean=('train_loss', 'mean'),\n",
    "    train_loss_std=('train_loss', 'std'),\n",
    "    test_loss_mean=('test_loss', 'mean'),\n",
    "    test_loss_std=('test_loss', 'std'),\n",
    "    train_r2_mean=('train_r2', 'mean'),\n",
    "    train_r2_std=('train_r2', 'std'),\n",
    "    test_r2_mean=('test_r2', 'mean'),\n",
    "    test_r2_std=('test_r2', 'std'),\n",
    "    test_r2_with_catboost_mean=('test_r2_with_catboost', 'mean'),\n",
    "    test_r2_with_catboost_std=('test_r2_with_catboost', 'std')\n",
    ").reset_index()\n",
    "\n",
    "# Display the grouped DataFrame\n",
    "print(\"\\nGrouped DataFrame with Mean and Std:\")\n",
    "print(grouped_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T23:58:43.180037Z",
     "iopub.status.busy": "2024-12-13T23:58:43.179738Z",
     "iopub.status.idle": "2024-12-13T23:58:43.184837Z",
     "shell.execute_reply": "2024-12-13T23:58:43.183820Z",
     "shell.execute_reply.started": "2024-12-13T23:58:43.180008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "# text_col_1 = 'description_no_numbers_v2_with_prompt' # for mask pooling\n",
    "# text_col_2 = 'title_company_location_skills_source_with_prompt' # for mask pooling\n",
    "\n",
    "# X = df[[text_col_1, text_col_2]]\n",
    "# y = df['log_salary_from']\n",
    "\n",
    "# # Split # already done in previous cell\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# model = SingleBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "\n",
    "\n",
    "# word_embedding_model1 = models.Transformer(model_name)\n",
    "# pooling_model1 = models.Pooling(word_embedding_model1.get_word_embedding_dimension(), \"cls\")\n",
    "# bert1 = SentenceTransformer(modules=[word_embedding_model1, pooling_model1])\n",
    "\n",
    "# word_embedding_model2 = models.Transformer(model_name)\n",
    "# pooling_model2 = models.Pooling(word_embedding_model2.get_word_embedding_dimension(), \"cls\")\n",
    "# bert2 = SentenceTransformer(modules=[word_embedding_model2, pooling_model2])\n",
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"./models/final_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
