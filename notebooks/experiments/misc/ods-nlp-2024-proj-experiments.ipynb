{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning transformer + MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:55:44.026555Z",
     "iopub.status.busy": "2024-12-13T10:55:44.026235Z",
     "iopub.status.idle": "2024-12-13T10:55:58.140091Z",
     "shell.execute_reply": "2024-12-13T10:55:58.139148Z",
     "shell.execute_reply.started": "2024-12-13T10:55:44.026518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir models\n",
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1_o4xDSF6j95vAiYdd97VavyWq4EHHPdP&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T11:02:20.304356Z",
     "iopub.status.busy": "2024-12-13T11:02:20.303414Z",
     "iopub.status.idle": "2024-12-13T11:02:23.397112Z",
     "shell.execute_reply": "2024-12-13T11:02:23.396046Z",
     "shell.execute_reply.started": "2024-12-13T11:02:20.304315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1Hysy7OYhugP0lrvn7sCfckNk0AUx2N69&export=download&authuser=1&confirm=t' -O './data/catboost_preds.npy'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "catboost_preds = np.load('./data/catboost_preds.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code example from https://sbert.net/examples/unsupervised_learning/TSDAE/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:55:58.142525Z",
     "iopub.status.busy": "2024-12-13T10:55:58.142192Z",
     "iopub.status.idle": "2024-12-13T10:56:08.277205Z",
     "shell.execute_reply": "2024-12-13T10:56:08.276113Z",
     "shell.execute_reply.started": "2024-12-13T10:55:58.142495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:56:08.440661Z",
     "iopub.status.busy": "2024-12-13T10:56:08.440306Z",
     "iopub.status.idle": "2024-12-13T10:56:24.403004Z",
     "shell.execute_reply": "2024-12-13T10:56:24.402249Z",
     "shell.execute_reply.started": "2024-12-13T10:56:08.440617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "def memory_cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# get number of words in each sentence in the text\n",
    "def get_sentence_lengths(text):\n",
    "    # pattern = r'(?<=[.!?])\\s+'\n",
    "    pattern = r'(?<=[.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    # remove empty strings\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    # get number of words in each sentence\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    return sentences, sentence_lengths\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Plot Training and Validation Metrics\n",
    "def plot(history):\n",
    "    # Plot Loss\n",
    "    plt.plot(range(2, len(history[\"train_loss\"]) + 1), history[\"train_loss\"][1:], label=\"Train Loss\")\n",
    "    plt.plot(range(2, len(history[\"test_loss\"]) + 1), history[\"test_loss\"][1:], label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Test Loss Over Epochs 2-...\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot R2 Score\n",
    "    plt.plot(range(2, len(history[\"train_r2\"]) + 1), history[\"train_r2\"][1:], label=\"Train R2\")\n",
    "    plt.plot(range(2, len(history[\"test_r2\"]) + 1), history[\"test_r2\"][1:], label=\"Test R2\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"R2 Score\")\n",
    "    plt.title(\"Train/Test R2 Score Over Epochs 2-...\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to train the TSDAE model\n",
    "def train_tsdae_bert(model_name, train_sentences):\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # Create the special denoising dataset that adds noise on-the-fly\n",
    "    train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "    \n",
    "    # DataLoader to batch your data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Use the denoising auto-encoder loss\n",
    "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "        model, decoder_name_or_path=model_name, tie_encoder_decoder=True,\n",
    "    )\n",
    "    \n",
    "    # Call the fit method\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        weight_decay=0,\n",
    "        scheduler=\"constantlr\",\n",
    "        optimizer_params={\"lr\": 3e-5},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traning-related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:56:24.405030Z",
     "iopub.status.busy": "2024-12-13T10:56:24.404427Z",
     "iopub.status.idle": "2024-12-13T10:56:24.424027Z",
     "shell.execute_reply": "2024-12-13T10:56:24.423095Z",
     "shell.execute_reply.started": "2024-12-13T10:56:24.405000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n",
    "\n",
    "# single bert\n",
    "class SingleBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(SingleBERTWithMLP, self).__init__()\n",
    "        # Initialize a single BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),  # Double hidden size for concatenation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Pass both inputs through the same BERT model\n",
    "        cls1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token for input1\n",
    "        cls2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token for input2\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        cls1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token\n",
    "        # mask1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 23, :]  # mask token \n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "        # mask2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 23, :]  # mask token\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "        # concat mask embeddings\n",
    "        # combined_mask = torch.cat([mask1, mask2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        # output = self.mlp(combined_mask)\n",
    "        return output\n",
    "\n",
    "# Define the dual BERT model with an MLP head and MASK pooling\n",
    "class MASKPoolDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, mlp_hidden_size):\n",
    "        super(MASKPoolDualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        mask1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 23, :]  # mask token \n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        mask2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 23, :]  # mask token\n",
    "\n",
    "        # concat mask embeddings\n",
    "        combined_mask = torch.cat([mask1, mask2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_mask)\n",
    "        return output\n",
    "\n",
    "# Define the dual BERT model with an MLP head\n",
    "class TSDAEDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config, bert1, bert2):\n",
    "        super(TSDAEDualBERTWithMLP, self).__init__()\n",
    "        # Load TSDAE-ed BERT models\n",
    "        self.bert1 = bert1\n",
    "        self.bert2 = bert2\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # idea from: https://github.com/UKPLab/sentence-transformers/issues/2494\n",
    "        # Forward pass through BERT1\n",
    "        input_dict1 = {\n",
    "            'input_ids': input1,\n",
    "            'attention_mask': attention_mask1\n",
    "        }\n",
    "        cls1 = self.bert1(input_dict1)['sentence_embedding']\n",
    "        \n",
    "        # Forward pass through BERT2\n",
    "        input_dict2 = {\n",
    "            'input_ids': input2,\n",
    "            'attention_mask': attention_mask2\n",
    "        }\n",
    "        cls2 = self.bert2(input_dict2)['sentence_embedding']\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def fit_eval(\n",
    "    seed,\n",
    "    model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    catboost_preds,\n",
    "    criterion,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    text_col_1 = 'description_no_numbers_v2',\n",
    "    text_col_2 = 'title_company_location_skills_source',\n",
    "):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    memory_cleanup()\n",
    "\n",
    "    # Unpack config\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    seq_length = config[\"seq_length\"]\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train, tokenizer, seq_length)\n",
    "    test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test, tokenizer, seq_length)\n",
    "    # Make dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        \n",
    "    # Training and Evaluation Loop\n",
    "    history = {\"train_loss\": [],\n",
    "               \"test_loss\": [], \n",
    "               \"train_r2\": [],\n",
    "               \"test_r2\": [],\n",
    "               \"test_r2_with_catboost\": [],\n",
    "               \"max_test_r2\": float('-inf'),\n",
    "               \"best_preds\": []\n",
    "               }\n",
    "    \n",
    "    # test_labels = y_test\n",
    "    # test_preds = []\n",
    "    print('Starting training/eval loop...')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting training...')\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            outputs = outputs.flatten()\n",
    "            # loss = criterion(outputs.squeeze(), targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            # all_preds.extend(outputs.squeeze().cpu().detach().numpy())\n",
    "            all_preds.extend(outputs.cpu().detach().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_r2 = r2_score(all_labels, all_preds)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_r2\"].append(train_r2)\n",
    "    \n",
    "        # Evaluation Phase\n",
    "        print('Epoch done, evaluating...')\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                inputs1, inputs2, targets = batch\n",
    "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "                targets = targets.to(device)\n",
    "    \n",
    "                outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "                outputs = outputs.flatten()\n",
    "                # loss = criterion(outputs.squeeze(), targets)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_losses.append(loss.item())\n",
    "                # all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "        test_loss = np.mean(test_losses)\n",
    "        test_r2 = r2_score(all_labels, all_preds)\n",
    "        test_r2_with_catboost = r2_score(all_labels, (np.array(all_preds) + catboost_preds) / 2)\n",
    "    \n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"test_r2\"].append(test_r2)\n",
    "        history[\"test_r2_with_catboost\"].append(test_r2_with_catboost)\n",
    "    \n",
    "        if test_r2 > history[\"max_test_r2\"]:\n",
    "            history[\"max_test_r2\"] = test_r2\n",
    "            history[\"best_preds\"] = all_preds\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}, Test R2 with catboost: {test_r2_with_catboost:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training-eval loop with experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pickle\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "seeds = [42, 78687, 123123]\n",
    "repetitions = 2\n",
    "combined_history = {}\n",
    "\n",
    "# Hyperparameters\n",
    "# seq_length = 512\n",
    "# hidden_size = 768  # BERT base hidden size\n",
    "# mlp_hidden_size = 256\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "    \"batch_size\": 32,\n",
    "    \"seq_length\": 1024,\n",
    "    \"hidden_size\": 312,\n",
    "    \"mlp_hidden_size\": 128,\n",
    "    # \"num_epochs\": 10,\n",
    "    \"num_epochs\": 2,\n",
    "    \"learning_rate\": 5e-6,\n",
    "}\n",
    "\n",
    "model_name = config['model_name']\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "text_col_1 = 'description_no_numbers_v2'\n",
    "text_col_2 = 'title_company_location_skills_source'\n",
    "\n",
    "\n",
    "X = df[[text_col_1, text_col_2]][:200]\n",
    "y = df['log_salary_from'][:200]\n",
    "\n",
    "for _ in range(repetitions):\n",
    "    for seed in seeds:\n",
    "\n",
    "        combined_history[seed] = {}\n",
    "\n",
    "        set_seed(seed)\n",
    "\n",
    "        # Split train-test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "        catboost_preds = np.zeros(len(y_test)) # dont use catboost preds for now\n",
    "\n",
    "        # fit-eval non-TSDAE-ed model\n",
    "        model = DualBERTWithMLP(config)\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "        # Loss Function\n",
    "        # criterion = nn.MSELoss()  # For regression\n",
    "        criterion = nn.HuberLoss()\n",
    "\n",
    "        model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\n",
    "\n",
    "        combined_history[seed]['double_huber'] = history    \n",
    "\n",
    "        # further split train data into regression train and tsdae train data\n",
    "        X_train_tsdae, X_tsdae, y_train_tsdae, y_tsdae = train_test_split(X_train, y_train, test_size=0.01, random_state=seed)\n",
    "\n",
    "        # # convert text_col_1 data into set of sentences and select 20-60 word sentences as a feature column:\n",
    "        # # Create a DataFrame of unique sentences and their lengths for X_tsdae\n",
    "        # unique_sentences = []\n",
    "        # unique_sentence_lengths = []\n",
    "        # for text in X_tsdae[text_col_1]:\n",
    "        #     sentences, sentence_lengths = get_sentence_lengths(text)\n",
    "        #     unique_sentences.extend(sentences)\n",
    "        #     unique_sentence_lengths.extend(sentence_lengths)\n",
    "\n",
    "        # unique_sentences_df = pd.DataFrame({\n",
    "        #     'sentence': unique_sentences,\n",
    "        #     'length': unique_sentence_lengths\n",
    "        # })\n",
    "\n",
    "        # unique_sentences = unique_sentences_df[(unique_sentences_df.length >= 10) & (unique_sentences_df.length <= 60)]['sentence']\n",
    "\n",
    "        # get array with features for each bert\n",
    "        train_sentences_array = [\n",
    "            # unique_sentences.tolist(),\n",
    "            X_tsdae[text_col_1].tolist(),\n",
    "            X_tsdae[text_col_2].tolist(),\n",
    "        ]\n",
    "\n",
    "        berts_after_tsdae = []\n",
    "        for index, train_sentences in enumerate(train_sentences_array):\n",
    "            berts_after_tsdae.append(train_tsdae_bert(model_name, train_sentences))\n",
    "            memory_cleanup()\n",
    "\n",
    "        tsdae_bert1, tsdae_bert2 = berts_after_tsdae\n",
    "\n",
    "        # Initialize the non-TSDAE-ed BERT models\n",
    "        model = TSDAEDualBERTWithMLP(config, tsdae_bert1, tsdae_bert2)\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "        # Loss Function\n",
    "        # criterion = nn.MSELoss()  # For regression\n",
    "        criterion = nn.HuberLoss()\n",
    "\n",
    "        model, history = fit_eval(seed, model, X_train, X_test, y_train, y_test, catboost_preds, criterion, tokenizer, config)\n",
    "\n",
    "        combined_history[seed]['double_huber_1p_descriptions_tsdae'] = history\n",
    "\n",
    "    # save the history as pickle\n",
    "    with open('./models/combined_history.pickle', 'wb') as handle:\n",
    "        pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:59:46.383937Z",
     "iopub.status.busy": "2024-12-13T10:59:46.383412Z",
     "iopub.status.idle": "2024-12-13T11:00:09.114741Z",
     "shell.execute_reply": "2024-12-13T11:00:09.113575Z",
     "shell.execute_reply.started": "2024-12-13T10:59:46.383903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "# text_col_1 = 'description_no_numbers_v2_with_prompt' # for mask pooling\n",
    "# text_col_2 = 'title_company_location_skills_source_with_prompt' # for mask pooling\n",
    "\n",
    "# X = df[[text_col_1, text_col_2]]\n",
    "# y = df['log_salary_from']\n",
    "\n",
    "# # Split # already done in previous cell\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# model = SingleBERTWithMLP(hidden_size, mlp_hidden_size)\n",
    "\n",
    "\n",
    "# word_embedding_model1 = models.Transformer(model_name)\n",
    "# pooling_model1 = models.Pooling(word_embedding_model1.get_word_embedding_dimension(), \"cls\")\n",
    "# bert1 = SentenceTransformer(modules=[word_embedding_model1, pooling_model1])\n",
    "\n",
    "# word_embedding_model2 = models.Transformer(model_name)\n",
    "# pooling_model2 = models.Pooling(word_embedding_model2.get_word_embedding_dimension(), \"cls\")\n",
    "# bert2 = SentenceTransformer(modules=[word_embedding_model2, pooling_model2])\n",
    "\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), \"./models/final_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
