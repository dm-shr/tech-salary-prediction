{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:22:41.832782Z",
     "iopub.status.busy": "2024-12-23T15:22:41.832442Z",
     "iopub.status.idle": "2024-12-23T15:22:49.663745Z",
     "shell.execute_reply": "2024-12-23T15:22:49.662747Z",
     "shell.execute_reply.started": "2024-12-23T15:22:41.832736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "# !mkdir data/history\n",
    "# !wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1iqg1FIPfbrZWlung6gZqve1MeQWc0Je4&export=download&authuser=1&confirm=t' -O './data/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:22:49.666696Z",
     "iopub.status.busy": "2024-12-23T15:22:49.666002Z",
     "iopub.status.idle": "2024-12-23T15:22:53.259053Z",
     "shell.execute_reply": "2024-12-23T15:22:53.258053Z",
     "shell.execute_reply.started": "2024-12-23T15:22:49.666659Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22224 entries, 0 to 22223\n",
      "Data columns (total 15 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   title                                 22224 non-null  object \n",
      " 1   company                               22224 non-null  object \n",
      " 2   location                              22224 non-null  object \n",
      " 3   skills                                14384 non-null  object \n",
      " 4   source                                22224 non-null  object \n",
      " 5   description_no_numbers_with_skills    22224 non-null  object \n",
      " 6   experience_from                       22224 non-null  float64\n",
      " 7   experience_to_adjusted_10             22224 non-null  float64\n",
      " 8   description_size                      22224 non-null  int64  \n",
      " 9   description                           22224 non-null  object \n",
      " 10  description_no_numbers                22224 non-null  object \n",
      " 11  description_no_numbers_v2             22224 non-null  object \n",
      " 12  title_company_location_skills_source  22224 non-null  object \n",
      " 13  salary_from                           22224 non-null  float64\n",
      " 14  log_salary_from                       22224 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(10)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:22:53.260498Z",
     "iopub.status.busy": "2024-12-23T15:22:53.260172Z",
     "iopub.status.idle": "2024-12-23T15:23:04.011729Z",
     "shell.execute_reply": "2024-12-23T15:23:04.010671Z",
     "shell.execute_reply.started": "2024-12-23T15:22:53.260468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentence-transformers datasets -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:04.013621Z",
     "iopub.status.busy": "2024-12-23T15:23:04.013305Z",
     "iopub.status.idle": "2024-12-23T15:23:05.826869Z",
     "shell.execute_reply": "2024-12-23T15:23:05.825976Z",
     "shell.execute_reply.started": "2024-12-23T15:23:04.013588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:05.829359Z",
     "iopub.status.busy": "2024-12-23T15:23:05.828930Z",
     "iopub.status.idle": "2024-12-23T15:23:24.089444Z",
     "shell.execute_reply": "2024-12-23T15:23:24.088728Z",
     "shell.execute_reply.started": "2024-12-23T15:23:05.829332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "# from numba import cuda\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"Clean up memory\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def get_sentence_lengths(text):\n",
    "    \"Get number of words in each sentence in the text\"\n",
    "    pattern = r'(?<=[.!?])'\n",
    "    sentences = re.split(pattern, text)\n",
    "    \n",
    "    # remove empty strings\n",
    "    sentences = [sentence for sentence in sentences if len(sentence) > 0]\n",
    "    \n",
    "    # get number of words in each sentence\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "    \n",
    "    return sentences, sentence_lengths\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"Set seed for reproducibility\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def train_tsdae_bert(model_name, train_sentences):\n",
    "    \"\"\"Train a denoising auto-encoder model with BERT model.\n",
    "    more examples at https://sbert.net/examples/unsupervised_learning/TSDAE/README.html\"\"\"\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # Create the special denoising dataset that adds noise on-the-fly\n",
    "    train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "    \n",
    "    # DataLoader to batch your data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Use the denoising auto-encoder loss\n",
    "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "        model, decoder_name_or_path=model_name, tie_encoder_decoder=True,\n",
    "    )\n",
    "    \n",
    "    # Call the fit method\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,\n",
    "        weight_decay=0,\n",
    "        scheduler=\"constantlr\",\n",
    "        optimizer_params={\"lr\": 3e-5},\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.090963Z",
     "iopub.status.busy": "2024-12-23T15:23:24.090459Z",
     "iopub.status.idle": "2024-12-23T15:23:24.102861Z",
     "shell.execute_reply": "2024-12-23T15:23:24.101909Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.090935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def display_metrics_with_ci(history: dict):\n",
    "    # plot mean and ci for train and test r2 for all seeds and all iterations, averaged over seeds\n",
    "    seeds = list(history.keys())\n",
    "    def mean_confidence_interval(data, confidence=0.95):\n",
    "        n = len(data)\n",
    "        m, se = np.mean(data), np.std(data) / np.sqrt(n)\n",
    "        h = se * t.ppf((1 + confidence) / 2, n-1)\n",
    "        return m, m-h, m+h\n",
    "\n",
    "    r2_train_values = [history[seed]['train_r2'] for seed in seeds]\n",
    "    r2_test_values = [history[seed]['test_r2'] for seed in seeds]\n",
    "\n",
    "    r2_train_values = np.array(r2_train_values)\n",
    "    r2_test_values = np.array(r2_test_values)\n",
    "\n",
    "    r2_train_mean = np.mean(r2_train_values, axis=0)\n",
    "    r2_test_mean = np.mean(r2_test_values, axis=0)\n",
    "\n",
    "    r2_train_ci = np.array([mean_confidence_interval(r2_train_values[:, i]) for i in range(r2_train_values.shape[1])])\n",
    "    r2_test_ci = np.array([mean_confidence_interval(r2_test_values[:, i]) for i in range(r2_test_values.shape[1])])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(r2_train_mean[1:], label='train')\n",
    "    plt.fill_between(range(len(r2_train_mean[1:])), r2_train_ci[1:, 1], r2_train_ci[1:, 2], alpha=0.3)\n",
    "\n",
    "    plt.plot(r2_test_mean[1:], label='test')\n",
    "    plt.fill_between(range(len(r2_test_mean[1:])), r2_test_ci[1:, 1], r2_test_ci[1:, 2], alpha=0.3)\n",
    "    plt.title('Mean R2 by epoch, with 95% CI')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    mae_test_values = [history[seed]['test_mae'] for seed in seeds]\n",
    "    rmse_test_values = [history[seed]['test_rmse'] for seed in seeds]\n",
    "\n",
    "    mae_test_values = np.array(mae_test_values)\n",
    "    rmse_test_values = np.array(rmse_test_values)\n",
    "\n",
    "    mae_test_mean = np.mean(mae_test_values, axis=0)\n",
    "    rmse_test_mean = np.mean(rmse_test_values, axis=0)\n",
    "\n",
    "    mae_test_ci = np.array([mean_confidence_interval(mae_test_values[:, i]) for i in range(mae_test_values.shape[1])])\n",
    "    rmse_test_ci = np.array([mean_confidence_interval(rmse_test_values[:, i]) for i in range(rmse_test_values.shape[1])])\n",
    "\n",
    "    # get an index of the epoch, where the test R2 is the highest\n",
    "    # get mean and CI for this epoch\n",
    "    best_epoch = np.argmax(r2_test_mean)\n",
    "    best_epoch_r2 = r2_test_mean[best_epoch]\n",
    "    best_epoch_mae = mae_test_mean[best_epoch]\n",
    "    best_epoch_rmse = rmse_test_mean[best_epoch]\n",
    "    best_epoch_r2_ci = r2_test_ci[best_epoch]\n",
    "    best_epoch_mae_ci = mae_test_ci[best_epoch]\n",
    "    best_epoch_rmse_ci = rmse_test_ci[best_epoch]\n",
    "\n",
    "    print(f'TEST METRICS FOR THE BEST EPOCH (#{best_epoch+1})')\n",
    "    print(f'R2: mean = {best_epoch_r2:.4f}, 95% CI = [{best_epoch_r2_ci[1]:.4f}, {best_epoch_r2_ci[2]:.4f}]')\n",
    "    print(f'MAE: mean = {best_epoch_mae:.4f}, 95% CI = [{best_epoch_mae_ci[1]:.4f}, {best_epoch_mae_ci[2]:.4f}]')\n",
    "    print(f'RMSE: mean = {best_epoch_rmse:.4f}, 95% CI = [{best_epoch_rmse_ci[1]:.4f}, {best_epoch_rmse_ci[2]:.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning-related classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.104442Z",
     "iopub.status.busy": "2024-12-23T15:23:24.104102Z",
     "iopub.status.idle": "2024-12-23T15:23:24.128006Z",
     "shell.execute_reply": "2024-12-23T15:23:24.127248Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.104413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dataset for dual textual features\n",
    "class DualTextDataset(Dataset):\n",
    "    def __init__(self, df, text_col_1, text_col_2, targets, tokenizer, max_len1, max_len2):\n",
    "        print('Creating the dataset...')\n",
    "        # Pre-tokenize and store inputs\n",
    "        self.tokenized_texts1 = tokenizer(df[text_col_1].tolist(), max_length=max_len1, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # self.tokenized_texts2 = tokenizer(df[text_col_2].tolist(), max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        self.targets = targets.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return only the slice for idx\n",
    "        inputs1 = {key: val[idx] for key, val in self.tokenized_texts1.items()}\n",
    "        inputs2 = {key: val[idx] for key, val in self.tokenized_texts2.items()}\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return inputs1, inputs2, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single-head BERT with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.145195Z",
     "iopub.status.busy": "2024-12-23T15:23:24.144895Z",
     "iopub.status.idle": "2024-12-23T15:23:24.157814Z",
     "shell.execute_reply": "2024-12-23T15:23:24.156961Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.145170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# single bert for two text features with MLP head for regression\n",
    "class SingleBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SingleBERTWithMLP, self).__init__()\n",
    "        model_name = config[\"model_name\"]\n",
    "        # Initialize a single BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        mlp_hidden_size = config[\"mlp_hidden_size\"]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),  # Double hidden size for concatenation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Pass both inputs through the same BERT model\n",
    "        cls1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token for input1\n",
    "        cls2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token for input2\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.159584Z",
     "iopub.status.busy": "2024-12-23T15:23:24.158882Z",
     "iopub.status.idle": "2024-12-23T15:23:24.174650Z",
     "shell.execute_reply": "2024-12-23T15:23:24.173945Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.159556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# two berts for two text features with MLP head for regression\n",
    "class DualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DualBERTWithMLP, self).__init__()\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        outputs1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state\n",
    "        print('shape of output1: ', outputs1.shape)\n",
    "        print('shape of att mask1: ', attention_mask1.shape)\n",
    "        cls1 = outputs1[:, 0, :]\n",
    "        # cls1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        outputs2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state  # CLS token\n",
    "        print('shape of output2: ', outputs2.shape)\n",
    "        print('shape of att mask2: ', attention_mask2.shape)\n",
    "        cls2 = outputs2[:, 0, :]\n",
    "        # cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with [MASK] token-based regression and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.175963Z",
     "iopub.status.busy": "2024-12-23T15:23:24.175640Z",
     "iopub.status.idle": "2024-12-23T15:23:24.192101Z",
     "shell.execute_reply": "2024-12-23T15:23:24.191137Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.175928Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# two berts for two text features with MLP head for regression over MASK token embedding\n",
    "class MASKPoolDualBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MASKPoolDualBERTWithMLP, self).__init__()\n",
    "        model_name = config['model_name']\n",
    "        # Initialize two independent BERT models\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mask_token_index = config['mask_token_index']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward pass through BERT1\n",
    "        output1 = self.bert1(input_ids=input1, attention_mask=attention_mask1).last_hidden_state\n",
    "        cls1 = output1[:, 0, :]\n",
    "        mask1 = output1[:, self.mask_token_index, :]\n",
    "\n",
    "        # Forward pass through BERT2\n",
    "        output2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state\n",
    "        cls2 = output2[:, 0, :]\n",
    "\n",
    "        # concat mask embeddings\n",
    "        combined_mask = torch.cat([cls1, mask1, cls2], dim=-1)  # Shape: [batch_size, 3 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "# one bert for two text features with MLP head for regression over MASK token embedding\n",
    "class MASKPoolSingleBERTWithMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MASKPoolSingleBERTWithMLP, self).__init__()\n",
    "        model_name = config['model_name']\n",
    "        # Initialize a BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mask_token_index = config['mask_token_index']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(3 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # Forward passes\n",
    "        output1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state\n",
    "        cls1 = output1[:, 0, :]\n",
    "        mask1 = output1[:, self.mask_token_index, :]\n",
    "\n",
    "        output2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state\n",
    "        cls2 = output2[:, 0, :]\n",
    "\n",
    "        # concat mask embeddings\n",
    "        combined_mask = torch.cat([cls1, mask1, cls2], dim=-1)  # Shape: [batch_size, 3 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with TSDAE pre-tuning and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.194052Z",
     "iopub.status.busy": "2024-12-23T15:23:24.193115Z",
     "iopub.status.idle": "2024-12-23T15:23:24.208495Z",
     "shell.execute_reply": "2024-12-23T15:23:24.207822Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.194011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TSDAEDualBERTWithMLP(nn.Module):\n",
    "    \"\"\"two berts for two text features with MLP head for regression.\n",
    "    The model is pre-tuned with TSDAE.\"\"\"\n",
    "    def __init__(self, config, bert1, bert2):\n",
    "        super(TSDAEDualBERTWithMLP, self).__init__()\n",
    "        # Load TSDAE-ed BERT models\n",
    "        self.bert1 = bert1\n",
    "        self.bert2 = bert2\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2):\n",
    "        # idea from: https://github.com/UKPLab/sentence-transformers/issues/2494\n",
    "        # Forward pass through BERT1\n",
    "        input_dict1 = {\n",
    "            'input_ids': input1,\n",
    "            'attention_mask': attention_mask1\n",
    "        }\n",
    "        cls1 = self.bert1(input_dict1)['sentence_embedding']\n",
    "        \n",
    "        # Forward pass through BERT2\n",
    "        input_dict2 = {\n",
    "            'input_ids': input2,\n",
    "            'attention_mask': attention_mask2\n",
    "        }\n",
    "        cls2 = self.bert2(input_dict2)['sentence_embedding']\n",
    "\n",
    "        # Concatenate CLS embeddings\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Shape: [batch_size, 2 * hidden_size]\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined_cls)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Double-head BERT with multitask learning and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.209634Z",
     "iopub.status.busy": "2024-12-23T15:23:24.209350Z",
     "iopub.status.idle": "2024-12-23T15:23:24.223859Z",
     "shell.execute_reply": "2024-12-23T15:23:24.223040Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.209609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# two berts for two text features with MLP head for regression\n",
    "class MaskBinDualBERTWithMLP(nn.Module):\n",
    "    \"\"\"two berts for two text features with regression over CLS token\n",
    "    and cosine similarity loss between MASK token and bin embeddings.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(MaskBinDualBERTWithMLP, self).__init__()\n",
    "\n",
    "        # Initialize two independent BERT models\n",
    "        model_name = config['model_name']\n",
    "        self.bert1 = AutoModel.from_pretrained(model_name)\n",
    "        self.bert2 = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Add new bin tokens to the tokenizer and resize the model embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        new_tokens = [\"[BIN_0]\", \"[BIN_1]\", \"[BIN_2]\", \"[BIN_3]\", \"[BIN_4]\", \"[BIN_5]\", \"[BIN_6]\", \"[BIN_7]\",\n",
    "                      \"[BIN_8]\", \"[BIN_9]\", \"[BIN_10]\", \"[BIN_11]\", \"[BIN_12]\", \"[BIN_13]\", \"[BIN_14]\"]\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "        self.bert1.resize_token_embeddings(len(self.tokenizer))  # Resize the model to accommodate new tokens\n",
    "        self.mask_token_index = config['mask_token_index']\n",
    "\n",
    "        # Define MLP head for regression\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "        # CosineEmbeddingLoss setup for MASK token prediction\n",
    "        self.criterion_similarity = nn.CosineEmbeddingLoss(margin=0.0)\n",
    "\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "    def forward(self, input1, attention_mask1, input2, attention_mask2, bin_ids=None):\n",
    "        # Forward pass through BERT1 (for MASK token prediction and bin prediction)\n",
    "        outputs1 = self.bert1(input_ids=input1, attention_mask=attention_mask1)\n",
    "        cls1 = outputs1.last_hidden_state[:, 0, :]  # CLS token\n",
    "        mask_idx = (input2 == self.mask_token_index).nonzero(as_tuple=True)  # Assume [MASK] token is at index self.mask_token_index\n",
    "        mask_embedding = outputs1.last_hidden_state[mask_idx]  # Mask token embedding\n",
    "\n",
    "        # Forward pass through BERT2 (for CLS token regression)\n",
    "        cls2 = self.bert2(input_ids=input2, attention_mask=attention_mask2).last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Concatenate CLS tokens from BERT1 and BERT2 and pass through MLP for regression output\n",
    "        combined_cls = torch.cat([cls1, cls2], dim=-1)  # Concatenate CLS embeddings\n",
    "        \n",
    "        # Pass through MLP head for salary regression\n",
    "        salary_output = self.mlp(combined_cls)\n",
    "        \n",
    "        # If bin_ids are provided, compute cosine similarity between MASK token and bin embeddings\n",
    "        if bin_ids is not None:\n",
    "            # Retrieve bin embeddings from the BERT model's embedding layer\n",
    "            print(bin_ids)\n",
    "            bin_embedding = self.bert1.embeddings.word_embeddings(bin_ids)  # [batch_size, hidden_size]\n",
    "            print(f\"mask_embedding shape: {mask_embedding.shape}\")\n",
    "            print(f\"bin_embedding shape: {bin_embedding.shape}\")\n",
    "            similarity_loss = self.criterion_similarity(mask_embedding, bin_embedding, torch.ones(mask_embedding.size(0)))\n",
    "            return salary_output, similarity_loss\n",
    "        \n",
    "        return salary_output, None  # For inference, return salary output only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.228019Z",
     "iopub.status.busy": "2024-12-23T15:23:24.227753Z",
     "iopub.status.idle": "2024-12-23T15:23:24.242849Z",
     "shell.execute_reply": "2024-12-23T15:23:24.242028Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.227969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "def fit_eval(\n",
    "    seed,\n",
    "    model,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train_reg,\n",
    "    y_test_reg,\n",
    "    criterion,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    text_col_1,\n",
    "    text_col_2,\n",
    "):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    memory_cleanup()\n",
    "\n",
    "    # Unpack config\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    if \"seq_length\" in config:\n",
    "        seq_length1 = config[\"seq_length\"]\n",
    "        seq_length2 = config[\"seq_length\"]\n",
    "    else:\n",
    "        seq_length1 = config[\"seq_length_1\"]\n",
    "        seq_length2 = config[\"seq_length_2\"]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make datasets\n",
    "    train_dataset = DualTextDataset(X_train, text_col_1, text_col_2, y_train_reg, tokenizer, seq_length1, seq_length2)\n",
    "    test_dataset = DualTextDataset(X_test, text_col_1, text_col_2, y_test_reg, tokenizer, seq_length1, seq_length2)\n",
    "    # Make dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training and Evaluation Loop\n",
    "    history = {\n",
    "                \"train_loss\": [],\n",
    "               \"test_loss\": [], \n",
    "                \"train_rmse\": [],\n",
    "               \"test_rmse\": [], \n",
    "               \"train_r2\": [],\n",
    "               \"train_r2\": [],\n",
    "               \"test_r2\": [],\n",
    "               \"train_mae\": [],\n",
    "               \"test_mae\": [],\n",
    "               \"y_pred\": [],\n",
    "               \"y_test\": [],\n",
    "               }\n",
    "\n",
    "    print('Starting training/eval loop...')\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting training...')\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_dataloader:\n",
    "            inputs1, inputs2, targets = batch\n",
    "            input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "            input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "            outputs = outputs.flatten()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            all_preds.extend(outputs.cpu().detach().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "        train_loss = np.mean(train_losses)\n",
    "        \n",
    "        train_r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "        train_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_r2\"].append(train_r2)\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"train_mae\"].append(train_mae)\n",
    "    \n",
    "        # Evaluation Phase\n",
    "        print('Epoch done, evaluating...')\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                inputs1, inputs2, targets = batch\n",
    "                input1 = inputs1[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask1 = inputs1[\"attention_mask\"].squeeze(1).to(device)\n",
    "                input2 = inputs2[\"input_ids\"].squeeze(1).to(device)\n",
    "                attention_mask2 = inputs2[\"attention_mask\"].squeeze(1).to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(input1, attention_mask1, input2, attention_mask2)\n",
    "                outputs = outputs.flatten()\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "        history[\"y_pred\"].append(all_preds)\n",
    "        history[\"y_test\"].append(all_labels)\n",
    "\n",
    "        test_loss = np.mean(test_losses)\n",
    "\n",
    "        test_r2 = r2_score(all_labels, all_preds)\n",
    "        \n",
    "        test_rmse = mean_squared_error(all_labels, all_preds, squared=False)\n",
    "        \n",
    "        test_mae = mean_absolute_error(all_labels, all_preds)\n",
    "    \n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        \n",
    "        history[\"test_r2\"].append(test_r2)\n",
    "        \n",
    "        history[\"test_rmse\"].append(test_rmse)\n",
    "        \n",
    "        history[\"test_mae\"].append(test_mae)\n",
    "    \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single bert, cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.264890Z",
     "iopub.status.busy": "2024-12-23T15:23:24.264633Z",
     "iopub.status.idle": "2024-12-23T15:23:24.279877Z",
     "shell.execute_reply": "2024-12-23T15:23:24.279167Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.264859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CrossAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=8): \n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.attention(query, key, value, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "        # nan_count = torch.isnan(attn_output).sum().item()\n",
    "        # print('nan count after attention,', nan_count)\n",
    "        \n",
    "        # Add residual connection and layer norm\n",
    "        output = self.layer_norm(query + attn_output)  # residual connection with query\n",
    "        return output\n",
    "\n",
    "\n",
    "class SingleBERTWithCrossAttention(nn.Module):\n",
    "    \"\"\"Single BERT model with cross-attention between the two text features.\n",
    "    Here, query is the first text feature and key, value are the second text feature.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(SingleBERTWithCrossAttention, self).__init__()\n",
    "        model_name = config['model_name']\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Define MLP head\n",
    "        hidden_size = config['hidden_size']\n",
    "        mlp_hidden_size = config['mlp_hidden_size']\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_size, mlp_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size, 1)  # Regression output\n",
    "        )\n",
    "\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.cross_attention = CrossAttentionLayer(hidden_size, num_heads=self.num_heads)\n",
    "      \n",
    "    def forward(self, input1: torch.Tensor, attention_mask1: torch.Tensor, input2: torch.Tensor, attention_mask2: torch.Tensor):\n",
    "        # Get BERT outputs\n",
    "        outputs1 = self.bert(input_ids=input1, attention_mask=attention_mask1).last_hidden_state\n",
    "        outputs2 = self.bert(input_ids=input2, attention_mask=attention_mask2).last_hidden_state\n",
    "        \n",
    "        # Get raw CLS token before attention\n",
    "        cls2 = outputs2[:, 0, :]\n",
    "\n",
    "        # prepare key_padding_mask\n",
    "        key_padding_mask = (attention_mask2 == 0).bool() # True indicates positions to exclude (padding tokens). Shape: (batch_size, source_len)\n",
    "\n",
    "        # prepare attention mask\n",
    "        # # Step 1: Expand to match query and key dimensions\n",
    "        # original shape: (batch_size, target_len)\n",
    "        attn_mask = attention_mask1.unsqueeze(2) # create a new dimension at the end to be able to expand. Shape: (batch_size, target_len, 1)\n",
    "        attn_mask = attn_mask.expand(-1, -1, attention_mask2.size(1)) # expand to match the source_len. Shape: (batch_size, target_len, source_len)\n",
    "\n",
    "        # # Step 2: Adjust for multi-head attention\n",
    "        attn_mask = attn_mask.unsqueeze(1) # Add head dimension at position 1. Shape: (batch_size, 1, target_len, source_len)\n",
    "        attn_mask = attn_mask.repeat(1, self.num_heads, 1, 1)  # Repeat for each head. Shape: (batch_size, num_heads, target_len, source_len)\n",
    "        attn_mask = attn_mask.view(-1, attention_mask1.size(1), attention_mask2.size(1))  # Merge batch and head dimensions. Shape: (batch_size * num_heads, target_len, source_len)\n",
    "\n",
    "        # # Step 3: Convert to boolean mask\n",
    "        attn_mask = (attn_mask == 0).bool()  # True indicates positions to exclude (padding tokens). Shape: (batch_size * num_heads, target_len, source_len)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        attended_features = self.cross_attention(\n",
    "            query=outputs1,\n",
    "            key=outputs2,\n",
    "            value=outputs2,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            # attn_mask=attn_mask,\n",
    "        )\n",
    "\n",
    "        # nan_count = torch.isnan(attended_features).sum().item()\n",
    "        # print('att features shape,', attended_features.shape)\n",
    "        # print('nan_count,', nan_count)\n",
    "\n",
    "        # Get CLS from attended features\n",
    "        cls1_attended = attended_features[:, 0, :]\n",
    "\n",
    "        # Concatenate the two [CLS] tokens\n",
    "        combined = torch.cat([cls1_attended, cls2], dim=1) # Shape: (batch_size, 2 * hidden_size)\n",
    "\n",
    "        # Pass through MLP head\n",
    "        output = self.mlp(combined)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-eval loop with experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define text feature/target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.281031Z",
     "iopub.status.busy": "2024-12-23T15:23:24.280745Z",
     "iopub.status.idle": "2024-12-23T15:23:24.297130Z",
     "shell.execute_reply": "2024-12-23T15:23:24.296385Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.280974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_col_1 = 'description_no_numbers'\n",
    "text_col_1_with_prompt = text_col_1 + '_with_prompt' # Add prompt to text for multitask learning\n",
    "\n",
    "text_col_2 = 'title_company_location_skills_source' # Merged text column, second feature\n",
    "\n",
    "bin_targets_col = 'salary_bin' # Bin targets for MASK token prediction and multitask learning\n",
    "target_col = 'log_salary_from' # regression target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create merged title/skills/location/source feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.298453Z",
     "iopub.status.busy": "2024-12-23T15:23:24.298136Z",
     "iopub.status.idle": "2024-12-23T15:23:24.735327Z",
     "shell.execute_reply": "2024-12-23T15:23:24.734341Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.298418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['skills'] = df['skills'].fillna('Не указаны')\n",
    "\n",
    "title_company_location_skills_feature_template = \"\"\"\n",
    "Позиция: {position}\n",
    "Компания: {company}\n",
    "Место: {location}\n",
    "Навыки: {skills}\n",
    "Источник: {source}\n",
    "\"\"\"\n",
    "\n",
    "df[text_col_2] = df.apply(lambda x: title_company_location_skills_feature_template.format(\n",
    "    position=x['title'],\n",
    "    company=x['company'],\n",
    "    location=x['location'],\n",
    "    skills=x['skills'],\n",
    "    source=x['source']\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create bins for target prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.736785Z",
     "iopub.status.busy": "2024-12-23T15:23:24.736435Z",
     "iopub.status.idle": "2024-12-23T15:23:24.812683Z",
     "shell.execute_reply": "2024-12-23T15:23:24.811592Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.736746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prompt to be added to feature 1 for multitask learning\n",
    "prompt = \"\"\"\\\n",
    "[CLS] Далее указано описание вакансии. \\\n",
    "Судя по описанию, зарплата на этой позиции составляет [MASK].[SEP]\\\n",
    "\"\"\"\n",
    "\n",
    "df[text_col_1_with_prompt] = prompt + df[text_col_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:24.814105Z",
     "iopub.status.busy": "2024-12-23T15:23:24.813765Z",
     "iopub.status.idle": "2024-12-23T15:23:25.126838Z",
     "shell.execute_reply": "2024-12-23T15:23:25.126030Z",
     "shell.execute_reply.started": "2024-12-23T15:23:24.814077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 1: Single BERT, MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:25.128512Z",
     "iopub.status.busy": "2024-12-23T15:23:25.128123Z",
     "iopub.status.idle": "2024-12-23T15:23:25.138773Z",
     "shell.execute_reply": "2024-12-23T15:23:25.137941Z",
     "shell.execute_reply.started": "2024-12-23T15:23:25.128472Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# experiment_name = 'single_bert_mse'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length\": 1024,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col, bin_targets_col]]\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = SingleBERTWithMLP(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 2: Dual BERT, MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:25.140207Z",
     "iopub.status.busy": "2024-12-23T15:23:25.139930Z",
     "iopub.status.idle": "2024-12-23T15:23:25.153402Z",
     "shell.execute_reply": "2024-12-23T15:23:25.152567Z",
     "shell.execute_reply.started": "2024-12-23T15:23:25.140183Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# experiment_name = 'dual_bert_mse'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length\": 1024,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]][:200]\n",
    "# y = df[[target_col,]][:200]\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = DualBERTWithMLP(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# # shape of output1:  torch.Size([16, 1024, 312])\n",
    "# # shape of output2:  torch.Size([16, 256, 312])\n",
    "\n",
    "# # shape of att mask1:  torch.Size([16, 1024])\n",
    "# # shape of att mask2:  torch.Size([16, 256])\n",
    "\n",
    "# # attn_mask (Optional[Tensor]) – If specified, a 2D or 3D mask preventing attention to certain positions. \n",
    "# # Must be of shape (L,S) or (N⋅num_heads,L,S),\n",
    "# # where NN is the batch size, L is the target sequence length, and S is the source sequence length. \n",
    "# # A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. \n",
    "# # Binary and float masks are supported. \n",
    "# # For a binary mask, a True value indicates that the corresponding position is not allowed to attend.\n",
    "# # For a float mask, the mask values will be added to the attention weight.\n",
    "# # If both attn_mask and key_padding_mask are supplied, their types should match.\n",
    "\n",
    "# # shape: 32 * 8, 1024, 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 3: Single BERT, Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:25.154657Z",
     "iopub.status.busy": "2024-12-23T15:23:25.154408Z",
     "iopub.status.idle": "2024-12-23T15:23:25.165759Z",
     "shell.execute_reply": "2024-12-23T15:23:25.165040Z",
     "shell.execute_reply.started": "2024-12-23T15:23:25.154633Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# experiment_name = 'single_bert_huber'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length\": 1024,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col, bin_targets_col]]\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = SingleBERTWithMLP(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.HuberLoss() # changed!\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 4: Double BERT, Huber loss: pre-tune with TSDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:25.167150Z",
     "iopub.status.busy": "2024-12-23T15:23:25.166895Z",
     "iopub.status.idle": "2024-12-23T15:23:25.180928Z",
     "shell.execute_reply": "2024-12-23T15:23:25.180181Z",
     "shell.execute_reply.started": "2024-12-23T15:23:25.167127Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# experiment_name = 'double_bert_huber_tsdae'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length\": 1024,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col, bin_targets_col]]\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     # further split train data into regression train and tsdae train data\n",
    "#     X_train_tsdae, X_tsdae, y_train_tsdae, _ = train_test_split(X_train, y_train, test_size=0.01, random_state=seed)\n",
    "\n",
    "#     # get test and train targets\n",
    "#     y_test_reg = y_test[target_col]\n",
    "#     y_train_tsdae_reg = y_train_tsdae[target_col]\n",
    "\n",
    "#     # X_tsdae used for TSDAE\n",
    "#     # X_train_tsdae, y_train_tsdae used for the downstream regression training\n",
    "#     # X_test, y_test used for the final model evaluation\n",
    "#     # y_test_tsdae is not used (hence set to '_')\n",
    "\n",
    "#     # TSDAE part:\n",
    "\n",
    "#     # 1. convert job descriptions into a set of sentences\n",
    "#     # and select 10-60 word sentences as a feature column.\n",
    "#     # 2. Train with TSDAE task on these selected sentences.\n",
    "    \n",
    "#     # 1. Create a DataFrame of unique sentences and their lengths for X_tsdae\n",
    "#     unique_sentences = []\n",
    "#     unique_sentence_lengths = []\n",
    "#     for text in X_tsdae[text_col_1]:\n",
    "#         sentences, sentence_lengths = get_sentence_lengths(text)\n",
    "#         unique_sentences.extend(sentences)\n",
    "#         unique_sentence_lengths.extend(sentence_lengths)\n",
    "\n",
    "#     unique_sentences_df = pd.DataFrame({\n",
    "#         'sentence': unique_sentences,\n",
    "#         'length': unique_sentence_lengths\n",
    "#     })\n",
    "\n",
    "#     unique_sentences = unique_sentences_df[(unique_sentences_df.length >= 10) & (unique_sentences_df.length <= 60)]['sentence']\n",
    "\n",
    "#     # get array with features for each bert\n",
    "#     train_sentences_array = [\n",
    "#         unique_sentences.tolist(),\n",
    "#         X_tsdae[text_col_2].tolist(),\n",
    "#     ]\n",
    "\n",
    "#     # 2. Train the models on TSDAE task\n",
    "#     berts_after_tsdae = []\n",
    "#     for index, train_sentences in enumerate(train_sentences_array):\n",
    "#         memory_cleanup()\n",
    "#         berts_after_tsdae.append(train_tsdae_bert(model_name, train_sentences))\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     tsdae_bert1, tsdae_bert2 = berts_after_tsdae\n",
    "\n",
    "#     # Initialize the model for regression\n",
    "#     model = TSDAEDualBERTWithMLP(config, tsdae_bert1, tsdae_bert2)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.HuberLoss()\n",
    "\n",
    "#     # fit-eval regression\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train_tsdae,\n",
    "#         X_test,\n",
    "#         y_train_tsdae_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "    \n",
    "#     combined_history[seed] = history\n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 5, dual bert (cls + mask pooling), huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:25.182594Z",
     "iopub.status.busy": "2024-12-23T15:23:25.182141Z",
     "iopub.status.idle": "2024-12-23T15:23:25.196313Z",
     "shell.execute_reply": "2024-12-23T15:23:25.195547Z",
     "shell.execute_reply.started": "2024-12-23T15:23:25.182558Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# experiment_name = 'dual_bert_cls_mask_huber'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length\": 1024,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 256,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 15, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col]]\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = MASKPoolDualBERTWithMLP(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.HuberLoss()\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1_with_prompt,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 6, single bert + cross attention + huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:23:25.216785Z",
     "iopub.status.busy": "2024-12-23T15:23:25.216464Z",
     "iopub.status.idle": "2024-12-23T15:24:13.454910Z",
     "shell.execute_reply": "2024-12-23T15:24:13.454062Z",
     "shell.execute_reply.started": "2024-12-23T15:23:25.216750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import time\n",
    "\n",
    "\n",
    "# experiment_name = 'single_bert_cross_attention_huber'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length_1\": 1024,\n",
    "#     \"seq_length_2\": 256,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "#     \"num_heads\": 8\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# # short version\n",
    "# # df_sample = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# # df_sample = df_sample.sample(n=5000, random_state=42)\n",
    "# # X = df_sample[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# # y = df_sample[[target_col,]]\n",
    "\n",
    "# # full version\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col,]]\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = SingleBERTWithCrossAttention(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.HuberLoss() # changed!\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# # no attention:\n",
    "# # 1694.4s\t178\tTEST METRICS FOR THE BEST EPOCH (#10)\n",
    "# # 1694.4s\t179\tR2: mean = 0.4464, 95% CI = [0.3873, 0.5055]\n",
    "# # 1694.4s\t180\tMAE: mean = 0.3759, 95% CI = [0.3352, 0.4166]\n",
    "# # 1694.4s\t181\tRMSE: mean = 0.4848, 95% CI = [0.4521, 0.5175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. single bert huber short seq 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import time\n",
    "\n",
    "\n",
    "# experiment_name = 'single_bert_huber_short_seq_len2'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length_1\": 1024,\n",
    "#     \"seq_length_2\": 256,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "#     \"num_heads\": 8\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# # short version\n",
    "# # df_sample = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# # df_sample = df_sample.sample(n=5000, random_state=42)\n",
    "# # X = df_sample[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# # y = df_sample[[target_col,]]\n",
    "\n",
    "# # full version\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col,]]\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = SingleBERTWithMLP(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.HuberLoss() # changed!\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# # no attention:\n",
    "# # 1694.4s\t178\tTEST METRICS FOR THE BEST EPOCH (#10)\n",
    "# # 1694.4s\t179\tR2: mean = 0.4464, 95% CI = [0.3873, 0.5055]\n",
    "# # 1694.4s\t180\tMAE: mean = 0.3759, 95% CI = [0.3352, 0.4166]\n",
    "# # 1694.4s\t181\tRMSE: mean = 0.4848, 95% CI = [0.4521, 0.5175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 8, single bert + huber loss + mask pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import pickle\n",
    "# import warnings\n",
    "# from transformers import AutoTokenizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import time\n",
    "\n",
    "\n",
    "# experiment_name = 'single_bert_huber_mask_pooling'\n",
    "# print(experiment_name.upper())\n",
    "# print('='*100)\n",
    "# print()\n",
    "\n",
    "# # Suppress all FutureWarnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# seeds = [42, 78687, 123123]\n",
    "# combined_history = {}\n",
    "\n",
    "# # Hyperparameters and configuration\n",
    "# config = {\n",
    "#     \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "#     \"batch_size\": 32,\n",
    "#     \"seq_length_1\": 1024,\n",
    "#     \"seq_length_2\": 1024,\n",
    "#     \"hidden_size\": 312,\n",
    "#     \"mlp_hidden_size\": 128,\n",
    "#     \"num_epochs\": 10,\n",
    "#     \"learning_rate\": 5e-6,\n",
    "#     \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "#     \"num_heads\": 8\n",
    "# }\n",
    "\n",
    "# memory_cleanup()\n",
    "\n",
    "# model_name = config['model_name']\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Dataset and DataLoader\n",
    "# # Prepare data\n",
    "# # short version\n",
    "# # df_sample = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# # df_sample = df_sample.sample(n=5000, random_state=42)\n",
    "# # X = df_sample[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# # y = df_sample[[target_col,]]\n",
    "\n",
    "# # full version\n",
    "# X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df[[target_col,]]\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# for seed in seeds:\n",
    "#     memory_cleanup()\n",
    "#     print()\n",
    "#     print(f'Starting for seed {str(seed)}...')\n",
    "#     print('-' * 100)\n",
    "#     print()\n",
    "\n",
    "#     combined_history[seed] = {}\n",
    "\n",
    "#     set_seed(seed)\n",
    "\n",
    "#     # Split train-test\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "#     y_train_reg = y_train[target_col]\n",
    "#     y_test_reg = y_test[target_col]\n",
    "\n",
    "#     # Initialize the model\n",
    "#     model = MASKPoolSingleBERTWithMLP(config)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "#     # Loss Function\n",
    "#     criterion = nn.HuberLoss() # changed!\n",
    "\n",
    "#     model, history = fit_eval(\n",
    "#         seed,\n",
    "#         model,\n",
    "#         X_train,\n",
    "#         X_test,\n",
    "#         y_train_reg,\n",
    "#         y_test_reg,\n",
    "#         criterion,\n",
    "#         tokenizer,\n",
    "#         config,\n",
    "#         text_col_1,\n",
    "#         text_col_2,\n",
    "#     )\n",
    "\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# # save the history as pickle\n",
    "# with open(f'./data/history/transfomers_{experiment_name}.pickle', 'wb') as handle:\n",
    "#     pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# end_time = time.time()\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# # no attention:\n",
    "# # 1694.4s\t178\tTEST METRICS FOR THE BEST EPOCH (#10)\n",
    "# # 1694.4s\t179\tR2: mean = 0.4464, 95% CI = [0.3873, 0.5055]\n",
    "# # 1694.4s\t180\tMAE: mean = 0.3759, 95% CI = [0.3352, 0.4166]\n",
    "# # 1694.4s\t181\tRMSE: mean = 0.4848, 95% CI = [0.4521, 0.5175]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 9 (baseline), average predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Starting for seed 42...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Starting for seed 78687...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Starting for seed 123123...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Average results:\n",
      "R2: mean = -0.0001, 95% CI = [-0.0003, -0.0000]\n",
      "MAE: mean = 0.5130, 95% CI = [0.5122, 0.5143]\n",
      "RMSE: mean = 0.6286, 95% CI = [0.6280, 0.6290]\n",
      "Execution time: 1.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# predict by average\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import warnings\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "experiment_name = 'average'\n",
    "print(experiment_name.upper())\n",
    "print('='*100)\n",
    "print()\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "seeds = [42, 78687, 123123]\n",
    "combined_history = {}\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "config = {\n",
    "    \"model_name\": \"sergeyzh/rubert-tiny-turbo\",\n",
    "    \"batch_size\": 32,\n",
    "    \"seq_length_1\": 1024,\n",
    "    \"seq_length_2\": 1024,\n",
    "    \"hidden_size\": 312,\n",
    "    \"mlp_hidden_size\": 128,\n",
    "    \"num_epochs\": 10,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"mask_token_index\": 17, # position of the [MASK] token in the feature 1 (to be used in MASK token prediction)\n",
    "    \"num_heads\": 8\n",
    "}\n",
    "\n",
    "memory_cleanup()\n",
    "\n",
    "model_name = config['model_name']\n",
    "# Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Prepare data\n",
    "# short version\n",
    "# df_sample = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_sample = df_sample.sample(n=5000, random_state=42)\n",
    "# X = df_sample[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "# y = df_sample[[target_col,]]\n",
    "\n",
    "# full version\n",
    "X = df[[text_col_1, text_col_1_with_prompt, text_col_2]]\n",
    "y = df[[target_col,]]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for seed in seeds:\n",
    "    memory_cleanup()\n",
    "    print()\n",
    "    print(f'Starting for seed {str(seed)}...')\n",
    "    print('-' * 100)\n",
    "    print()\n",
    "\n",
    "    combined_history[seed] = {}\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    y_train_reg = y_train[target_col]\n",
    "    y_test_reg = y_test[target_col]\n",
    "\n",
    "    # Initialize the model\n",
    "    # model = MASKPoolSingleBERTWithMLP(config)\n",
    "    # model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # Loss Function\n",
    "    # criterion = nn.HuberLoss() # changed!\n",
    "\n",
    "    # model, history = fit_eval(\n",
    "    #     seed,\n",
    "    #     model,\n",
    "    #     X_train,\n",
    "    #     X_test,\n",
    "    #     y_train_reg,\n",
    "    #     y_test_reg,\n",
    "    #     criterion,\n",
    "    #     tokenizer,\n",
    "    #     config,\n",
    "    #     text_col_1,\n",
    "    #     text_col_2,\n",
    "    # )\n",
    "\n",
    "    y_pred = y_train_reg.mean().repeat(len(y_test_reg))\n",
    "    y_train_by_average = y_train_reg.mean().repeat(len(y_train_reg))\n",
    "    train_r2 = r2_score(y_train_reg, y_train_by_average)\n",
    "    train_mae = mean_absolute_error(y_train_reg, y_train_by_average)\n",
    "    train_rmse = mean_squared_error(y_train_reg, y_train_by_average, squared=False)\n",
    "    test_r2 = r2_score(y_test_reg, y_pred)\n",
    "    test_mae = mean_absolute_error(y_test_reg, y_pred)\n",
    "    test_rmse = mean_squared_error(y_test_reg, y_pred, squared=False)\n",
    "    history = {\n",
    "        'train_r2': train_r2,\n",
    "        'train_mae': train_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_r2': test_r2,\n",
    "        'test_mae': test_mae,\n",
    "        'test_rmse': test_rmse,\n",
    "    }\n",
    "\n",
    "    memory_cleanup()\n",
    "\n",
    "    combined_history[seed] = history    \n",
    "\n",
    "# # Display metrics\n",
    "# display_metrics_with_ci(combined_history)\n",
    "\n",
    "# save the history as pickle\n",
    "with open(f'./data/history/baseline_{experiment_name}.pickle', 'wb') as handle:\n",
    "    pickle.dump(combined_history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# print average results\n",
    "print('Average results:')\n",
    "for metric in ['r2', 'mae', 'rmse']:\n",
    "    train_metric = [combined_history[seed][f'train_{metric}'] for seed in seeds]\n",
    "    test_metric = [combined_history[seed][f'test_{metric}'] for seed in seeds]\n",
    "    print(f'{metric.upper()}: mean = {np.mean(test_metric):.4f}, 95% CI = [{np.percentile(test_metric, 2.5):.4f}, {np.percentile(test_metric, 97.5):.4f}]')\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# no attention:\n",
    "# 1694.4s\t178\tTEST METRICS FOR THE BEST EPOCH (#10)\n",
    "# 1694.4s\t179\tR2: mean = 0.4464, 95% CI = [0.3873, 0.5055]\n",
    "# 1694.4s\t180\tMAE: mean = 0.3759, 95% CI = [0.3352, 0.4166]\n",
    "# 1694.4s\t181\tRMSE: mean = 0.4848, 95% CI = [0.4521, 0.5175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: mean = -0.0001, 95% CI = (-0.00014983344452172234, -0.00048374797871964285, 0.00018408108967619814)\n",
      "MAE: mean = 0.5130, 95% CI = (0.5130321940487806, 0.5106100256899847, 0.5154543624075766)\n",
      "RMSE: mean = 0.6286, 95% CI = (0.6286108107707062, 0.6274224108817511, 0.6297992106596614)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "        n = len(data)\n",
    "        m, se = np.mean(data), np.std(data) / np.sqrt(n)\n",
    "        h = se * t.ppf((1 + confidence) / 2, n-1)\n",
    "        return m, m-h, m+h\n",
    "\n",
    "# get mean and +- 95% CI for the metrics\n",
    "r2 = [combined_history[seed]['test_r2'] for seed in seeds]\n",
    "mae = [combined_history[seed]['test_mae'] for seed in seeds]\n",
    "rmse = [combined_history[seed]['test_rmse'] for seed in seeds]\n",
    "\n",
    "print(f'R2: mean = {np.mean(r2):.4f}, 95% CI = {mean_confidence_interval(r2)}')\n",
    "print(f'MAE: mean = {np.mean(mae):.4f}, 95% CI = {mean_confidence_interval(mae)}')\n",
    "print(f'RMSE: mean = {np.mean(rmse):.4f}, 95% CI = {mean_confidence_interval(rmse)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ods-final-project-salary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
