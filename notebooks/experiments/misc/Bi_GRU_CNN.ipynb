{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the data and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_md')\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy gensim -qqq\n",
        "!python -m spacy download ru_core_news_md -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrHmDLFej4k3",
        "outputId": "baa3972f-0bf0-4061-d80b-29dd6c71d965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: data: File exists\n",
            "mkdir: data/history: File exists\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 22224 entries, 0 to 22223\n",
            "Data columns (total 15 columns):\n",
            " #   Column                                Non-Null Count  Dtype  \n",
            "---  ------                                --------------  -----  \n",
            " 0   title                                 22224 non-null  object \n",
            " 1   company                               22224 non-null  object \n",
            " 2   location                              22224 non-null  object \n",
            " 3   skills                                14384 non-null  object \n",
            " 4   source                                22224 non-null  object \n",
            " 5   description_no_numbers_with_skills    22224 non-null  object \n",
            " 6   experience_from                       22224 non-null  float64\n",
            " 7   experience_to_adjusted_10             22224 non-null  float64\n",
            " 8   description_size                      22224 non-null  int64  \n",
            " 9   description                           22224 non-null  object \n",
            " 10  description_no_numbers                22224 non-null  object \n",
            " 11  description_no_numbers_v2             22224 non-null  object \n",
            " 12  title_company_location_skills_source  22224 non-null  object \n",
            " 13  salary_from                           22224 non-null  float64\n",
            " 14  log_salary_from                       22224 non-null  float64\n",
            "dtypes: float64(4), int64(1), object(10)\n",
            "memory usage: 2.5+ MB\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!mkdir data/history\n",
        "# !wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1iqg1FIPfbrZWlung6gZqve1MeQWc0Je4&export=download&authuser=1&confirm=t' -O './data/dataset.csv'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import gc\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#####################\n",
        "# Utility functions #\n",
        "#####################\n",
        "\n",
        "def memory_cleanup():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#####################\n",
        "# Data Preprocessing#\n",
        "#####################\n",
        "\n",
        "df = pd.read_csv('../data/dataset.csv')\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create merged title/skills/location/source feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['skills'].fillna('Не указаны', inplace=True)\n",
        "\n",
        "title_company_location_skills_feature_template = \"\"\"\n",
        "Позиция: {position}\n",
        "Компания: {company}\n",
        "Место: {location}\n",
        "Навыки: {skills}\n",
        "Источник: {source}\n",
        "\"\"\"\n",
        "\n",
        "df['title_company_location_skills_source'] = df.apply(lambda x: title_company_location_skills_feature_template.format(\n",
        "    position=x['title'],\n",
        "    company=x['company'],\n",
        "    location=x['location'],\n",
        "    skills=x['skills'],\n",
        "    source=x['source']\n",
        "), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aQxLJf4j9EC",
        "outputId": "e8f5ff94-b52f-4d55-e9cd-79d98278af48"
      },
      "outputs": [],
      "source": [
        "# Предположим, что у нас есть столбец 'description_no_numbers' или 'description',\n",
        "# и таргет 'log_salary_from'. Если нет, адаптируйте поля.\n",
        "text_col = 'description_no_numbers'\n",
        "text_col2 = 'title_company_location_skills_source'\n",
        "target_col = 'log_salary_from'\n",
        "\n",
        "# Удаляем пропуски\n",
        "df = df.dropna(subset=[text_col, text_col2, target_col])\n",
        "\n",
        "# get a subset of the data to try tokenization on\n",
        "sample = df.sample(100, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['в', 'аккредитованную', 'it', 'компанию', 'подбираем', 'ведущий', 'сетевой', 'инженер', '<PERIOD>', 'чем', 'предстоит', 'заниматься', 'выполнять', 'настройку', '<COMMA>', 'установку', 'и', 'поддержку', 'оборудования', 'локальных', '<COMMA>', 'глобальных', '<COMMA>', 'беспроводных', 'вычислительных', 'сетей', '<PERIOD>', 'обеспечивать', 'бесперебойную', 'работу', 'служб', 'удаленного', 'доступа', 'к', 'ресурсам', 'вычислительных', 'сетей', '<PERIOD>', 'выполнять', 'настойку', 'и', 'поддержку', 'систем', 'мониторинга', 'и', 'систем', 'сбора', 'статистики', 'по', 'использованию', 'вычислительных', 'сетей', '<COMMA>', 'каналов', 'связи', '<PERIOD>', 'устранять', 'нештатные', 'ситуации', '<PERIOD>', 'выполнять', 'заявки', '<COMMA>', 'поступающие', 'от', 'оператора', 'технической', 'поддержки', '<PERIOD>', 'оказывать', 'техническую', 'помощь', 'и', 'консультации', 'структурным', 'подразделениям', 'организации', '<PERIOD>', 'разрабатывать', 'и', 'доводить', 'до', 'сведения', 'руководителя', 'подразделения', '<COMMA>', 'предложения', '<COMMA>', 'направленные', 'на', 'повышение', 'эффективности', 'или', 'модернизации', 'использования', 'оборудования', 'вычислительных', 'сетей', '<COMMA>', 'систем', 'управления', 'вычислительными', 'сетями', '<COMMA>', 'каналов', 'связи', '<PERIOD>', 'поддерживать', 'в', 'актуальном', 'состоянии', 'техническую', 'документацию', '<PERIOD>', 'требования', 'опыт', 'работы', 'по', 'данному', 'направлению', 'от', '2', '<DASH>', 'х', 'лет', '<SEMICOLON>', 'опыт', 'работы', 'с', 'локальными', 'и', 'беспроводными', 'сетями', 'на', 'базе', 'cisco', '<COMMA>', 'eltex', '<COMMA>', 'mikrotik', '<SEMICOLON>', 'опыт', 'настройки', 'и', 'сопровождения', 'сетей', 'на', 'базе', 'cisco', '<COMMA>', 'fortigate', '<COMMA>', 'eltex', '<COMMA>', 'checkpoint', '<COMMA>', 'mikrotik', '<LEFT_PAREN>', 'плюсом', 'будет', 'знание', 'решений', 'usergate', '<COMMA>', 'ideco', '<COMMA>', 'с', '<DASH>', 'терра', '<COMMA>', 'континент', '<COMMA>', 'juniper', '<RIGHT_PAREN>', '<SEMICOLON>', 'опыт', 'настройки', 'и', 'сопровождения', 'удаленного', 'vpn', '<DASH>', 'доступа', 'пользователей', 'с', 'использованием', 'cisco', '<LEFT_PAREN>', 'vpn', '<DASH>', 'клиент', 'cisco', 'anyconnect', 'vpn', '<DASH>', 'шлюз', 'cisco', 'firepower', '<RIGHT_PAREN>', '<COMMA>', 'плюсом', 'будет', 'знание', 'решений', 'checkpoint', 'и', 'решений', 'на', 'свободном', 'по', '<PERIOD>', 'будет', 'плюсом', 'опыт', 'настройки', 'поддержки', 'технологий', 'обеспечения', 'приоритезации', 'сетевого', 'трафика', '<LEFT_PAREN>', 'qos', '<RIGHT_PAREN>', 'в', 'вычислительных', 'сетях', '<SEMICOLON>', 'опыт', 'работы', 'с', 'системой', 'мониторинга', 'zabbix', '<SEMICOLON>', 'опыт', 'открытия', 'и', 'сопровождения', 'кейсов', 'в', 'службе', 'технической', 'поддержки', '<SEMICOLON>', 'опыт', 'работы', 'ос', 'linux', '<LEFT_PAREN>', 'centos', 'debian', '<RIGHT_PAREN>', '<SEMICOLON>', 'умение', 'вести', 'техническую', 'документацию', 'и', 'писать', 'инструкции', '<SEMICOLON>', 'уровень', 'английского', 'позволяющий', 'читать', 'документацию', 'и', 'профильные', 'ресурсы', '<SEMICOLON>', 'условия', 'график', 'работы', '5', '2', '<COMMA>', 'с', '9', '<PERIOD>', '00', 'по', '18', '<PERIOD>', '00', '<PERIOD>', 'формат', 'работы', '<DASH>', 'гибрид', '<LEFT_PAREN>', 'возможно', 'удаленно', 'в', 'будущем', '<RIGHT_PAREN>', 'конкурентный', 'уровень', 'заработной', 'платы', '<COMMA>', 'премиальная', 'программа', 'трудоустройство', 'по', 'тк', 'рф', 'дмс', '<LEFT_PAREN>', 'со', 'стоматологией', '<RIGHT_PAREN>', 'с', 'первого', 'дня', 'готовы', 'сдвинуть', 'начало', 'рабочего', 'дня', 'на', 'час', 'раньше', 'позже', 'по', 'договоренности', 'оплата', 'питания', 'при', 'посещении', 'офиса', 'компенсация', 'за', 'использование', 'своей', 'техники', 'при', 'удаленном', 'формате']\n"
          ]
        }
      ],
      "source": [
        "def token_lookup(word):\n",
        "    \"\"\"\n",
        "    Generate a dict to turn punctuation into a token.\n",
        "    :return: Token corresponding to the punctuation.\n",
        "    \"\"\"\n",
        "    token = dict()\n",
        "    token['.'] = ' <PERIOD>'\n",
        "    token[','] = ' <COMMA>'\n",
        "    token[','] = ' <COMMA>'\n",
        "    token['\"'] = ' <QUOTATION_MARK>'\n",
        "    token[';'] = ' <SEMICOLON>'\n",
        "    token['!'] = ' <EXCLAMATION_MARK>'\n",
        "    token['?'] = ' <QUESTION_MARK>'\n",
        "    token['('] = ' <LEFT_PAREN>'\n",
        "    token[')'] = ' <RIGHT_PAREN>'\n",
        "    token['-'] = ' <DASH>'\n",
        "    token['\\n'] = ' <NEW_LINE>'\n",
        "    return token.get(word, word)\n",
        "\n",
        "def replace_punctuation(text):\n",
        "    \"\"\"\n",
        "    Replace punctuation with tokens so we can use them in our model.\n",
        "    :param text: The text to be modified.\n",
        "    :return: The text with punctuation replaced.\n",
        "    \"\"\"\n",
        "    return ''.join([token_lookup(word) for word in text.lower()])\n",
        "\n",
        "# Простая токенизация: разбиение по словам, очистка\n",
        "def simple_tokenize(text):\n",
        "    # В реальном случае можно добавить более сложную предобработку, лемматизацию и т.д.\n",
        "    tokens = re.findall(r'\\b\\w+\\b|<\\w+>', text)\n",
        "    return tokens\n",
        "\n",
        "sample_tokens = sample[text_col].apply(replace_punctuation).apply(simple_tokenize)\n",
        "print(sample_tokens.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['softmedialab', '<DASH>', 'резидент', 'сколково', '<COMMA>', 'аккредитованная', 'ит', 'компания', '<COMMA>', 'мы', 'занимаемся', 'цифровизацией', 'с', '2015', 'года', '<PERIOD>', 'специализируемся', 'на', 'сложных', 'решениях', 'для', 'крупных', 'корпоративных', 'клиентов', 'в', 'различных', 'отраслях', '<COMMA>', 'с', 'высокими', 'требованиями', 'к', 'качеству', '<COMMA>', 'надежности', 'и', 'производительности', 'решений', '<PERIOD>', 'на', 'данный', 'момент', 'сделали', '80', 'проектов', 'в', 'россии', '<COMMA>', 'канаде', '<COMMA>', 'европе', '<COMMA>', 'израиле', '<COMMA>', 'сингапуре', 'и', 'сша', '<PERIOD>', 'цель', 'компании', 'на', '2024', 'год', '<DASH>', 'вырасти', 'в', 'два', 'раза', '<COMMA>', 'как', 'по', 'выручке', 'так', 'и', 'по', 'численности', '<PERIOD>', 'мы', 'наращиваем', 'продуктовую', 'экспертизу', '<COMMA>', 'создавая', 'специализированные', 'направления', '<PERIOD>', 'ищем', 'devops', '<DASH>', 'инженера', 'в', 'аутстафф', 'проект', '<PERIOD>', 'мы', 'рассматриваем', 'специалистов', 'в', 'своём', 'деле', '<PERIOD>', 'ваше', 'образование', '<COMMA>', 'возраст', 'значения', 'не', 'имеют', '<PERIOD>', 'задачи', '<COMMA>', 'которые', 'придется', 'решать', '1', '<PERIOD>', 'развертывание', 'и', 'настройка', 'общесистемного', 'по', 'в', 'test', '<DASH>', 'среде', '2', '<PERIOD>', 'развертывание', 'и', 'настройка', 'общесистемного', 'по', 'в', 'prod', '<DASH>', 'среде', '3', '<PERIOD>', 'развертывание', 'прикладного', 'по', '<LEFT_PAREN>', 'приложения', '<RIGHT_PAREN>', 'в', 'test', '<DASH>', 'среде', '4', '<PERIOD>', 'развертывание', 'прикладного', 'по', '<LEFT_PAREN>', 'приложения', '<RIGHT_PAREN>', 'в', 'prod', '<DASH>', 'среде', '5', '<PERIOD>', 'сопровождение', 'test', '<DASH>', 'среды', '6', '<PERIOD>', 'сопровождение', 'prod', '<DASH>', 'среды', 'мы', 'ожидаем', 'от', 'кандидата', '1', '<RIGHT_PAREN>', 'базовые', 'знания', 'ci', 'cd', 'gitlab', '<COMMA>', 'nexus', '<LEFT_PAREN>', 'уровень', 'пользователя', '<RIGHT_PAREN>', '2', '<RIGHT_PAREN>', 'знание', 'k8s', 'на', 'уровне', 'администратора', '3', '<RIGHT_PAREN>', 'базовые', 'знания', 'баз', 'данных', 'postgres', 'и', 'cassandra', '<LEFT_PAREN>', 'бэкап', '<COMMA>', 'восстановление', '<COMMA>', 'дебаг', '<RIGHT_PAREN>', '4', '<RIGHT_PAREN>', 'базовые', 'знания', 'opensource', 'nifi', '<COMMA>', 'kafka', '<COMMA>', 'keycloak', '<COMMA>', 'rabbitmq', '<COMMA>', 'redis', '<COMMA>', 'stolon', '<COMMA>', 'opensearch', '<COMMA>', 'grafana', '<COMMA>', 'jaeger', '5', '<RIGHT_PAREN>', 'основы', 'понимания', 'сетей', '6', '<RIGHT_PAREN>', 'знание', 'линукс', 'на', 'уровне', 'администратора', '7', '<RIGHT_PAREN>', 'опыт', 'работы', 'с', 'платформой', 'ziiot', '8', '<RIGHT_PAREN>', 'локация', 'мск', 'и', 'мо', '<COMMA>', 'готовность', 'к', 'командировкам', 'на', 'предприятие', 'нпп', 'нефтехимия', 'в', 'г', '<PERIOD>', 'москва', 'мы', 'в', 'свою', 'очередь', 'предлагаем', 'работа', 'в', 'большом', 'масштабным', 'проекте', '<SEMICOLON>', 'возможность', 'быстро', 'прокачаться', 'и', 'поработать', 'с', 'разными', 'технологиями', '<SEMICOLON>', 'сильная', 'техническая', 'команда', 'и', 'четкое', 'руководство', '<PERIOD>', 'стабильные', 'ежемесячные', 'выплаты', '<LEFT_PAREN>', 'почасовая', 'оплата', 'от', '2300', 'руб', '<PERIOD>', '<RIGHT_PAREN>', '<SEMICOLON>', 'гибкий', 'график', '<COMMA>', 'частичная', 'удаленка', '<SEMICOLON>', 'для', 'начала', 'познакомимся', 'на', 'первичном', 'интервью', '<COMMA>', 'где', 'расскажем', 'подробности', 'о', 'проектах', 'и', 'вакансии', '<LEFT_PAREN>', 'любим', 'общаться', 'с', 'видео', 'гуглмит', '<COMMA>', 'зум', '<COMMA>', 'телеграмм', '<RIGHT_PAREN>', '<PERIOD>', 'далее', 'техническое', 'собеседование', '<COMMA>', 'где', 'обсудите', 'с', 'тимлидом', 'задачи', '<COMMA>', 'с', 'которыми', 'работали', '<PERIOD>', 'ценим', 'время', 'и', 'нервы', '<COMMA>', 'поэтому', 'все', 'будет', 'оперативно', 'и', 'доброжелательно', '<PERIOD>', 'готов', 'присоединиться', '<QUESTION_MARK>', 'пиши', '<COMMA>', 'отвечаем', 'сразу', 'как', 'получим', 'письмо', '<RIGHT_PAREN>']\n"
          ]
        }
      ],
      "source": [
        "print(sample_tokens.iloc[50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hello', '<COMMA>', 'world', '<EXCLAMATION_MARK>']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simple_tokenize(replace_punctuation('Hello, world!'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "to be incorp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import gensim.downloader as api\n",
        "gensim_model = api.load(\"word2vec-ruscorpora-300\")\n",
        "\n",
        "\n",
        "\n",
        "# Load the pre-trained Russian model\n",
        "token_to_pos = spacy.load(\"ru_core_news_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Мама NOUN\n",
            "  SPACE\n",
            "мыла NOUN\n",
            "1 NUM\n",
            "раму NOUN\n"
          ]
        }
      ],
      "source": [
        "text = 'Мама  мыла 1 раму'\n",
        "doc = token_to_pos(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_with_pos(text):\n",
        "    doc = token_to_pos(text.lower())\n",
        "    token_w_pos_lst = [f\"{token.text}_{token.pos_}\" for token in doc]\n",
        "    return token_w_pos_lst\n",
        "\n",
        "\n",
        "def average_word_vectors(token_w_pos_lst, word2vec_model):\n",
        "    mean = gensim_model.vectors.mean(1).mean()\n",
        "    std = gensim_model.vectors.std(1).mean()\n",
        "    word_vectors = [word2vec_model.get_vector(token_w_pos) for token_w_pos in token_w_pos_lst if token_w_pos in word2vec_model]\n",
        "    if len(word_vectors) == 0:\n",
        "        word_vectors = [np.random.normal(mean, std, word2vec_model.vector_size)]\n",
        "        # print(f'No words in model for sentence')\n",
        "        # return np.zeros(word2vec_model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# write a function that takes in a text (like sentence) \n",
        "# and returns the list of word vectors up to a certain length\n",
        "# if the sentence is shorter than the length, pad with with padding vectors at the beginning\n",
        "# if there are words that are not in the model, use a random vector with the same mean and std as the model\n",
        "# do not average the vectors, just return the list of vectors\n",
        "\n",
        "def get_word_vectors(text, word2vec_model, max_len=100):\n",
        "    token_w_pos_lst = tokenize_with_pos(text)\n",
        "    mean = gensim_model.vectors.mean(1).mean()\n",
        "    std = gensim_model.vectors.std(1).mean()\n",
        "    word_vectors = [word2vec_model.get_vector(token_w_pos) for token_w_pos in token_w_pos_lst if token_w_pos in word2vec_model]\n",
        "    # now pad the vectors\n",
        "    # padding = [np.random.normal(mean, std, word2vec_model.vector_size)] * (max_len - len(word_vectors))\n",
        "    padding = [np.zeros(word2vec_model.vector_size)] * (max_len - len(word_vectors))\n",
        "    return padding + word_vectors[:max_len]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Разделяем на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[text_col], df[target_col], test_size=0.2, random_state=42)\n",
        "\n",
        "# Простая токенизация: разбиение по словам, очистка\n",
        "def simple_tokenize(text):\n",
        "    # В реальном случае можно добавить более сложную предобработку, лемматизацию и т.д.\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "# Строим словарь\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    freq = {}\n",
        "    for t in texts:\n",
        "        for w in t:\n",
        "            freq[w] = freq.get(w, 0) + 1\n",
        "    # Фильтруем по частоте\n",
        "    freq = {k:v for k,v in freq.items() if v >= min_freq}\n",
        "    # Создаем словарь слово->индекс\n",
        "    # 0 - <PAD>, 1 - <UNK>\n",
        "    word2idx = {'<PAD>':0, '<UNK>':1}\n",
        "    for w in freq:\n",
        "        word2idx[w] = len(word2idx)\n",
        "    return word2idx\n",
        "\n",
        "X_train_tokens = [simple_tokenize(txt) for txt in X_train]\n",
        "X_test_tokens = [simple_tokenize(txt) for txt in X_test]\n",
        "\n",
        "word2idx = build_vocab(X_train_tokens, min_freq=2)\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "def text_to_seq(tokens, word2idx):\n",
        "    return [word2idx.get(w, 1) for w in tokens]  # 1 для <UNK>\n",
        "\n",
        "# Считаем максимальную длину\n",
        "row_max_length = max(max(len(t) for t in X_train_tokens), max(len(t) for t in X_test_tokens))\n",
        "print(\"Max sequence length:\", row_max_length)\n",
        "\n",
        "def pad_sequence(seq, max_len, pad_idx=0):\n",
        "    if len(seq) < max_len:\n",
        "        seq = seq + [pad_idx]*(max_len - len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_len]\n",
        "    return seq\n",
        "\n",
        "X_train_seq = [pad_sequence(text_to_seq(t, word2idx), row_max_length) for t in X_train_tokens]\n",
        "X_test_seq = [pad_sequence(text_to_seq(t, word2idx), row_max_length) for t in X_test_tokens]\n",
        "\n",
        "X_train_seq = np.array(X_train_seq)\n",
        "X_test_seq = np.array(X_test_seq)\n",
        "\n",
        "y_train = y_train.values.astype(np.float32)\n",
        "y_test = y_test.values.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ2iB18Zj9GX"
      },
      "outputs": [],
      "source": [
        "######################################\n",
        "# PyTorch Dataset and DataLoaders    #\n",
        "######################################\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset = TextDataset(X_train_seq, y_train)\n",
        "test_dataset = TextDataset(X_test_seq, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d1zF92xkIib"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "# Bi_GRU_CNN Model in PyTorch     #\n",
        "###################################\n",
        "\n",
        "class BiGRUCNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, gru_units=128, conv_filters=64, kernel_size=3):\n",
        "        super(BiGRUCNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # BiGRU: return_sequences = True аналогично: batch, seq, hidden*2\n",
        "        self.gru = nn.GRU(embed_dim, gru_units, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Conv1D: в PyTorch Conv1d ожидает (batch, channels, seq_len)\n",
        "        # У нас выход GRU (batch, seq_len, hidden*2), нужно переставить оси\n",
        "        self.conv = nn.Conv1d(in_channels=gru_units*2, out_channels=conv_filters, kernel_size=kernel_size, padding='same')\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(conv_filters * (row_max_length//2), 1) # после пула длина примерно в 2 раза меньше\n",
        "        # Важно: после пула seq_len уменьшается в 2 раза. Если row_max_length нечет, потоки аккуратны, либо подбираем размер.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        gru_out, _ = self.gru(emb)  # (batch, seq_len, hidden*2)\n",
        "        # permute для Conv1d\n",
        "        gru_out = gru_out.permute(0, 2, 1)  # (batch, hidden*2, seq_len)\n",
        "        conv_out = self.conv(gru_out) # (batch, conv_filters, seq_len)\n",
        "        pooled = self.pool(conv_out) # (batch, conv_filters, seq_len/2)\n",
        "        # распрямляем\n",
        "        flat = pooled.flatten(start_dim=1) # (batch, conv_filters*(seq_len/2))\n",
        "        drop = self.dropout(flat)\n",
        "        out = self.fc(drop)  # (batch, 1)\n",
        "        return out.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dELsmtqOjJ_j",
        "outputId": "58da1e66-f61c-4bc7-a254-f7b77631d723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | Train Loss: 0.4870, Test Loss: 0.4092, Train R2: -0.1599, Test R2: 0.3198, Train MAE: 0.4871, Test MAE: 0.4082\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "# Training loop with metrics      #\n",
        "###################################\n",
        "\n",
        "def fit_eval(model, train_dl, test_dl, num_epochs=10, lr=1e-3, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()  # MAE; можно поменять на MSELoss, HuberLoss\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"train_mae\": [],\n",
        "        \"test_mae\": [],\n",
        "        \"train_r2\": [],\n",
        "        \"test_r2\": [],\n",
        "        \"train_rmse\": [],\n",
        "        \"test_rmse\": [],\n",
        "        \"y_pred\": [],\n",
        "        \"y_test\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for Xb, yb in train_dl:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(Xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(yb.cpu().numpy())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_mae = mean_absolute_error(all_labels, all_preds)\n",
        "        train_r2 = r2_score(all_labels, all_preds)\n",
        "        train_rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
        "\n",
        "        model.eval()\n",
        "        test_losses = []\n",
        "        all_preds_test = []\n",
        "        all_labels_test = []\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb in test_dl:\n",
        "                Xb = Xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                preds = model(Xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                test_losses.append(loss.item())\n",
        "                all_preds_test.extend(preds.cpu().numpy())\n",
        "                all_labels_test.extend(yb.cpu().numpy())\n",
        "\n",
        "        test_loss = np.mean(test_losses)\n",
        "        test_mae = mean_absolute_error(all_labels_test, all_preds_test)\n",
        "        test_r2 = r2_score(all_labels_test, all_preds_test)\n",
        "        test_rmse = np.sqrt(mean_squared_error(all_labels_test, all_preds_test))\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"train_mae\"].append(train_mae)\n",
        "        history[\"test_mae\"].append(test_mae)\n",
        "        history[\"train_r2\"].append(train_r2)\n",
        "        history[\"test_r2\"].append(test_r2)\n",
        "        history[\"train_rmse\"].append(train_rmse)\n",
        "        history[\"test_rmse\"].append(test_rmse)\n",
        "        history[\"y_pred\"].append(all_preds_test)\n",
        "        history[\"y_test\"].append(all_labels_test)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, \"\n",
        "              f\"Train R2: {train_r2:.4f}, Test R2: {test_r2:.4f}, Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "###################################\n",
        "# Run training                    #\n",
        "###################################\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiGRUCNNModel(vocab_size=vocab_size, embed_dim=256, gru_units=128, conv_filters=64, kernel_size=3)\n",
        "history = fit_eval(model, train_dataloader, test_dataloader, num_epochs=10, lr=1e-3, device=device)\n",
        "\n",
        "# Сохраним историю\n",
        "with open('./data/history/bi_gru_cnn_history.pickle', 'wb') as f:\n",
        "    pickle.dump(history, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYZhRg5OjQi7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ods-final-project-salary",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
